{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OLQQJSgP0SHO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BCGdDklY0SHQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import GPT2LMHeadModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujrc0H0-xtLu",
        "outputId": "f2eb60ec-be8d-472c-f427-5e5e94c7f217"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Ky6TfkfA0SHQ"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/566 Project/linares_2002.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "CN7Kps4r35As",
        "outputId": "45217be5-491c-4eb3-84aa-fe3002c14ca7"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Game Number  Move Number Player  Move  \\\n",
              "0               1            1  White  e2e4   \n",
              "1               1            2  Black  e7e6   \n",
              "2               1            3  White  d2d4   \n",
              "3               1            4  Black  d7d5   \n",
              "4               1            5  White  b1c3   \n",
              "...           ...          ...    ...   ...   \n",
              "3058           42           87  White  g3g4   \n",
              "3059           42           88  Black  h5g4   \n",
              "3060           42           89  White  h3g4   \n",
              "3061           42           90  Black  g7f7   \n",
              "3062           42           91  White  e8h8   \n",
              "\n",
              "                                                  Board  \\\n",
              "0     r n b q k b n r\\np p p p p p p p\\n. . . . . . ...   \n",
              "1     r n b q k b n r\\np p p p p p p p\\n. . . . . . ...   \n",
              "2     r n b q k b n r\\np p p p . p p p\\n. . . . p . ...   \n",
              "3     r n b q k b n r\\np p p p . p p p\\n. . . . p . ...   \n",
              "4     r n b q k b n r\\np p p . . p p p\\n. . . . p . ...   \n",
              "...                                                 ...   \n",
              "3058  . . . . R . . .\\n. . . . P . k .\\n. . . . . . ...   \n",
              "3059  . . . . R . . .\\n. . . . P . k .\\n. . . . . . ...   \n",
              "3060  . . . . R . . .\\n. . . . P . k .\\n. . . . . . ...   \n",
              "3061  . . . . R . . .\\n. . . . P . k .\\n. . . . . . ...   \n",
              "3062  . . . . R . . .\\n. . . . P k . .\\n. . . . . . ...   \n",
              "\n",
              "                                                Comment  \n",
              "0     Ponomariov plays 1. e4 in much the same way as...  \n",
              "1     Now, along with Pe4 there is an indication Bla...  \n",
              "2                                                   NaN  \n",
              "3                                                   NaN  \n",
              "4     I've had doubts about this move for a long tim...  \n",
              "...                                                 ...  \n",
              "3058                                                NaN  \n",
              "3059                                                NaN  \n",
              "3060                                                NaN  \n",
              "3061  Now that White is down to one rook pawn this i...  \n",
              "3062  Good defense from Ivanchuk. He's played very w...  \n",
              "\n",
              "[3063 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-18891883-1d83-4bf5-8d8b-15bb0ce7e0d5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Game Number</th>\n",
              "      <th>Move Number</th>\n",
              "      <th>Player</th>\n",
              "      <th>Move</th>\n",
              "      <th>Board</th>\n",
              "      <th>Comment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>White</td>\n",
              "      <td>e2e4</td>\n",
              "      <td>r n b q k b n r\\np p p p p p p p\\n. . . . . . ...</td>\n",
              "      <td>Ponomariov plays 1. e4 in much the same way as...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>Black</td>\n",
              "      <td>e7e6</td>\n",
              "      <td>r n b q k b n r\\np p p p p p p p\\n. . . . . . ...</td>\n",
              "      <td>Now, along with Pe4 there is an indication Bla...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>White</td>\n",
              "      <td>d2d4</td>\n",
              "      <td>r n b q k b n r\\np p p p . p p p\\n. . . . p . ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>Black</td>\n",
              "      <td>d7d5</td>\n",
              "      <td>r n b q k b n r\\np p p p . p p p\\n. . . . p . ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>White</td>\n",
              "      <td>b1c3</td>\n",
              "      <td>r n b q k b n r\\np p p . . p p p\\n. . . . p . ...</td>\n",
              "      <td>I've had doubts about this move for a long tim...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3058</th>\n",
              "      <td>42</td>\n",
              "      <td>87</td>\n",
              "      <td>White</td>\n",
              "      <td>g3g4</td>\n",
              "      <td>. . . . R . . .\\n. . . . P . k .\\n. . . . . . ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3059</th>\n",
              "      <td>42</td>\n",
              "      <td>88</td>\n",
              "      <td>Black</td>\n",
              "      <td>h5g4</td>\n",
              "      <td>. . . . R . . .\\n. . . . P . k .\\n. . . . . . ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3060</th>\n",
              "      <td>42</td>\n",
              "      <td>89</td>\n",
              "      <td>White</td>\n",
              "      <td>h3g4</td>\n",
              "      <td>. . . . R . . .\\n. . . . P . k .\\n. . . . . . ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3061</th>\n",
              "      <td>42</td>\n",
              "      <td>90</td>\n",
              "      <td>Black</td>\n",
              "      <td>g7f7</td>\n",
              "      <td>. . . . R . . .\\n. . . . P . k .\\n. . . . . . ...</td>\n",
              "      <td>Now that White is down to one rook pawn this i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3062</th>\n",
              "      <td>42</td>\n",
              "      <td>91</td>\n",
              "      <td>White</td>\n",
              "      <td>e8h8</td>\n",
              "      <td>. . . . R . . .\\n. . . . P k . .\\n. . . . . . ...</td>\n",
              "      <td>Good defense from Ivanchuk. He's played very w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3063 rows Ã— 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-18891883-1d83-4bf5-8d8b-15bb0ce7e0d5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-18891883-1d83-4bf5-8d8b-15bb0ce7e0d5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-18891883-1d83-4bf5-8d8b-15bb0ce7e0d5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6934325e-d19e-4997-b5cb-fa1a5d321683\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6934325e-d19e-4997-b5cb-fa1a5d321683')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6934325e-d19e-4997-b5cb-fa1a5d321683 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 3063,\n  \"fields\": [\n    {\n      \"column\": \"Game Number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12,\n        \"min\": 1,\n        \"max\": 42,\n        \"num_unique_values\": 42,\n        \"samples\": [\n          26,\n          14,\n          9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Move Number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28,\n        \"min\": 1,\n        \"max\": 151,\n        \"num_unique_values\": 151,\n        \"samples\": [\n          77,\n          19,\n          83\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Player\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Black\",\n          \"White\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Move\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 882,\n        \"samples\": [\n          \"e2f1\",\n          \"c5b5\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Board\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2606,\n        \"samples\": [\n          \". . r r . q . .\\np . . . . p k .\\n. p . . p n p p\\n. . . p . . . .\\n. n . P . P P .\\nP P N . P . . P\\n. R . Q . . B .\\n. . R . . . K .\",\n          \". . r r . q . .\\np . . . n p k .\\n. p . . p n p p\\n. . . p . . . .\\n. P . P . P P .\\nP . N . P . . P\\n. . R Q . . B .\\n. . R . . . K .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Comment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 725,\n        \"samples\": [\n          \"continuing to avoid ...Ng8-f6. In fact, some players use ...Ng8-e7, after ...a7-a6 chases Nb5 away.\",\n          \"The bad news is that the attack doesn't end. The f-file is open and Ne4 has good control of f6 and the h-file is open, so Kg8 still isn't safe after a queen trade. I suppose his worst frustration is that he's given a pawn and hoped to keep some material edge as compensation for his positional woes, but now he's just behind material and position. One of the things I liked most about this game is that Ivanchuk didn't force much. He let Vallejo Pons strangle on his own inability to develop and maneuver properly. Ivanchuk just kept flexible and attacked when he had specific targets and all his pieces involved. Chess looks so easy sometimes.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Piece encoding"
      ],
      "metadata": {
        "id": "SXUSFJO4xkhp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "4Wnt3sof0SHQ"
      },
      "outputs": [],
      "source": [
        "PIECES = {\n",
        "    \".\": 0,\n",
        "    \",\": 1,\n",
        "    \"P\": 2,\n",
        "    \"p\": 3,\n",
        "    \"R\": 4,\n",
        "    \"r\": 5,\n",
        "    \"N\": 6,\n",
        "    \"n\": 7,\n",
        "    \"B\": 8,\n",
        "    \"b\": 9,\n",
        "    \"Q\": 10,\n",
        "    \"q\": 11,\n",
        "    \"K\": 12,\n",
        "    \"k\": 13,\n",
        "}\n",
        "TURN = {\"Black\": 0, \"White\": 1}\n",
        "\n",
        "SQUARES = {\n",
        "    \"a8\": 0,\n",
        "    \"b8\": 1,\n",
        "    \"c8\": 2,\n",
        "    \"d8\": 3,\n",
        "    \"e8\": 4,\n",
        "    \"f8\": 5,\n",
        "    \"g8\": 6,\n",
        "    \"h8\": 7,\n",
        "    \"a7\": 8,\n",
        "    \"b7\": 9,\n",
        "    \"c7\": 10,\n",
        "    \"d7\": 11,\n",
        "    \"e7\": 12,\n",
        "    \"f7\": 13,\n",
        "    \"g7\": 14,\n",
        "    \"h7\": 15,\n",
        "    \"a6\": 16,\n",
        "    \"b6\": 17,\n",
        "    \"c6\": 18,\n",
        "    \"d6\": 19,\n",
        "    \"e6\": 20,\n",
        "    \"f6\": 21,\n",
        "    \"g6\": 22,\n",
        "    \"h6\": 23,\n",
        "    \"a5\": 24,\n",
        "    \"b5\": 25,\n",
        "    \"c5\": 26,\n",
        "    \"d5\": 27,\n",
        "    \"e5\": 28,\n",
        "    \"f5\": 29,\n",
        "    \"g5\": 30,\n",
        "    \"h5\": 31,\n",
        "    \"a4\": 32,\n",
        "    \"b4\": 33,\n",
        "    \"c4\": 34,\n",
        "    \"d4\": 35,\n",
        "    \"e4\": 36,\n",
        "    \"f4\": 37,\n",
        "    \"g4\": 38,\n",
        "    \"h4\": 39,\n",
        "    \"a3\": 40,\n",
        "    \"b3\": 41,\n",
        "    \"c3\": 42,\n",
        "    \"d3\": 43,\n",
        "    \"e3\": 44,\n",
        "    \"f3\": 45,\n",
        "    \"g3\": 46,\n",
        "    \"h3\": 47,\n",
        "    \"a2\": 48,\n",
        "    \"b2\": 49,\n",
        "    \"c2\": 50,\n",
        "    \"d2\": 51,\n",
        "    \"e2\": 52,\n",
        "    \"f2\": 53,\n",
        "    \"g2\": 54,\n",
        "    \"h2\": 55,\n",
        "    \"a1\": 56,\n",
        "    \"b1\": 57,\n",
        "    \"c1\": 58,\n",
        "    \"d1\": 59,\n",
        "    \"e1\": 60,\n",
        "    \"f1\": 61,\n",
        "    \"g1\": 62,\n",
        "    \"h1\": 63,\n",
        "}\n",
        "UCI_MOVES = {\n",
        "    \"a1h8\": 0,\n",
        "    \"a1a8\": 1,\n",
        "    \"a1g7\": 2,\n",
        "    \"a1a7\": 3,\n",
        "    \"a1f6\": 4,\n",
        "    \"a1a6\": 5,\n",
        "    \"a1e5\": 6,\n",
        "    \"a1a5\": 7,\n",
        "    \"a1d4\": 8,\n",
        "    \"a1a4\": 9,\n",
        "    \"a1c3\": 10,\n",
        "    \"a1a3\": 11,\n",
        "    \"a1b2\": 12,\n",
        "    \"a1a2\": 13,\n",
        "    \"a1h1\": 14,\n",
        "    \"a1g1\": 15,\n",
        "    \"a1f1\": 16,\n",
        "    \"a1e1\": 17,\n",
        "    \"a1d1\": 18,\n",
        "    \"a1c1\": 19,\n",
        "    \"a1b1\": 20,\n",
        "    \"a2g8\": 21,\n",
        "    \"a2a8\": 22,\n",
        "    \"a2f7\": 23,\n",
        "    \"a2a7\": 24,\n",
        "    \"a2e6\": 25,\n",
        "    \"a2a6\": 26,\n",
        "    \"a2d5\": 27,\n",
        "    \"a2a5\": 28,\n",
        "    \"a2c4\": 29,\n",
        "    \"a2a4\": 30,\n",
        "    \"a2b3\": 31,\n",
        "    \"a2a3\": 32,\n",
        "    \"a2h2\": 33,\n",
        "    \"a2g2\": 34,\n",
        "    \"a2f2\": 35,\n",
        "    \"a2e2\": 36,\n",
        "    \"a2d2\": 37,\n",
        "    \"a2c2\": 38,\n",
        "    \"a2b2\": 39,\n",
        "    \"a2b1\": 40,\n",
        "    \"a2a1\": 41,\n",
        "    \"a3f8\": 42,\n",
        "    \"a3a8\": 43,\n",
        "    \"a3e7\": 44,\n",
        "    \"a3a7\": 45,\n",
        "    \"a3d6\": 46,\n",
        "    \"a3a6\": 47,\n",
        "    \"a3c5\": 48,\n",
        "    \"a3a5\": 49,\n",
        "    \"a3b4\": 50,\n",
        "    \"a3a4\": 51,\n",
        "    \"a3h3\": 52,\n",
        "    \"a3g3\": 53,\n",
        "    \"a3f3\": 54,\n",
        "    \"a3e3\": 55,\n",
        "    \"a3d3\": 56,\n",
        "    \"a3c3\": 57,\n",
        "    \"a3b3\": 58,\n",
        "    \"a3b2\": 59,\n",
        "    \"a3a2\": 60,\n",
        "    \"a3c1\": 61,\n",
        "    \"a3a1\": 62,\n",
        "    \"a4e8\": 63,\n",
        "    \"a4a8\": 64,\n",
        "    \"a4d7\": 65,\n",
        "    \"a4a7\": 66,\n",
        "    \"a4c6\": 67,\n",
        "    \"a4a6\": 68,\n",
        "    \"a4b5\": 69,\n",
        "    \"a4a5\": 70,\n",
        "    \"a4h4\": 71,\n",
        "    \"a4g4\": 72,\n",
        "    \"a4f4\": 73,\n",
        "    \"a4e4\": 74,\n",
        "    \"a4d4\": 75,\n",
        "    \"a4c4\": 76,\n",
        "    \"a4b4\": 77,\n",
        "    \"a4b3\": 78,\n",
        "    \"a4a3\": 79,\n",
        "    \"a4c2\": 80,\n",
        "    \"a4a2\": 81,\n",
        "    \"a4d1\": 82,\n",
        "    \"a4a1\": 83,\n",
        "    \"a5d8\": 84,\n",
        "    \"a5a8\": 85,\n",
        "    \"a5c7\": 86,\n",
        "    \"a5a7\": 87,\n",
        "    \"a5b6\": 88,\n",
        "    \"a5a6\": 89,\n",
        "    \"a5h5\": 90,\n",
        "    \"a5g5\": 91,\n",
        "    \"a5f5\": 92,\n",
        "    \"a5e5\": 93,\n",
        "    \"a5d5\": 94,\n",
        "    \"a5c5\": 95,\n",
        "    \"a5b5\": 96,\n",
        "    \"a5b4\": 97,\n",
        "    \"a5a4\": 98,\n",
        "    \"a5c3\": 99,\n",
        "    \"a5a3\": 100,\n",
        "    \"a5d2\": 101,\n",
        "    \"a5a2\": 102,\n",
        "    \"a5e1\": 103,\n",
        "    \"a5a1\": 104,\n",
        "    \"a6c8\": 105,\n",
        "    \"a6a8\": 106,\n",
        "    \"a6b7\": 107,\n",
        "    \"a6a7\": 108,\n",
        "    \"a6h6\": 109,\n",
        "    \"a6g6\": 110,\n",
        "    \"a6f6\": 111,\n",
        "    \"a6e6\": 112,\n",
        "    \"a6d6\": 113,\n",
        "    \"a6c6\": 114,\n",
        "    \"a6b6\": 115,\n",
        "    \"a6b5\": 116,\n",
        "    \"a6a5\": 117,\n",
        "    \"a6c4\": 118,\n",
        "    \"a6a4\": 119,\n",
        "    \"a6d3\": 120,\n",
        "    \"a6a3\": 121,\n",
        "    \"a6e2\": 122,\n",
        "    \"a6a2\": 123,\n",
        "    \"a6f1\": 124,\n",
        "    \"a6a1\": 125,\n",
        "    \"a7b8\": 126,\n",
        "    \"a7a8\": 127,\n",
        "    \"a7h7\": 128,\n",
        "    \"a7g7\": 129,\n",
        "    \"a7f7\": 130,\n",
        "    \"a7e7\": 131,\n",
        "    \"a7d7\": 132,\n",
        "    \"a7c7\": 133,\n",
        "    \"a7b7\": 134,\n",
        "    \"a7b6\": 135,\n",
        "    \"a7a6\": 136,\n",
        "    \"a7c5\": 137,\n",
        "    \"a7a5\": 138,\n",
        "    \"a7d4\": 139,\n",
        "    \"a7a4\": 140,\n",
        "    \"a7e3\": 141,\n",
        "    \"a7a3\": 142,\n",
        "    \"a7f2\": 143,\n",
        "    \"a7a2\": 144,\n",
        "    \"a7g1\": 145,\n",
        "    \"a7a1\": 146,\n",
        "    \"a8h8\": 147,\n",
        "    \"a8g8\": 148,\n",
        "    \"a8f8\": 149,\n",
        "    \"a8e8\": 150,\n",
        "    \"a8d8\": 151,\n",
        "    \"a8c8\": 152,\n",
        "    \"a8b8\": 153,\n",
        "    \"a8b7\": 154,\n",
        "    \"a8a7\": 155,\n",
        "    \"a8c6\": 156,\n",
        "    \"a8a6\": 157,\n",
        "    \"a8d5\": 158,\n",
        "    \"a8a5\": 159,\n",
        "    \"a8e4\": 160,\n",
        "    \"a8a4\": 161,\n",
        "    \"a8f3\": 162,\n",
        "    \"a8a3\": 163,\n",
        "    \"a8g2\": 164,\n",
        "    \"a8a2\": 165,\n",
        "    \"a8h1\": 166,\n",
        "    \"a8a1\": 167,\n",
        "    \"b1b8\": 168,\n",
        "    \"b1h7\": 169,\n",
        "    \"b1b7\": 170,\n",
        "    \"b1g6\": 171,\n",
        "    \"b1b6\": 172,\n",
        "    \"b1f5\": 173,\n",
        "    \"b1b5\": 174,\n",
        "    \"b1e4\": 175,\n",
        "    \"b1b4\": 176,\n",
        "    \"b1d3\": 177,\n",
        "    \"b1b3\": 178,\n",
        "    \"b1c2\": 179,\n",
        "    \"b1b2\": 180,\n",
        "    \"b1a2\": 181,\n",
        "    \"b1h1\": 182,\n",
        "    \"b1g1\": 183,\n",
        "    \"b1f1\": 184,\n",
        "    \"b1e1\": 185,\n",
        "    \"b1d1\": 186,\n",
        "    \"b1c1\": 187,\n",
        "    \"b1a1\": 188,\n",
        "    \"b2h8\": 189,\n",
        "    \"b2b8\": 190,\n",
        "    \"b2g7\": 191,\n",
        "    \"b2b7\": 192,\n",
        "    \"b2f6\": 193,\n",
        "    \"b2b6\": 194,\n",
        "    \"b2e5\": 195,\n",
        "    \"b2b5\": 196,\n",
        "    \"b2d4\": 197,\n",
        "    \"b2b4\": 198,\n",
        "    \"b2c3\": 199,\n",
        "    \"b2b3\": 200,\n",
        "    \"b2a3\": 201,\n",
        "    \"b2h2\": 202,\n",
        "    \"b2g2\": 203,\n",
        "    \"b2f2\": 204,\n",
        "    \"b2e2\": 205,\n",
        "    \"b2d2\": 206,\n",
        "    \"b2c2\": 207,\n",
        "    \"b2a2\": 208,\n",
        "    \"b2c1\": 209,\n",
        "    \"b2b1\": 210,\n",
        "    \"b2a1\": 211,\n",
        "    \"b3g8\": 212,\n",
        "    \"b3b8\": 213,\n",
        "    \"b3f7\": 214,\n",
        "    \"b3b7\": 215,\n",
        "    \"b3e6\": 216,\n",
        "    \"b3b6\": 217,\n",
        "    \"b3d5\": 218,\n",
        "    \"b3b5\": 219,\n",
        "    \"b3c4\": 220,\n",
        "    \"b3b4\": 221,\n",
        "    \"b3a4\": 222,\n",
        "    \"b3h3\": 223,\n",
        "    \"b3g3\": 224,\n",
        "    \"b3f3\": 225,\n",
        "    \"b3e3\": 226,\n",
        "    \"b3d3\": 227,\n",
        "    \"b3c3\": 228,\n",
        "    \"b3a3\": 229,\n",
        "    \"b3c2\": 230,\n",
        "    \"b3b2\": 231,\n",
        "    \"b3a2\": 232,\n",
        "    \"b3d1\": 233,\n",
        "    \"b3b1\": 234,\n",
        "    \"b4f8\": 235,\n",
        "    \"b4b8\": 236,\n",
        "    \"b4e7\": 237,\n",
        "    \"b4b7\": 238,\n",
        "    \"b4d6\": 239,\n",
        "    \"b4b6\": 240,\n",
        "    \"b4c5\": 241,\n",
        "    \"b4b5\": 242,\n",
        "    \"b4a5\": 243,\n",
        "    \"b4h4\": 244,\n",
        "    \"b4g4\": 245,\n",
        "    \"b4f4\": 246,\n",
        "    \"b4e4\": 247,\n",
        "    \"b4d4\": 248,\n",
        "    \"b4c4\": 249,\n",
        "    \"b4a4\": 250,\n",
        "    \"b4c3\": 251,\n",
        "    \"b4b3\": 252,\n",
        "    \"b4a3\": 253,\n",
        "    \"b4d2\": 254,\n",
        "    \"b4b2\": 255,\n",
        "    \"b4e1\": 256,\n",
        "    \"b4b1\": 257,\n",
        "    \"b5e8\": 258,\n",
        "    \"b5b8\": 259,\n",
        "    \"b5d7\": 260,\n",
        "    \"b5b7\": 261,\n",
        "    \"b5c6\": 262,\n",
        "    \"b5b6\": 263,\n",
        "    \"b5a6\": 264,\n",
        "    \"b5h5\": 265,\n",
        "    \"b5g5\": 266,\n",
        "    \"b5f5\": 267,\n",
        "    \"b5e5\": 268,\n",
        "    \"b5d5\": 269,\n",
        "    \"b5c5\": 270,\n",
        "    \"b5a5\": 271,\n",
        "    \"b5c4\": 272,\n",
        "    \"b5b4\": 273,\n",
        "    \"b5a4\": 274,\n",
        "    \"b5d3\": 275,\n",
        "    \"b5b3\": 276,\n",
        "    \"b5e2\": 277,\n",
        "    \"b5b2\": 278,\n",
        "    \"b5f1\": 279,\n",
        "    \"b5b1\": 280,\n",
        "    \"b6d8\": 281,\n",
        "    \"b6b8\": 282,\n",
        "    \"b6c7\": 283,\n",
        "    \"b6b7\": 284,\n",
        "    \"b6a7\": 285,\n",
        "    \"b6h6\": 286,\n",
        "    \"b6g6\": 287,\n",
        "    \"b6f6\": 288,\n",
        "    \"b6e6\": 289,\n",
        "    \"b6d6\": 290,\n",
        "    \"b6c6\": 291,\n",
        "    \"b6a6\": 292,\n",
        "    \"b6c5\": 293,\n",
        "    \"b6b5\": 294,\n",
        "    \"b6a5\": 295,\n",
        "    \"b6d4\": 296,\n",
        "    \"b6b4\": 297,\n",
        "    \"b6e3\": 298,\n",
        "    \"b6b3\": 299,\n",
        "    \"b6f2\": 300,\n",
        "    \"b6b2\": 301,\n",
        "    \"b6g1\": 302,\n",
        "    \"b6b1\": 303,\n",
        "    \"b7c8\": 304,\n",
        "    \"b7b8\": 305,\n",
        "    \"b7a8\": 306,\n",
        "    \"b7h7\": 307,\n",
        "    \"b7g7\": 308,\n",
        "    \"b7f7\": 309,\n",
        "    \"b7e7\": 310,\n",
        "    \"b7d7\": 311,\n",
        "    \"b7c7\": 312,\n",
        "    \"b7a7\": 313,\n",
        "    \"b7c6\": 314,\n",
        "    \"b7b6\": 315,\n",
        "    \"b7a6\": 316,\n",
        "    \"b7d5\": 317,\n",
        "    \"b7b5\": 318,\n",
        "    \"b7e4\": 319,\n",
        "    \"b7b4\": 320,\n",
        "    \"b7f3\": 321,\n",
        "    \"b7b3\": 322,\n",
        "    \"b7g2\": 323,\n",
        "    \"b7b2\": 324,\n",
        "    \"b7h1\": 325,\n",
        "    \"b7b1\": 326,\n",
        "    \"b8h8\": 327,\n",
        "    \"b8g8\": 328,\n",
        "    \"b8f8\": 329,\n",
        "    \"b8e8\": 330,\n",
        "    \"b8d8\": 331,\n",
        "    \"b8c8\": 332,\n",
        "    \"b8a8\": 333,\n",
        "    \"b8c7\": 334,\n",
        "    \"b8b7\": 335,\n",
        "    \"b8a7\": 336,\n",
        "    \"b8d6\": 337,\n",
        "    \"b8b6\": 338,\n",
        "    \"b8e5\": 339,\n",
        "    \"b8b5\": 340,\n",
        "    \"b8f4\": 341,\n",
        "    \"b8b4\": 342,\n",
        "    \"b8g3\": 343,\n",
        "    \"b8b3\": 344,\n",
        "    \"b8h2\": 345,\n",
        "    \"b8b2\": 346,\n",
        "    \"b8b1\": 347,\n",
        "    \"c1c8\": 348,\n",
        "    \"c1c7\": 349,\n",
        "    \"c1h6\": 350,\n",
        "    \"c1c6\": 351,\n",
        "    \"c1g5\": 352,\n",
        "    \"c1c5\": 353,\n",
        "    \"c1f4\": 354,\n",
        "    \"c1c4\": 355,\n",
        "    \"c1e3\": 356,\n",
        "    \"c1c3\": 357,\n",
        "    \"c1a3\": 358,\n",
        "    \"c1d2\": 359,\n",
        "    \"c1c2\": 360,\n",
        "    \"c1b2\": 361,\n",
        "    \"c1h1\": 362,\n",
        "    \"c1g1\": 363,\n",
        "    \"c1f1\": 364,\n",
        "    \"c1e1\": 365,\n",
        "    \"c1d1\": 366,\n",
        "    \"c1b1\": 367,\n",
        "    \"c1a1\": 368,\n",
        "    \"c2c8\": 369,\n",
        "    \"c2h7\": 370,\n",
        "    \"c2c7\": 371,\n",
        "    \"c2g6\": 372,\n",
        "    \"c2c6\": 373,\n",
        "    \"c2f5\": 374,\n",
        "    \"c2c5\": 375,\n",
        "    \"c2e4\": 376,\n",
        "    \"c2c4\": 377,\n",
        "    \"c2a4\": 378,\n",
        "    \"c2d3\": 379,\n",
        "    \"c2c3\": 380,\n",
        "    \"c2b3\": 381,\n",
        "    \"c2h2\": 382,\n",
        "    \"c2g2\": 383,\n",
        "    \"c2f2\": 384,\n",
        "    \"c2e2\": 385,\n",
        "    \"c2d2\": 386,\n",
        "    \"c2b2\": 387,\n",
        "    \"c2a2\": 388,\n",
        "    \"c2d1\": 389,\n",
        "    \"c2c1\": 390,\n",
        "    \"c2b1\": 391,\n",
        "    \"c3h8\": 392,\n",
        "    \"c3c8\": 393,\n",
        "    \"c3g7\": 394,\n",
        "    \"c3c7\": 395,\n",
        "    \"c3f6\": 396,\n",
        "    \"c3c6\": 397,\n",
        "    \"c3e5\": 398,\n",
        "    \"c3c5\": 399,\n",
        "    \"c3a5\": 400,\n",
        "    \"c3d4\": 401,\n",
        "    \"c3c4\": 402,\n",
        "    \"c3b4\": 403,\n",
        "    \"c3h3\": 404,\n",
        "    \"c3g3\": 405,\n",
        "    \"c3f3\": 406,\n",
        "    \"c3e3\": 407,\n",
        "    \"c3d3\": 408,\n",
        "    \"c3b3\": 409,\n",
        "    \"c3a3\": 410,\n",
        "    \"c3d2\": 411,\n",
        "    \"c3c2\": 412,\n",
        "    \"c3b2\": 413,\n",
        "    \"c3e1\": 414,\n",
        "    \"c3c1\": 415,\n",
        "    \"c3a1\": 416,\n",
        "    \"c4g8\": 417,\n",
        "    \"c4c8\": 418,\n",
        "    \"c4f7\": 419,\n",
        "    \"c4c7\": 420,\n",
        "    \"c4e6\": 421,\n",
        "    \"c4c6\": 422,\n",
        "    \"c4a6\": 423,\n",
        "    \"c4d5\": 424,\n",
        "    \"c4c5\": 425,\n",
        "    \"c4b5\": 426,\n",
        "    \"c4h4\": 427,\n",
        "    \"c4g4\": 428,\n",
        "    \"c4f4\": 429,\n",
        "    \"c4e4\": 430,\n",
        "    \"c4d4\": 431,\n",
        "    \"c4b4\": 432,\n",
        "    \"c4a4\": 433,\n",
        "    \"c4d3\": 434,\n",
        "    \"c4c3\": 435,\n",
        "    \"c4b3\": 436,\n",
        "    \"c4e2\": 437,\n",
        "    \"c4c2\": 438,\n",
        "    \"c4a2\": 439,\n",
        "    \"c4f1\": 440,\n",
        "    \"c4c1\": 441,\n",
        "    \"c5f8\": 442,\n",
        "    \"c5c8\": 443,\n",
        "    \"c5e7\": 444,\n",
        "    \"c5c7\": 445,\n",
        "    \"c5a7\": 446,\n",
        "    \"c5d6\": 447,\n",
        "    \"c5c6\": 448,\n",
        "    \"c5b6\": 449,\n",
        "    \"c5h5\": 450,\n",
        "    \"c5g5\": 451,\n",
        "    \"c5f5\": 452,\n",
        "    \"c5e5\": 453,\n",
        "    \"c5d5\": 454,\n",
        "    \"c5b5\": 455,\n",
        "    \"c5a5\": 456,\n",
        "    \"c5d4\": 457,\n",
        "    \"c5c4\": 458,\n",
        "    \"c5b4\": 459,\n",
        "    \"c5e3\": 460,\n",
        "    \"c5c3\": 461,\n",
        "    \"c5a3\": 462,\n",
        "    \"c5f2\": 463,\n",
        "    \"c5c2\": 464,\n",
        "    \"c5g1\": 465,\n",
        "    \"c5c1\": 466,\n",
        "    \"c6e8\": 467,\n",
        "    \"c6c8\": 468,\n",
        "    \"c6a8\": 469,\n",
        "    \"c6d7\": 470,\n",
        "    \"c6c7\": 471,\n",
        "    \"c6b7\": 472,\n",
        "    \"c6h6\": 473,\n",
        "    \"c6g6\": 474,\n",
        "    \"c6f6\": 475,\n",
        "    \"c6e6\": 476,\n",
        "    \"c6d6\": 477,\n",
        "    \"c6b6\": 478,\n",
        "    \"c6a6\": 479,\n",
        "    \"c6d5\": 480,\n",
        "    \"c6c5\": 481,\n",
        "    \"c6b5\": 482,\n",
        "    \"c6e4\": 483,\n",
        "    \"c6c4\": 484,\n",
        "    \"c6a4\": 485,\n",
        "    \"c6f3\": 486,\n",
        "    \"c6c3\": 487,\n",
        "    \"c6g2\": 488,\n",
        "    \"c6c2\": 489,\n",
        "    \"c6h1\": 490,\n",
        "    \"c6c1\": 491,\n",
        "    \"c7d8\": 492,\n",
        "    \"c7c8\": 493,\n",
        "    \"c7b8\": 494,\n",
        "    \"c7h7\": 495,\n",
        "    \"c7g7\": 496,\n",
        "    \"c7f7\": 497,\n",
        "    \"c7e7\": 498,\n",
        "    \"c7d7\": 499,\n",
        "    \"c7b7\": 500,\n",
        "    \"c7a7\": 501,\n",
        "    \"c7d6\": 502,\n",
        "    \"c7c6\": 503,\n",
        "    \"c7b6\": 504,\n",
        "    \"c7e5\": 505,\n",
        "    \"c7c5\": 506,\n",
        "    \"c7a5\": 507,\n",
        "    \"c7f4\": 508,\n",
        "    \"c7c4\": 509,\n",
        "    \"c7g3\": 510,\n",
        "    \"c7c3\": 511,\n",
        "    \"c7h2\": 512,\n",
        "    \"c7c2\": 513,\n",
        "    \"c7c1\": 514,\n",
        "    \"c8h8\": 515,\n",
        "    \"c8g8\": 516,\n",
        "    \"c8f8\": 517,\n",
        "    \"c8e8\": 518,\n",
        "    \"c8d8\": 519,\n",
        "    \"c8b8\": 520,\n",
        "    \"c8a8\": 521,\n",
        "    \"c8d7\": 522,\n",
        "    \"c8c7\": 523,\n",
        "    \"c8b7\": 524,\n",
        "    \"c8e6\": 525,\n",
        "    \"c8c6\": 526,\n",
        "    \"c8a6\": 527,\n",
        "    \"c8f5\": 528,\n",
        "    \"c8c5\": 529,\n",
        "    \"c8g4\": 530,\n",
        "    \"c8c4\": 531,\n",
        "    \"c8h3\": 532,\n",
        "    \"c8c3\": 533,\n",
        "    \"c8c2\": 534,\n",
        "    \"c8c1\": 535,\n",
        "    \"d1d8\": 536,\n",
        "    \"d1d7\": 537,\n",
        "    \"d1d6\": 538,\n",
        "    \"d1h5\": 539,\n",
        "    \"d1d5\": 540,\n",
        "    \"d1g4\": 541,\n",
        "    \"d1d4\": 542,\n",
        "    \"d1a4\": 543,\n",
        "    \"d1f3\": 544,\n",
        "    \"d1d3\": 545,\n",
        "    \"d1b3\": 546,\n",
        "    \"d1e2\": 547,\n",
        "    \"d1d2\": 548,\n",
        "    \"d1c2\": 549,\n",
        "    \"d1h1\": 550,\n",
        "    \"d1g1\": 551,\n",
        "    \"d1f1\": 552,\n",
        "    \"d1e1\": 553,\n",
        "    \"d1c1\": 554,\n",
        "    \"d1b1\": 555,\n",
        "    \"d1a1\": 556,\n",
        "    \"d2d8\": 557,\n",
        "    \"d2d7\": 558,\n",
        "    \"d2h6\": 559,\n",
        "    \"d2d6\": 560,\n",
        "    \"d2g5\": 561,\n",
        "    \"d2d5\": 562,\n",
        "    \"d2a5\": 563,\n",
        "    \"d2f4\": 564,\n",
        "    \"d2d4\": 565,\n",
        "    \"d2b4\": 566,\n",
        "    \"d2e3\": 567,\n",
        "    \"d2d3\": 568,\n",
        "    \"d2c3\": 569,\n",
        "    \"d2h2\": 570,\n",
        "    \"d2g2\": 571,\n",
        "    \"d2f2\": 572,\n",
        "    \"d2e2\": 573,\n",
        "    \"d2c2\": 574,\n",
        "    \"d2b2\": 575,\n",
        "    \"d2a2\": 576,\n",
        "    \"d2e1\": 577,\n",
        "    \"d2d1\": 578,\n",
        "    \"d2c1\": 579,\n",
        "    \"d3d8\": 580,\n",
        "    \"d3h7\": 581,\n",
        "    \"d3d7\": 582,\n",
        "    \"d3g6\": 583,\n",
        "    \"d3d6\": 584,\n",
        "    \"d3a6\": 585,\n",
        "    \"d3f5\": 586,\n",
        "    \"d3d5\": 587,\n",
        "    \"d3b5\": 588,\n",
        "    \"d3e4\": 589,\n",
        "    \"d3d4\": 590,\n",
        "    \"d3c4\": 591,\n",
        "    \"d3h3\": 592,\n",
        "    \"d3g3\": 593,\n",
        "    \"d3f3\": 594,\n",
        "    \"d3e3\": 595,\n",
        "    \"d3c3\": 596,\n",
        "    \"d3b3\": 597,\n",
        "    \"d3a3\": 598,\n",
        "    \"d3e2\": 599,\n",
        "    \"d3d2\": 600,\n",
        "    \"d3c2\": 601,\n",
        "    \"d3f1\": 602,\n",
        "    \"d3d1\": 603,\n",
        "    \"d3b1\": 604,\n",
        "    \"d4h8\": 605,\n",
        "    \"d4d8\": 606,\n",
        "    \"d4g7\": 607,\n",
        "    \"d4d7\": 608,\n",
        "    \"d4a7\": 609,\n",
        "    \"d4f6\": 610,\n",
        "    \"d4d6\": 611,\n",
        "    \"d4b6\": 612,\n",
        "    \"d4e5\": 613,\n",
        "    \"d4d5\": 614,\n",
        "    \"d4c5\": 615,\n",
        "    \"d4h4\": 616,\n",
        "    \"d4g4\": 617,\n",
        "    \"d4f4\": 618,\n",
        "    \"d4e4\": 619,\n",
        "    \"d4c4\": 620,\n",
        "    \"d4b4\": 621,\n",
        "    \"d4a4\": 622,\n",
        "    \"d4e3\": 623,\n",
        "    \"d4d3\": 624,\n",
        "    \"d4c3\": 625,\n",
        "    \"d4f2\": 626,\n",
        "    \"d4d2\": 627,\n",
        "    \"d4b2\": 628,\n",
        "    \"d4g1\": 629,\n",
        "    \"d4d1\": 630,\n",
        "    \"d4a1\": 631,\n",
        "    \"d5g8\": 632,\n",
        "    \"d5d8\": 633,\n",
        "    \"d5a8\": 634,\n",
        "    \"d5f7\": 635,\n",
        "    \"d5d7\": 636,\n",
        "    \"d5b7\": 637,\n",
        "    \"d5e6\": 638,\n",
        "    \"d5d6\": 639,\n",
        "    \"d5c6\": 640,\n",
        "    \"d5h5\": 641,\n",
        "    \"d5g5\": 642,\n",
        "    \"d5f5\": 643,\n",
        "    \"d5e5\": 644,\n",
        "    \"d5c5\": 645,\n",
        "    \"d5b5\": 646,\n",
        "    \"d5a5\": 647,\n",
        "    \"d5e4\": 648,\n",
        "    \"d5d4\": 649,\n",
        "    \"d5c4\": 650,\n",
        "    \"d5f3\": 651,\n",
        "    \"d5d3\": 652,\n",
        "    \"d5b3\": 653,\n",
        "    \"d5g2\": 654,\n",
        "    \"d5d2\": 655,\n",
        "    \"d5a2\": 656,\n",
        "    \"d5h1\": 657,\n",
        "    \"d5d1\": 658,\n",
        "    \"d6f8\": 659,\n",
        "    \"d6d8\": 660,\n",
        "    \"d6b8\": 661,\n",
        "    \"d6e7\": 662,\n",
        "    \"d6d7\": 663,\n",
        "    \"d6c7\": 664,\n",
        "    \"d6h6\": 665,\n",
        "    \"d6g6\": 666,\n",
        "    \"d6f6\": 667,\n",
        "    \"d6e6\": 668,\n",
        "    \"d6c6\": 669,\n",
        "    \"d6b6\": 670,\n",
        "    \"d6a6\": 671,\n",
        "    \"d6e5\": 672,\n",
        "    \"d6d5\": 673,\n",
        "    \"d6c5\": 674,\n",
        "    \"d6f4\": 675,\n",
        "    \"d6d4\": 676,\n",
        "    \"d6b4\": 677,\n",
        "    \"d6g3\": 678,\n",
        "    \"d6d3\": 679,\n",
        "    \"d6a3\": 680,\n",
        "    \"d6h2\": 681,\n",
        "    \"d6d2\": 682,\n",
        "    \"d6d1\": 683,\n",
        "    \"d7e8\": 684,\n",
        "    \"d7d8\": 685,\n",
        "    \"d7c8\": 686,\n",
        "    \"d7h7\": 687,\n",
        "    \"d7g7\": 688,\n",
        "    \"d7f7\": 689,\n",
        "    \"d7e7\": 690,\n",
        "    \"d7c7\": 691,\n",
        "    \"d7b7\": 692,\n",
        "    \"d7a7\": 693,\n",
        "    \"d7e6\": 694,\n",
        "    \"d7d6\": 695,\n",
        "    \"d7c6\": 696,\n",
        "    \"d7f5\": 697,\n",
        "    \"d7d5\": 698,\n",
        "    \"d7b5\": 699,\n",
        "    \"d7g4\": 700,\n",
        "    \"d7d4\": 701,\n",
        "    \"d7a4\": 702,\n",
        "    \"d7h3\": 703,\n",
        "    \"d7d3\": 704,\n",
        "    \"d7d2\": 705,\n",
        "    \"d7d1\": 706,\n",
        "    \"d8h8\": 707,\n",
        "    \"d8g8\": 708,\n",
        "    \"d8f8\": 709,\n",
        "    \"d8e8\": 710,\n",
        "    \"d8c8\": 711,\n",
        "    \"d8b8\": 712,\n",
        "    \"d8a8\": 713,\n",
        "    \"d8e7\": 714,\n",
        "    \"d8d7\": 715,\n",
        "    \"d8c7\": 716,\n",
        "    \"d8f6\": 717,\n",
        "    \"d8d6\": 718,\n",
        "    \"d8b6\": 719,\n",
        "    \"d8g5\": 720,\n",
        "    \"d8d5\": 721,\n",
        "    \"d8a5\": 722,\n",
        "    \"d8h4\": 723,\n",
        "    \"d8d4\": 724,\n",
        "    \"d8d3\": 725,\n",
        "    \"d8d2\": 726,\n",
        "    \"d8d1\": 727,\n",
        "    \"e1e8\": 728,\n",
        "    \"e1e7\": 729,\n",
        "    \"e1e6\": 730,\n",
        "    \"e1e5\": 731,\n",
        "    \"e1a5\": 732,\n",
        "    \"e1h4\": 733,\n",
        "    \"e1e4\": 734,\n",
        "    \"e1b4\": 735,\n",
        "    \"e1g3\": 736,\n",
        "    \"e1e3\": 737,\n",
        "    \"e1c3\": 738,\n",
        "    \"e1f2\": 739,\n",
        "    \"e1e2\": 740,\n",
        "    \"e1d2\": 741,\n",
        "    \"e1h1\": 742,\n",
        "    \"e1g1\": 743,\n",
        "    \"e1f1\": 744,\n",
        "    \"e1d1\": 745,\n",
        "    \"e1c1\": 746,\n",
        "    \"e1b1\": 747,\n",
        "    \"e1a1\": 748,\n",
        "    \"e2e8\": 749,\n",
        "    \"e2e7\": 750,\n",
        "    \"e2e6\": 751,\n",
        "    \"e2a6\": 752,\n",
        "    \"e2h5\": 753,\n",
        "    \"e2e5\": 754,\n",
        "    \"e2b5\": 755,\n",
        "    \"e2g4\": 756,\n",
        "    \"e2e4\": 757,\n",
        "    \"e2c4\": 758,\n",
        "    \"e2f3\": 759,\n",
        "    \"e2e3\": 760,\n",
        "    \"e2d3\": 761,\n",
        "    \"e2h2\": 762,\n",
        "    \"e2g2\": 763,\n",
        "    \"e2f2\": 764,\n",
        "    \"e2d2\": 765,\n",
        "    \"e2c2\": 766,\n",
        "    \"e2b2\": 767,\n",
        "    \"e2a2\": 768,\n",
        "    \"e2f1\": 769,\n",
        "    \"e2e1\": 770,\n",
        "    \"e2d1\": 771,\n",
        "    \"e3e8\": 772,\n",
        "    \"e3e7\": 773,\n",
        "    \"e3a7\": 774,\n",
        "    \"e3h6\": 775,\n",
        "    \"e3e6\": 776,\n",
        "    \"e3b6\": 777,\n",
        "    \"e3g5\": 778,\n",
        "    \"e3e5\": 779,\n",
        "    \"e3c5\": 780,\n",
        "    \"e3f4\": 781,\n",
        "    \"e3e4\": 782,\n",
        "    \"e3d4\": 783,\n",
        "    \"e3h3\": 784,\n",
        "    \"e3g3\": 785,\n",
        "    \"e3f3\": 786,\n",
        "    \"e3d3\": 787,\n",
        "    \"e3c3\": 788,\n",
        "    \"e3b3\": 789,\n",
        "    \"e3a3\": 790,\n",
        "    \"e3f2\": 791,\n",
        "    \"e3e2\": 792,\n",
        "    \"e3d2\": 793,\n",
        "    \"e3g1\": 794,\n",
        "    \"e3e1\": 795,\n",
        "    \"e3c1\": 796,\n",
        "    \"e4e8\": 797,\n",
        "    \"e4a8\": 798,\n",
        "    \"e4h7\": 799,\n",
        "    \"e4e7\": 800,\n",
        "    \"e4b7\": 801,\n",
        "    \"e4g6\": 802,\n",
        "    \"e4e6\": 803,\n",
        "    \"e4c6\": 804,\n",
        "    \"e4f5\": 805,\n",
        "    \"e4e5\": 806,\n",
        "    \"e4d5\": 807,\n",
        "    \"e4h4\": 808,\n",
        "    \"e4g4\": 809,\n",
        "    \"e4f4\": 810,\n",
        "    \"e4d4\": 811,\n",
        "    \"e4c4\": 812,\n",
        "    \"e4b4\": 813,\n",
        "    \"e4a4\": 814,\n",
        "    \"e4f3\": 815,\n",
        "    \"e4e3\": 816,\n",
        "    \"e4d3\": 817,\n",
        "    \"e4g2\": 818,\n",
        "    \"e4e2\": 819,\n",
        "    \"e4c2\": 820,\n",
        "    \"e4h1\": 821,\n",
        "    \"e4e1\": 822,\n",
        "    \"e4b1\": 823,\n",
        "    \"e5h8\": 824,\n",
        "    \"e5e8\": 825,\n",
        "    \"e5b8\": 826,\n",
        "    \"e5g7\": 827,\n",
        "    \"e5e7\": 828,\n",
        "    \"e5c7\": 829,\n",
        "    \"e5f6\": 830,\n",
        "    \"e5e6\": 831,\n",
        "    \"e5d6\": 832,\n",
        "    \"e5h5\": 833,\n",
        "    \"e5g5\": 834,\n",
        "    \"e5f5\": 835,\n",
        "    \"e5d5\": 836,\n",
        "    \"e5c5\": 837,\n",
        "    \"e5b5\": 838,\n",
        "    \"e5a5\": 839,\n",
        "    \"e5f4\": 840,\n",
        "    \"e5e4\": 841,\n",
        "    \"e5d4\": 842,\n",
        "    \"e5g3\": 843,\n",
        "    \"e5e3\": 844,\n",
        "    \"e5c3\": 845,\n",
        "    \"e5h2\": 846,\n",
        "    \"e5e2\": 847,\n",
        "    \"e5b2\": 848,\n",
        "    \"e5e1\": 849,\n",
        "    \"e5a1\": 850,\n",
        "    \"e6g8\": 851,\n",
        "    \"e6e8\": 852,\n",
        "    \"e6c8\": 853,\n",
        "    \"e6f7\": 854,\n",
        "    \"e6e7\": 855,\n",
        "    \"e6d7\": 856,\n",
        "    \"e6h6\": 857,\n",
        "    \"e6g6\": 858,\n",
        "    \"e6f6\": 859,\n",
        "    \"e6d6\": 860,\n",
        "    \"e6c6\": 861,\n",
        "    \"e6b6\": 862,\n",
        "    \"e6a6\": 863,\n",
        "    \"e6f5\": 864,\n",
        "    \"e6e5\": 865,\n",
        "    \"e6d5\": 866,\n",
        "    \"e6g4\": 867,\n",
        "    \"e6e4\": 868,\n",
        "    \"e6c4\": 869,\n",
        "    \"e6h3\": 870,\n",
        "    \"e6e3\": 871,\n",
        "    \"e6b3\": 872,\n",
        "    \"e6e2\": 873,\n",
        "    \"e6a2\": 874,\n",
        "    \"e6e1\": 875,\n",
        "    \"e7f8\": 876,\n",
        "    \"e7e8\": 877,\n",
        "    \"e7d8\": 878,\n",
        "    \"e7h7\": 879,\n",
        "    \"e7g7\": 880,\n",
        "    \"e7f7\": 881,\n",
        "    \"e7d7\": 882,\n",
        "    \"e7c7\": 883,\n",
        "    \"e7b7\": 884,\n",
        "    \"e7a7\": 885,\n",
        "    \"e7f6\": 886,\n",
        "    \"e7e6\": 887,\n",
        "    \"e7d6\": 888,\n",
        "    \"e7g5\": 889,\n",
        "    \"e7e5\": 890,\n",
        "    \"e7c5\": 891,\n",
        "    \"e7h4\": 892,\n",
        "    \"e7e4\": 893,\n",
        "    \"e7b4\": 894,\n",
        "    \"e7e3\": 895,\n",
        "    \"e7a3\": 896,\n",
        "    \"e7e2\": 897,\n",
        "    \"e7e1\": 898,\n",
        "    \"e8h8\": 899,\n",
        "    \"e8g8\": 900,\n",
        "    \"e8f8\": 901,\n",
        "    \"e8d8\": 902,\n",
        "    \"e8c8\": 903,\n",
        "    \"e8b8\": 904,\n",
        "    \"e8a8\": 905,\n",
        "    \"e8f7\": 906,\n",
        "    \"e8e7\": 907,\n",
        "    \"e8d7\": 908,\n",
        "    \"e8g6\": 909,\n",
        "    \"e8e6\": 910,\n",
        "    \"e8c6\": 911,\n",
        "    \"e8h5\": 912,\n",
        "    \"e8e5\": 913,\n",
        "    \"e8b5\": 914,\n",
        "    \"e8e4\": 915,\n",
        "    \"e8a4\": 916,\n",
        "    \"e8e3\": 917,\n",
        "    \"e8e2\": 918,\n",
        "    \"e8e1\": 919,\n",
        "    \"f1f8\": 920,\n",
        "    \"f1f7\": 921,\n",
        "    \"f1f6\": 922,\n",
        "    \"f1a6\": 923,\n",
        "    \"f1f5\": 924,\n",
        "    \"f1b5\": 925,\n",
        "    \"f1f4\": 926,\n",
        "    \"f1c4\": 927,\n",
        "    \"f1h3\": 928,\n",
        "    \"f1f3\": 929,\n",
        "    \"f1d3\": 930,\n",
        "    \"f1g2\": 931,\n",
        "    \"f1f2\": 932,\n",
        "    \"f1e2\": 933,\n",
        "    \"f1h1\": 934,\n",
        "    \"f1g1\": 935,\n",
        "    \"f1e1\": 936,\n",
        "    \"f1d1\": 937,\n",
        "    \"f1c1\": 938,\n",
        "    \"f1b1\": 939,\n",
        "    \"f1a1\": 940,\n",
        "    \"f2f8\": 941,\n",
        "    \"f2f7\": 942,\n",
        "    \"f2a7\": 943,\n",
        "    \"f2f6\": 944,\n",
        "    \"f2b6\": 945,\n",
        "    \"f2f5\": 946,\n",
        "    \"f2c5\": 947,\n",
        "    \"f2h4\": 948,\n",
        "    \"f2f4\": 949,\n",
        "    \"f2d4\": 950,\n",
        "    \"f2g3\": 951,\n",
        "    \"f2f3\": 952,\n",
        "    \"f2e3\": 953,\n",
        "    \"f2h2\": 954,\n",
        "    \"f2g2\": 955,\n",
        "    \"f2e2\": 956,\n",
        "    \"f2d2\": 957,\n",
        "    \"f2c2\": 958,\n",
        "    \"f2b2\": 959,\n",
        "    \"f2a2\": 960,\n",
        "    \"f2g1\": 961,\n",
        "    \"f2f1\": 962,\n",
        "    \"f2e1\": 963,\n",
        "    \"f3f8\": 964,\n",
        "    \"f3a8\": 965,\n",
        "    \"f3f7\": 966,\n",
        "    \"f3b7\": 967,\n",
        "    \"f3f6\": 968,\n",
        "    \"f3c6\": 969,\n",
        "    \"f3h5\": 970,\n",
        "    \"f3f5\": 971,\n",
        "    \"f3d5\": 972,\n",
        "    \"f3g4\": 973,\n",
        "    \"f3f4\": 974,\n",
        "    \"f3e4\": 975,\n",
        "    \"f3h3\": 976,\n",
        "    \"f3g3\": 977,\n",
        "    \"f3e3\": 978,\n",
        "    \"f3d3\": 979,\n",
        "    \"f3c3\": 980,\n",
        "    \"f3b3\": 981,\n",
        "    \"f3a3\": 982,\n",
        "    \"f3g2\": 983,\n",
        "    \"f3f2\": 984,\n",
        "    \"f3e2\": 985,\n",
        "    \"f3h1\": 986,\n",
        "    \"f3f1\": 987,\n",
        "    \"f3d1\": 988,\n",
        "    \"f4f8\": 989,\n",
        "    \"f4b8\": 990,\n",
        "    \"f4f7\": 991,\n",
        "    \"f4c7\": 992,\n",
        "    \"f4h6\": 993,\n",
        "    \"f4f6\": 994,\n",
        "    \"f4d6\": 995,\n",
        "    \"f4g5\": 996,\n",
        "    \"f4f5\": 997,\n",
        "    \"f4e5\": 998,\n",
        "    \"f4h4\": 999,\n",
        "    \"f4g4\": 1000,\n",
        "    \"f4e4\": 1001,\n",
        "    \"f4d4\": 1002,\n",
        "    \"f4c4\": 1003,\n",
        "    \"f4b4\": 1004,\n",
        "    \"f4a4\": 1005,\n",
        "    \"f4g3\": 1006,\n",
        "    \"f4f3\": 1007,\n",
        "    \"f4e3\": 1008,\n",
        "    \"f4h2\": 1009,\n",
        "    \"f4f2\": 1010,\n",
        "    \"f4d2\": 1011,\n",
        "    \"f4f1\": 1012,\n",
        "    \"f4c1\": 1013,\n",
        "    \"f5f8\": 1014,\n",
        "    \"f5c8\": 1015,\n",
        "    \"f5h7\": 1016,\n",
        "    \"f5f7\": 1017,\n",
        "    \"f5d7\": 1018,\n",
        "    \"f5g6\": 1019,\n",
        "    \"f5f6\": 1020,\n",
        "    \"f5e6\": 1021,\n",
        "    \"f5h5\": 1022,\n",
        "    \"f5g5\": 1023,\n",
        "    \"f5e5\": 1024,\n",
        "    \"f5d5\": 1025,\n",
        "    \"f5c5\": 1026,\n",
        "    \"f5b5\": 1027,\n",
        "    \"f5a5\": 1028,\n",
        "    \"f5g4\": 1029,\n",
        "    \"f5f4\": 1030,\n",
        "    \"f5e4\": 1031,\n",
        "    \"f5h3\": 1032,\n",
        "    \"f5f3\": 1033,\n",
        "    \"f5d3\": 1034,\n",
        "    \"f5f2\": 1035,\n",
        "    \"f5c2\": 1036,\n",
        "    \"f5f1\": 1037,\n",
        "    \"f5b1\": 1038,\n",
        "    \"f6h8\": 1039,\n",
        "    \"f6f8\": 1040,\n",
        "    \"f6d8\": 1041,\n",
        "    \"f6g7\": 1042,\n",
        "    \"f6f7\": 1043,\n",
        "    \"f6e7\": 1044,\n",
        "    \"f6h6\": 1045,\n",
        "    \"f6g6\": 1046,\n",
        "    \"f6e6\": 1047,\n",
        "    \"f6d6\": 1048,\n",
        "    \"f6c6\": 1049,\n",
        "    \"f6b6\": 1050,\n",
        "    \"f6a6\": 1051,\n",
        "    \"f6g5\": 1052,\n",
        "    \"f6f5\": 1053,\n",
        "    \"f6e5\": 1054,\n",
        "    \"f6h4\": 1055,\n",
        "    \"f6f4\": 1056,\n",
        "    \"f6d4\": 1057,\n",
        "    \"f6f3\": 1058,\n",
        "    \"f6c3\": 1059,\n",
        "    \"f6f2\": 1060,\n",
        "    \"f6b2\": 1061,\n",
        "    \"f6f1\": 1062,\n",
        "    \"f6a1\": 1063,\n",
        "    \"f7g8\": 1064,\n",
        "    \"f7f8\": 1065,\n",
        "    \"f7e8\": 1066,\n",
        "    \"f7h7\": 1067,\n",
        "    \"f7g7\": 1068,\n",
        "    \"f7e7\": 1069,\n",
        "    \"f7d7\": 1070,\n",
        "    \"f7c7\": 1071,\n",
        "    \"f7b7\": 1072,\n",
        "    \"f7a7\": 1073,\n",
        "    \"f7g6\": 1074,\n",
        "    \"f7f6\": 1075,\n",
        "    \"f7e6\": 1076,\n",
        "    \"f7h5\": 1077,\n",
        "    \"f7f5\": 1078,\n",
        "    \"f7d5\": 1079,\n",
        "    \"f7f4\": 1080,\n",
        "    \"f7c4\": 1081,\n",
        "    \"f7f3\": 1082,\n",
        "    \"f7b3\": 1083,\n",
        "    \"f7f2\": 1084,\n",
        "    \"f7a2\": 1085,\n",
        "    \"f7f1\": 1086,\n",
        "    \"f8h8\": 1087,\n",
        "    \"f8g8\": 1088,\n",
        "    \"f8e8\": 1089,\n",
        "    \"f8d8\": 1090,\n",
        "    \"f8c8\": 1091,\n",
        "    \"f8b8\": 1092,\n",
        "    \"f8a8\": 1093,\n",
        "    \"f8g7\": 1094,\n",
        "    \"f8f7\": 1095,\n",
        "    \"f8e7\": 1096,\n",
        "    \"f8h6\": 1097,\n",
        "    \"f8f6\": 1098,\n",
        "    \"f8d6\": 1099,\n",
        "    \"f8f5\": 1100,\n",
        "    \"f8c5\": 1101,\n",
        "    \"f8f4\": 1102,\n",
        "    \"f8b4\": 1103,\n",
        "    \"f8f3\": 1104,\n",
        "    \"f8a3\": 1105,\n",
        "    \"f8f2\": 1106,\n",
        "    \"f8f1\": 1107,\n",
        "    \"g1g8\": 1108,\n",
        "    \"g1g7\": 1109,\n",
        "    \"g1a7\": 1110,\n",
        "    \"g1g6\": 1111,\n",
        "    \"g1b6\": 1112,\n",
        "    \"g1g5\": 1113,\n",
        "    \"g1c5\": 1114,\n",
        "    \"g1g4\": 1115,\n",
        "    \"g1d4\": 1116,\n",
        "    \"g1g3\": 1117,\n",
        "    \"g1e3\": 1118,\n",
        "    \"g1h2\": 1119,\n",
        "    \"g1g2\": 1120,\n",
        "    \"g1f2\": 1121,\n",
        "    \"g1h1\": 1122,\n",
        "    \"g1f1\": 1123,\n",
        "    \"g1e1\": 1124,\n",
        "    \"g1d1\": 1125,\n",
        "    \"g1c1\": 1126,\n",
        "    \"g1b1\": 1127,\n",
        "    \"g1a1\": 1128,\n",
        "    \"g2g8\": 1129,\n",
        "    \"g2a8\": 1130,\n",
        "    \"g2g7\": 1131,\n",
        "    \"g2b7\": 1132,\n",
        "    \"g2g6\": 1133,\n",
        "    \"g2c6\": 1134,\n",
        "    \"g2g5\": 1135,\n",
        "    \"g2d5\": 1136,\n",
        "    \"g2g4\": 1137,\n",
        "    \"g2e4\": 1138,\n",
        "    \"g2h3\": 1139,\n",
        "    \"g2g3\": 1140,\n",
        "    \"g2f3\": 1141,\n",
        "    \"g2h2\": 1142,\n",
        "    \"g2f2\": 1143,\n",
        "    \"g2e2\": 1144,\n",
        "    \"g2d2\": 1145,\n",
        "    \"g2c2\": 1146,\n",
        "    \"g2b2\": 1147,\n",
        "    \"g2a2\": 1148,\n",
        "    \"g2h1\": 1149,\n",
        "    \"g2g1\": 1150,\n",
        "    \"g2f1\": 1151,\n",
        "    \"g3g8\": 1152,\n",
        "    \"g3b8\": 1153,\n",
        "    \"g3g7\": 1154,\n",
        "    \"g3c7\": 1155,\n",
        "    \"g3g6\": 1156,\n",
        "    \"g3d6\": 1157,\n",
        "    \"g3g5\": 1158,\n",
        "    \"g3e5\": 1159,\n",
        "    \"g3h4\": 1160,\n",
        "    \"g3g4\": 1161,\n",
        "    \"g3f4\": 1162,\n",
        "    \"g3h3\": 1163,\n",
        "    \"g3f3\": 1164,\n",
        "    \"g3e3\": 1165,\n",
        "    \"g3d3\": 1166,\n",
        "    \"g3c3\": 1167,\n",
        "    \"g3b3\": 1168,\n",
        "    \"g3a3\": 1169,\n",
        "    \"g3h2\": 1170,\n",
        "    \"g3g2\": 1171,\n",
        "    \"g3f2\": 1172,\n",
        "    \"g3g1\": 1173,\n",
        "    \"g3e1\": 1174,\n",
        "    \"g4g8\": 1175,\n",
        "    \"g4c8\": 1176,\n",
        "    \"g4g7\": 1177,\n",
        "    \"g4d7\": 1178,\n",
        "    \"g4g6\": 1179,\n",
        "    \"g4e6\": 1180,\n",
        "    \"g4h5\": 1181,\n",
        "    \"g4g5\": 1182,\n",
        "    \"g4f5\": 1183,\n",
        "    \"g4h4\": 1184,\n",
        "    \"g4f4\": 1185,\n",
        "    \"g4e4\": 1186,\n",
        "    \"g4d4\": 1187,\n",
        "    \"g4c4\": 1188,\n",
        "    \"g4b4\": 1189,\n",
        "    \"g4a4\": 1190,\n",
        "    \"g4h3\": 1191,\n",
        "    \"g4g3\": 1192,\n",
        "    \"g4f3\": 1193,\n",
        "    \"g4g2\": 1194,\n",
        "    \"g4e2\": 1195,\n",
        "    \"g4g1\": 1196,\n",
        "    \"g4d1\": 1197,\n",
        "    \"g5g8\": 1198,\n",
        "    \"g5d8\": 1199,\n",
        "    \"g5g7\": 1200,\n",
        "    \"g5e7\": 1201,\n",
        "    \"g5h6\": 1202,\n",
        "    \"g5g6\": 1203,\n",
        "    \"g5f6\": 1204,\n",
        "    \"g5h5\": 1205,\n",
        "    \"g5f5\": 1206,\n",
        "    \"g5e5\": 1207,\n",
        "    \"g5d5\": 1208,\n",
        "    \"g5c5\": 1209,\n",
        "    \"g5b5\": 1210,\n",
        "    \"g5a5\": 1211,\n",
        "    \"g5h4\": 1212,\n",
        "    \"g5g4\": 1213,\n",
        "    \"g5f4\": 1214,\n",
        "    \"g5g3\": 1215,\n",
        "    \"g5e3\": 1216,\n",
        "    \"g5g2\": 1217,\n",
        "    \"g5d2\": 1218,\n",
        "    \"g5g1\": 1219,\n",
        "    \"g5c1\": 1220,\n",
        "    \"g6g8\": 1221,\n",
        "    \"g6e8\": 1222,\n",
        "    \"g6h7\": 1223,\n",
        "    \"g6g7\": 1224,\n",
        "    \"g6f7\": 1225,\n",
        "    \"g6h6\": 1226,\n",
        "    \"g6f6\": 1227,\n",
        "    \"g6e6\": 1228,\n",
        "    \"g6d6\": 1229,\n",
        "    \"g6c6\": 1230,\n",
        "    \"g6b6\": 1231,\n",
        "    \"g6a6\": 1232,\n",
        "    \"g6h5\": 1233,\n",
        "    \"g6g5\": 1234,\n",
        "    \"g6f5\": 1235,\n",
        "    \"g6g4\": 1236,\n",
        "    \"g6e4\": 1237,\n",
        "    \"g6g3\": 1238,\n",
        "    \"g6d3\": 1239,\n",
        "    \"g6g2\": 1240,\n",
        "    \"g6c2\": 1241,\n",
        "    \"g6g1\": 1242,\n",
        "    \"g6b1\": 1243,\n",
        "    \"g7h8\": 1244,\n",
        "    \"g7g8\": 1245,\n",
        "    \"g7f8\": 1246,\n",
        "    \"g7h7\": 1247,\n",
        "    \"g7f7\": 1248,\n",
        "    \"g7e7\": 1249,\n",
        "    \"g7d7\": 1250,\n",
        "    \"g7c7\": 1251,\n",
        "    \"g7b7\": 1252,\n",
        "    \"g7a7\": 1253,\n",
        "    \"g7h6\": 1254,\n",
        "    \"g7g6\": 1255,\n",
        "    \"g7f6\": 1256,\n",
        "    \"g7g5\": 1257,\n",
        "    \"g7e5\": 1258,\n",
        "    \"g7g4\": 1259,\n",
        "    \"g7d4\": 1260,\n",
        "    \"g7g3\": 1261,\n",
        "    \"g7c3\": 1262,\n",
        "    \"g7g2\": 1263,\n",
        "    \"g7b2\": 1264,\n",
        "    \"g7g1\": 1265,\n",
        "    \"g7a1\": 1266,\n",
        "    \"g8h8\": 1267,\n",
        "    \"g8f8\": 1268,\n",
        "    \"g8e8\": 1269,\n",
        "    \"g8d8\": 1270,\n",
        "    \"g8c8\": 1271,\n",
        "    \"g8b8\": 1272,\n",
        "    \"g8a8\": 1273,\n",
        "    \"g8h7\": 1274,\n",
        "    \"g8g7\": 1275,\n",
        "    \"g8f7\": 1276,\n",
        "    \"g8g6\": 1277,\n",
        "    \"g8e6\": 1278,\n",
        "    \"g8g5\": 1279,\n",
        "    \"g8d5\": 1280,\n",
        "    \"g8g4\": 1281,\n",
        "    \"g8c4\": 1282,\n",
        "    \"g8g3\": 1283,\n",
        "    \"g8b3\": 1284,\n",
        "    \"g8g2\": 1285,\n",
        "    \"g8a2\": 1286,\n",
        "    \"g8g1\": 1287,\n",
        "    \"h1h8\": 1288,\n",
        "    \"h1a8\": 1289,\n",
        "    \"h1h7\": 1290,\n",
        "    \"h1b7\": 1291,\n",
        "    \"h1h6\": 1292,\n",
        "    \"h1c6\": 1293,\n",
        "    \"h1h5\": 1294,\n",
        "    \"h1d5\": 1295,\n",
        "    \"h1h4\": 1296,\n",
        "    \"h1e4\": 1297,\n",
        "    \"h1h3\": 1298,\n",
        "    \"h1f3\": 1299,\n",
        "    \"h1h2\": 1300,\n",
        "    \"h1g2\": 1301,\n",
        "    \"h1g1\": 1302,\n",
        "    \"h1f1\": 1303,\n",
        "    \"h1e1\": 1304,\n",
        "    \"h1d1\": 1305,\n",
        "    \"h1c1\": 1306,\n",
        "    \"h1b1\": 1307,\n",
        "    \"h1a1\": 1308,\n",
        "    \"h2h8\": 1309,\n",
        "    \"h2b8\": 1310,\n",
        "    \"h2h7\": 1311,\n",
        "    \"h2c7\": 1312,\n",
        "    \"h2h6\": 1313,\n",
        "    \"h2d6\": 1314,\n",
        "    \"h2h5\": 1315,\n",
        "    \"h2e5\": 1316,\n",
        "    \"h2h4\": 1317,\n",
        "    \"h2f4\": 1318,\n",
        "    \"h2h3\": 1319,\n",
        "    \"h2g3\": 1320,\n",
        "    \"h2g2\": 1321,\n",
        "    \"h2f2\": 1322,\n",
        "    \"h2e2\": 1323,\n",
        "    \"h2d2\": 1324,\n",
        "    \"h2c2\": 1325,\n",
        "    \"h2b2\": 1326,\n",
        "    \"h2a2\": 1327,\n",
        "    \"h2h1\": 1328,\n",
        "    \"h2g1\": 1329,\n",
        "    \"h3h8\": 1330,\n",
        "    \"h3c8\": 1331,\n",
        "    \"h3h7\": 1332,\n",
        "    \"h3d7\": 1333,\n",
        "    \"h3h6\": 1334,\n",
        "    \"h3e6\": 1335,\n",
        "    \"h3h5\": 1336,\n",
        "    \"h3f5\": 1337,\n",
        "    \"h3h4\": 1338,\n",
        "    \"h3g4\": 1339,\n",
        "    \"h3g3\": 1340,\n",
        "    \"h3f3\": 1341,\n",
        "    \"h3e3\": 1342,\n",
        "    \"h3d3\": 1343,\n",
        "    \"h3c3\": 1344,\n",
        "    \"h3b3\": 1345,\n",
        "    \"h3a3\": 1346,\n",
        "    \"h3h2\": 1347,\n",
        "    \"h3g2\": 1348,\n",
        "    \"h3h1\": 1349,\n",
        "    \"h3f1\": 1350,\n",
        "    \"h4h8\": 1351,\n",
        "    \"h4d8\": 1352,\n",
        "    \"h4h7\": 1353,\n",
        "    \"h4e7\": 1354,\n",
        "    \"h4h6\": 1355,\n",
        "    \"h4f6\": 1356,\n",
        "    \"h4h5\": 1357,\n",
        "    \"h4g5\": 1358,\n",
        "    \"h4g4\": 1359,\n",
        "    \"h4f4\": 1360,\n",
        "    \"h4e4\": 1361,\n",
        "    \"h4d4\": 1362,\n",
        "    \"h4c4\": 1363,\n",
        "    \"h4b4\": 1364,\n",
        "    \"h4a4\": 1365,\n",
        "    \"h4h3\": 1366,\n",
        "    \"h4g3\": 1367,\n",
        "    \"h4h2\": 1368,\n",
        "    \"h4f2\": 1369,\n",
        "    \"h4h1\": 1370,\n",
        "    \"h4e1\": 1371,\n",
        "    \"h5h8\": 1372,\n",
        "    \"h5e8\": 1373,\n",
        "    \"h5h7\": 1374,\n",
        "    \"h5f7\": 1375,\n",
        "    \"h5h6\": 1376,\n",
        "    \"h5g6\": 1377,\n",
        "    \"h5g5\": 1378,\n",
        "    \"h5f5\": 1379,\n",
        "    \"h5e5\": 1380,\n",
        "    \"h5d5\": 1381,\n",
        "    \"h5c5\": 1382,\n",
        "    \"h5b5\": 1383,\n",
        "    \"h5a5\": 1384,\n",
        "    \"h5h4\": 1385,\n",
        "    \"h5g4\": 1386,\n",
        "    \"h5h3\": 1387,\n",
        "    \"h5f3\": 1388,\n",
        "    \"h5h2\": 1389,\n",
        "    \"h5e2\": 1390,\n",
        "    \"h5h1\": 1391,\n",
        "    \"h5d1\": 1392,\n",
        "    \"h6h8\": 1393,\n",
        "    \"h6f8\": 1394,\n",
        "    \"h6h7\": 1395,\n",
        "    \"h6g7\": 1396,\n",
        "    \"h6g6\": 1397,\n",
        "    \"h6f6\": 1398,\n",
        "    \"h6e6\": 1399,\n",
        "    \"h6d6\": 1400,\n",
        "    \"h6c6\": 1401,\n",
        "    \"h6b6\": 1402,\n",
        "    \"h6a6\": 1403,\n",
        "    \"h6h5\": 1404,\n",
        "    \"h6g5\": 1405,\n",
        "    \"h6h4\": 1406,\n",
        "    \"h6f4\": 1407,\n",
        "    \"h6h3\": 1408,\n",
        "    \"h6e3\": 1409,\n",
        "    \"h6h2\": 1410,\n",
        "    \"h6d2\": 1411,\n",
        "    \"h6h1\": 1412,\n",
        "    \"h6c1\": 1413,\n",
        "    \"h7h8\": 1414,\n",
        "    \"h7g8\": 1415,\n",
        "    \"h7g7\": 1416,\n",
        "    \"h7f7\": 1417,\n",
        "    \"h7e7\": 1418,\n",
        "    \"h7d7\": 1419,\n",
        "    \"h7c7\": 1420,\n",
        "    \"h7b7\": 1421,\n",
        "    \"h7a7\": 1422,\n",
        "    \"h7h6\": 1423,\n",
        "    \"h7g6\": 1424,\n",
        "    \"h7h5\": 1425,\n",
        "    \"h7f5\": 1426,\n",
        "    \"h7h4\": 1427,\n",
        "    \"h7e4\": 1428,\n",
        "    \"h7h3\": 1429,\n",
        "    \"h7d3\": 1430,\n",
        "    \"h7h2\": 1431,\n",
        "    \"h7c2\": 1432,\n",
        "    \"h7h1\": 1433,\n",
        "    \"h7b1\": 1434,\n",
        "    \"h8g8\": 1435,\n",
        "    \"h8f8\": 1436,\n",
        "    \"h8e8\": 1437,\n",
        "    \"h8d8\": 1438,\n",
        "    \"h8c8\": 1439,\n",
        "    \"h8b8\": 1440,\n",
        "    \"h8a8\": 1441,\n",
        "    \"h8h7\": 1442,\n",
        "    \"h8g7\": 1443,\n",
        "    \"h8h6\": 1444,\n",
        "    \"h8f6\": 1445,\n",
        "    \"h8h5\": 1446,\n",
        "    \"h8e5\": 1447,\n",
        "    \"h8h4\": 1448,\n",
        "    \"h8d4\": 1449,\n",
        "    \"h8h3\": 1450,\n",
        "    \"h8c3\": 1451,\n",
        "    \"h8h2\": 1452,\n",
        "    \"h8b2\": 1453,\n",
        "    \"h8h1\": 1454,\n",
        "    \"h8a1\": 1455,\n",
        "    \"a1b3\": 1456,\n",
        "    \"a1c2\": 1457,\n",
        "    \"a2b4\": 1458,\n",
        "    \"a2c3\": 1459,\n",
        "    \"a2c1\": 1460,\n",
        "    \"a3b5\": 1461,\n",
        "    \"a3c4\": 1462,\n",
        "    \"a3c2\": 1463,\n",
        "    \"a3b1\": 1464,\n",
        "    \"a4b6\": 1465,\n",
        "    \"a4c5\": 1466,\n",
        "    \"a4c3\": 1467,\n",
        "    \"a4b2\": 1468,\n",
        "    \"a5b7\": 1469,\n",
        "    \"a5c6\": 1470,\n",
        "    \"a5c4\": 1471,\n",
        "    \"a5b3\": 1472,\n",
        "    \"a6b8\": 1473,\n",
        "    \"a6c7\": 1474,\n",
        "    \"a6c5\": 1475,\n",
        "    \"a6b4\": 1476,\n",
        "    \"a7c8\": 1477,\n",
        "    \"a7c6\": 1478,\n",
        "    \"a7b5\": 1479,\n",
        "    \"a8c7\": 1480,\n",
        "    \"a8b6\": 1481,\n",
        "    \"b1c3\": 1482,\n",
        "    \"b1a3\": 1483,\n",
        "    \"b1d2\": 1484,\n",
        "    \"b2c4\": 1485,\n",
        "    \"b2a4\": 1486,\n",
        "    \"b2d3\": 1487,\n",
        "    \"b2d1\": 1488,\n",
        "    \"b3c5\": 1489,\n",
        "    \"b3a5\": 1490,\n",
        "    \"b3d4\": 1491,\n",
        "    \"b3d2\": 1492,\n",
        "    \"b3c1\": 1493,\n",
        "    \"b3a1\": 1494,\n",
        "    \"b4c6\": 1495,\n",
        "    \"b4a6\": 1496,\n",
        "    \"b4d5\": 1497,\n",
        "    \"b4d3\": 1498,\n",
        "    \"b4c2\": 1499,\n",
        "    \"b4a2\": 1500,\n",
        "    \"b5c7\": 1501,\n",
        "    \"b5a7\": 1502,\n",
        "    \"b5d6\": 1503,\n",
        "    \"b5d4\": 1504,\n",
        "    \"b5c3\": 1505,\n",
        "    \"b5a3\": 1506,\n",
        "    \"b6c8\": 1507,\n",
        "    \"b6a8\": 1508,\n",
        "    \"b6d7\": 1509,\n",
        "    \"b6d5\": 1510,\n",
        "    \"b6c4\": 1511,\n",
        "    \"b6a4\": 1512,\n",
        "    \"b7d8\": 1513,\n",
        "    \"b7d6\": 1514,\n",
        "    \"b7c5\": 1515,\n",
        "    \"b7a5\": 1516,\n",
        "    \"b8d7\": 1517,\n",
        "    \"b8c6\": 1518,\n",
        "    \"b8a6\": 1519,\n",
        "    \"c1d3\": 1520,\n",
        "    \"c1b3\": 1521,\n",
        "    \"c1e2\": 1522,\n",
        "    \"c1a2\": 1523,\n",
        "    \"c2d4\": 1524,\n",
        "    \"c2b4\": 1525,\n",
        "    \"c2e3\": 1526,\n",
        "    \"c2a3\": 1527,\n",
        "    \"c2e1\": 1528,\n",
        "    \"c2a1\": 1529,\n",
        "    \"c3d5\": 1530,\n",
        "    \"c3b5\": 1531,\n",
        "    \"c3e4\": 1532,\n",
        "    \"c3a4\": 1533,\n",
        "    \"c3e2\": 1534,\n",
        "    \"c3a2\": 1535,\n",
        "    \"c3d1\": 1536,\n",
        "    \"c3b1\": 1537,\n",
        "    \"c4d6\": 1538,\n",
        "    \"c4b6\": 1539,\n",
        "    \"c4e5\": 1540,\n",
        "    \"c4a5\": 1541,\n",
        "    \"c4e3\": 1542,\n",
        "    \"c4a3\": 1543,\n",
        "    \"c4d2\": 1544,\n",
        "    \"c4b2\": 1545,\n",
        "    \"c5d7\": 1546,\n",
        "    \"c5b7\": 1547,\n",
        "    \"c5e6\": 1548,\n",
        "    \"c5a6\": 1549,\n",
        "    \"c5e4\": 1550,\n",
        "    \"c5a4\": 1551,\n",
        "    \"c5d3\": 1552,\n",
        "    \"c5b3\": 1553,\n",
        "    \"c6d8\": 1554,\n",
        "    \"c6b8\": 1555,\n",
        "    \"c6e7\": 1556,\n",
        "    \"c6a7\": 1557,\n",
        "    \"c6e5\": 1558,\n",
        "    \"c6a5\": 1559,\n",
        "    \"c6d4\": 1560,\n",
        "    \"c6b4\": 1561,\n",
        "    \"c7e8\": 1562,\n",
        "    \"c7a8\": 1563,\n",
        "    \"c7e6\": 1564,\n",
        "    \"c7a6\": 1565,\n",
        "    \"c7d5\": 1566,\n",
        "    \"c7b5\": 1567,\n",
        "    \"c8e7\": 1568,\n",
        "    \"c8a7\": 1569,\n",
        "    \"c8d6\": 1570,\n",
        "    \"c8b6\": 1571,\n",
        "    \"d1e3\": 1572,\n",
        "    \"d1c3\": 1573,\n",
        "    \"d1f2\": 1574,\n",
        "    \"d1b2\": 1575,\n",
        "    \"d2e4\": 1576,\n",
        "    \"d2c4\": 1577,\n",
        "    \"d2f3\": 1578,\n",
        "    \"d2b3\": 1579,\n",
        "    \"d2f1\": 1580,\n",
        "    \"d2b1\": 1581,\n",
        "    \"d3e5\": 1582,\n",
        "    \"d3c5\": 1583,\n",
        "    \"d3f4\": 1584,\n",
        "    \"d3b4\": 1585,\n",
        "    \"d3f2\": 1586,\n",
        "    \"d3b2\": 1587,\n",
        "    \"d3e1\": 1588,\n",
        "    \"d3c1\": 1589,\n",
        "    \"d4e6\": 1590,\n",
        "    \"d4c6\": 1591,\n",
        "    \"d4f5\": 1592,\n",
        "    \"d4b5\": 1593,\n",
        "    \"d4f3\": 1594,\n",
        "    \"d4b3\": 1595,\n",
        "    \"d4e2\": 1596,\n",
        "    \"d4c2\": 1597,\n",
        "    \"d5e7\": 1598,\n",
        "    \"d5c7\": 1599,\n",
        "    \"d5f6\": 1600,\n",
        "    \"d5b6\": 1601,\n",
        "    \"d5f4\": 1602,\n",
        "    \"d5b4\": 1603,\n",
        "    \"d5e3\": 1604,\n",
        "    \"d5c3\": 1605,\n",
        "    \"d6e8\": 1606,\n",
        "    \"d6c8\": 1607,\n",
        "    \"d6f7\": 1608,\n",
        "    \"d6b7\": 1609,\n",
        "    \"d6f5\": 1610,\n",
        "    \"d6b5\": 1611,\n",
        "    \"d6e4\": 1612,\n",
        "    \"d6c4\": 1613,\n",
        "    \"d7f8\": 1614,\n",
        "    \"d7b8\": 1615,\n",
        "    \"d7f6\": 1616,\n",
        "    \"d7b6\": 1617,\n",
        "    \"d7e5\": 1618,\n",
        "    \"d7c5\": 1619,\n",
        "    \"d8f7\": 1620,\n",
        "    \"d8b7\": 1621,\n",
        "    \"d8e6\": 1622,\n",
        "    \"d8c6\": 1623,\n",
        "    \"e1f3\": 1624,\n",
        "    \"e1d3\": 1625,\n",
        "    \"e1g2\": 1626,\n",
        "    \"e1c2\": 1627,\n",
        "    \"e2f4\": 1628,\n",
        "    \"e2d4\": 1629,\n",
        "    \"e2g3\": 1630,\n",
        "    \"e2c3\": 1631,\n",
        "    \"e2g1\": 1632,\n",
        "    \"e2c1\": 1633,\n",
        "    \"e3f5\": 1634,\n",
        "    \"e3d5\": 1635,\n",
        "    \"e3g4\": 1636,\n",
        "    \"e3c4\": 1637,\n",
        "    \"e3g2\": 1638,\n",
        "    \"e3c2\": 1639,\n",
        "    \"e3f1\": 1640,\n",
        "    \"e3d1\": 1641,\n",
        "    \"e4f6\": 1642,\n",
        "    \"e4d6\": 1643,\n",
        "    \"e4g5\": 1644,\n",
        "    \"e4c5\": 1645,\n",
        "    \"e4g3\": 1646,\n",
        "    \"e4c3\": 1647,\n",
        "    \"e4f2\": 1648,\n",
        "    \"e4d2\": 1649,\n",
        "    \"e5f7\": 1650,\n",
        "    \"e5d7\": 1651,\n",
        "    \"e5g6\": 1652,\n",
        "    \"e5c6\": 1653,\n",
        "    \"e5g4\": 1654,\n",
        "    \"e5c4\": 1655,\n",
        "    \"e5f3\": 1656,\n",
        "    \"e5d3\": 1657,\n",
        "    \"e6f8\": 1658,\n",
        "    \"e6d8\": 1659,\n",
        "    \"e6g7\": 1660,\n",
        "    \"e6c7\": 1661,\n",
        "    \"e6g5\": 1662,\n",
        "    \"e6c5\": 1663,\n",
        "    \"e6f4\": 1664,\n",
        "    \"e6d4\": 1665,\n",
        "    \"e7g8\": 1666,\n",
        "    \"e7c8\": 1667,\n",
        "    \"e7g6\": 1668,\n",
        "    \"e7c6\": 1669,\n",
        "    \"e7f5\": 1670,\n",
        "    \"e7d5\": 1671,\n",
        "    \"e8g7\": 1672,\n",
        "    \"e8c7\": 1673,\n",
        "    \"e8f6\": 1674,\n",
        "    \"e8d6\": 1675,\n",
        "    \"f1g3\": 1676,\n",
        "    \"f1e3\": 1677,\n",
        "    \"f1h2\": 1678,\n",
        "    \"f1d2\": 1679,\n",
        "    \"f2g4\": 1680,\n",
        "    \"f2e4\": 1681,\n",
        "    \"f2h3\": 1682,\n",
        "    \"f2d3\": 1683,\n",
        "    \"f2h1\": 1684,\n",
        "    \"f2d1\": 1685,\n",
        "    \"f3g5\": 1686,\n",
        "    \"f3e5\": 1687,\n",
        "    \"f3h4\": 1688,\n",
        "    \"f3d4\": 1689,\n",
        "    \"f3h2\": 1690,\n",
        "    \"f3d2\": 1691,\n",
        "    \"f3g1\": 1692,\n",
        "    \"f3e1\": 1693,\n",
        "    \"f4g6\": 1694,\n",
        "    \"f4e6\": 1695,\n",
        "    \"f4h5\": 1696,\n",
        "    \"f4d5\": 1697,\n",
        "    \"f4h3\": 1698,\n",
        "    \"f4d3\": 1699,\n",
        "    \"f4g2\": 1700,\n",
        "    \"f4e2\": 1701,\n",
        "    \"f5g7\": 1702,\n",
        "    \"f5e7\": 1703,\n",
        "    \"f5h6\": 1704,\n",
        "    \"f5d6\": 1705,\n",
        "    \"f5h4\": 1706,\n",
        "    \"f5d4\": 1707,\n",
        "    \"f5g3\": 1708,\n",
        "    \"f5e3\": 1709,\n",
        "    \"f6g8\": 1710,\n",
        "    \"f6e8\": 1711,\n",
        "    \"f6h7\": 1712,\n",
        "    \"f6d7\": 1713,\n",
        "    \"f6h5\": 1714,\n",
        "    \"f6d5\": 1715,\n",
        "    \"f6g4\": 1716,\n",
        "    \"f6e4\": 1717,\n",
        "    \"f7h8\": 1718,\n",
        "    \"f7d8\": 1719,\n",
        "    \"f7h6\": 1720,\n",
        "    \"f7d6\": 1721,\n",
        "    \"f7g5\": 1722,\n",
        "    \"f7e5\": 1723,\n",
        "    \"f8h7\": 1724,\n",
        "    \"f8d7\": 1725,\n",
        "    \"f8g6\": 1726,\n",
        "    \"f8e6\": 1727,\n",
        "    \"g1h3\": 1728,\n",
        "    \"g1f3\": 1729,\n",
        "    \"g1e2\": 1730,\n",
        "    \"g2h4\": 1731,\n",
        "    \"g2f4\": 1732,\n",
        "    \"g2e3\": 1733,\n",
        "    \"g2e1\": 1734,\n",
        "    \"g3h5\": 1735,\n",
        "    \"g3f5\": 1736,\n",
        "    \"g3e4\": 1737,\n",
        "    \"g3e2\": 1738,\n",
        "    \"g3h1\": 1739,\n",
        "    \"g3f1\": 1740,\n",
        "    \"g4h6\": 1741,\n",
        "    \"g4f6\": 1742,\n",
        "    \"g4e5\": 1743,\n",
        "    \"g4e3\": 1744,\n",
        "    \"g4h2\": 1745,\n",
        "    \"g4f2\": 1746,\n",
        "    \"g5h7\": 1747,\n",
        "    \"g5f7\": 1748,\n",
        "    \"g5e6\": 1749,\n",
        "    \"g5e4\": 1750,\n",
        "    \"g5h3\": 1751,\n",
        "    \"g5f3\": 1752,\n",
        "    \"g6h8\": 1753,\n",
        "    \"g6f8\": 1754,\n",
        "    \"g6e7\": 1755,\n",
        "    \"g6e5\": 1756,\n",
        "    \"g6h4\": 1757,\n",
        "    \"g6f4\": 1758,\n",
        "    \"g7e8\": 1759,\n",
        "    \"g7e6\": 1760,\n",
        "    \"g7h5\": 1761,\n",
        "    \"g7f5\": 1762,\n",
        "    \"g8e7\": 1763,\n",
        "    \"g8h6\": 1764,\n",
        "    \"g8f6\": 1765,\n",
        "    \"h1g3\": 1766,\n",
        "    \"h1f2\": 1767,\n",
        "    \"h2g4\": 1768,\n",
        "    \"h2f3\": 1769,\n",
        "    \"h2f1\": 1770,\n",
        "    \"h3g5\": 1771,\n",
        "    \"h3f4\": 1772,\n",
        "    \"h3f2\": 1773,\n",
        "    \"h3g1\": 1774,\n",
        "    \"h4g6\": 1775,\n",
        "    \"h4f5\": 1776,\n",
        "    \"h4f3\": 1777,\n",
        "    \"h4g2\": 1778,\n",
        "    \"h5g7\": 1779,\n",
        "    \"h5f6\": 1780,\n",
        "    \"h5f4\": 1781,\n",
        "    \"h5g3\": 1782,\n",
        "    \"h6g8\": 1783,\n",
        "    \"h6f7\": 1784,\n",
        "    \"h6f5\": 1785,\n",
        "    \"h6g4\": 1786,\n",
        "    \"h7f8\": 1787,\n",
        "    \"h7f6\": 1788,\n",
        "    \"h7g5\": 1789,\n",
        "    \"h8f7\": 1790,\n",
        "    \"h8g6\": 1791,\n",
        "    \"a7b8q\": 1792,\n",
        "    \"a7b8r\": 1793,\n",
        "    \"a7b8b\": 1794,\n",
        "    \"a7b8n\": 1795,\n",
        "    \"a7a8q\": 1796,\n",
        "    \"a7a8r\": 1797,\n",
        "    \"a7a8b\": 1798,\n",
        "    \"a7a8n\": 1799,\n",
        "    \"a2b1q\": 1800,\n",
        "    \"a2b1r\": 1801,\n",
        "    \"a2b1b\": 1802,\n",
        "    \"a2b1n\": 1803,\n",
        "    \"a2a1q\": 1804,\n",
        "    \"a2a1r\": 1805,\n",
        "    \"a2a1b\": 1806,\n",
        "    \"a2a1n\": 1807,\n",
        "    \"b7c8q\": 1808,\n",
        "    \"b7c8r\": 1809,\n",
        "    \"b7c8b\": 1810,\n",
        "    \"b7c8n\": 1811,\n",
        "    \"b7a8q\": 1812,\n",
        "    \"b7a8r\": 1813,\n",
        "    \"b7a8b\": 1814,\n",
        "    \"b7a8n\": 1815,\n",
        "    \"b7b8q\": 1816,\n",
        "    \"b7b8r\": 1817,\n",
        "    \"b7b8b\": 1818,\n",
        "    \"b7b8n\": 1819,\n",
        "    \"b2c1q\": 1820,\n",
        "    \"b2c1r\": 1821,\n",
        "    \"b2c1b\": 1822,\n",
        "    \"b2c1n\": 1823,\n",
        "    \"b2a1q\": 1824,\n",
        "    \"b2a1r\": 1825,\n",
        "    \"b2a1b\": 1826,\n",
        "    \"b2a1n\": 1827,\n",
        "    \"b2b1q\": 1828,\n",
        "    \"b2b1r\": 1829,\n",
        "    \"b2b1b\": 1830,\n",
        "    \"b2b1n\": 1831,\n",
        "    \"c7d8q\": 1832,\n",
        "    \"c7d8r\": 1833,\n",
        "    \"c7d8b\": 1834,\n",
        "    \"c7d8n\": 1835,\n",
        "    \"c7b8q\": 1836,\n",
        "    \"c7b8r\": 1837,\n",
        "    \"c7b8b\": 1838,\n",
        "    \"c7b8n\": 1839,\n",
        "    \"c7c8q\": 1840,\n",
        "    \"c7c8r\": 1841,\n",
        "    \"c7c8b\": 1842,\n",
        "    \"c7c8n\": 1843,\n",
        "    \"c2d1q\": 1844,\n",
        "    \"c2d1r\": 1845,\n",
        "    \"c2d1b\": 1846,\n",
        "    \"c2d1n\": 1847,\n",
        "    \"c2b1q\": 1848,\n",
        "    \"c2b1r\": 1849,\n",
        "    \"c2b1b\": 1850,\n",
        "    \"c2b1n\": 1851,\n",
        "    \"c2c1q\": 1852,\n",
        "    \"c2c1r\": 1853,\n",
        "    \"c2c1b\": 1854,\n",
        "    \"c2c1n\": 1855,\n",
        "    \"d7e8q\": 1856,\n",
        "    \"d7e8r\": 1857,\n",
        "    \"d7e8b\": 1858,\n",
        "    \"d7e8n\": 1859,\n",
        "    \"d7c8q\": 1860,\n",
        "    \"d7c8r\": 1861,\n",
        "    \"d7c8b\": 1862,\n",
        "    \"d7c8n\": 1863,\n",
        "    \"d7d8q\": 1864,\n",
        "    \"d7d8r\": 1865,\n",
        "    \"d7d8b\": 1866,\n",
        "    \"d7d8n\": 1867,\n",
        "    \"d2e1q\": 1868,\n",
        "    \"d2e1r\": 1869,\n",
        "    \"d2e1b\": 1870,\n",
        "    \"d2e1n\": 1871,\n",
        "    \"d2c1q\": 1872,\n",
        "    \"d2c1r\": 1873,\n",
        "    \"d2c1b\": 1874,\n",
        "    \"d2c1n\": 1875,\n",
        "    \"d2d1q\": 1876,\n",
        "    \"d2d1r\": 1877,\n",
        "    \"d2d1b\": 1878,\n",
        "    \"d2d1n\": 1879,\n",
        "    \"e7f8q\": 1880,\n",
        "    \"e7f8r\": 1881,\n",
        "    \"e7f8b\": 1882,\n",
        "    \"e7f8n\": 1883,\n",
        "    \"e7d8q\": 1884,\n",
        "    \"e7d8r\": 1885,\n",
        "    \"e7d8b\": 1886,\n",
        "    \"e7d8n\": 1887,\n",
        "    \"e7e8q\": 1888,\n",
        "    \"e7e8r\": 1889,\n",
        "    \"e7e8b\": 1890,\n",
        "    \"e7e8n\": 1891,\n",
        "    \"e2f1q\": 1892,\n",
        "    \"e2f1r\": 1893,\n",
        "    \"e2f1b\": 1894,\n",
        "    \"e2f1n\": 1895,\n",
        "    \"e2d1q\": 1896,\n",
        "    \"e2d1r\": 1897,\n",
        "    \"e2d1b\": 1898,\n",
        "    \"e2d1n\": 1899,\n",
        "    \"e2e1q\": 1900,\n",
        "    \"e2e1r\": 1901,\n",
        "    \"e2e1b\": 1902,\n",
        "    \"e2e1n\": 1903,\n",
        "    \"f7g8q\": 1904,\n",
        "    \"f7g8r\": 1905,\n",
        "    \"f7g8b\": 1906,\n",
        "    \"f7g8n\": 1907,\n",
        "    \"f7e8q\": 1908,\n",
        "    \"f7e8r\": 1909,\n",
        "    \"f7e8b\": 1910,\n",
        "    \"f7e8n\": 1911,\n",
        "    \"f7f8q\": 1912,\n",
        "    \"f7f8r\": 1913,\n",
        "    \"f7f8b\": 1914,\n",
        "    \"f7f8n\": 1915,\n",
        "    \"f2g1q\": 1916,\n",
        "    \"f2g1r\": 1917,\n",
        "    \"f2g1b\": 1918,\n",
        "    \"f2g1n\": 1919,\n",
        "    \"f2e1q\": 1920,\n",
        "    \"f2e1r\": 1921,\n",
        "    \"f2e1b\": 1922,\n",
        "    \"f2e1n\": 1923,\n",
        "    \"f2f1q\": 1924,\n",
        "    \"f2f1r\": 1925,\n",
        "    \"f2f1b\": 1926,\n",
        "    \"f2f1n\": 1927,\n",
        "    \"g7h8q\": 1928,\n",
        "    \"g7h8r\": 1929,\n",
        "    \"g7h8b\": 1930,\n",
        "    \"g7h8n\": 1931,\n",
        "    \"g7f8q\": 1932,\n",
        "    \"g7f8r\": 1933,\n",
        "    \"g7f8b\": 1934,\n",
        "    \"g7f8n\": 1935,\n",
        "    \"g7g8q\": 1936,\n",
        "    \"g7g8r\": 1937,\n",
        "    \"g7g8b\": 1938,\n",
        "    \"g7g8n\": 1939,\n",
        "    \"g2h1q\": 1940,\n",
        "    \"g2h1r\": 1941,\n",
        "    \"g2h1b\": 1942,\n",
        "    \"g2h1n\": 1943,\n",
        "    \"g2f1q\": 1944,\n",
        "    \"g2f1r\": 1945,\n",
        "    \"g2f1b\": 1946,\n",
        "    \"g2f1n\": 1947,\n",
        "    \"g2g1q\": 1948,\n",
        "    \"g2g1r\": 1949,\n",
        "    \"g2g1b\": 1950,\n",
        "    \"g2g1n\": 1951,\n",
        "    \"h7g8q\": 1952,\n",
        "    \"h7g8r\": 1953,\n",
        "    \"h7g8b\": 1954,\n",
        "    \"h7g8n\": 1955,\n",
        "    \"h7h8q\": 1956,\n",
        "    \"h7h8r\": 1957,\n",
        "    \"h7h8b\": 1958,\n",
        "    \"h7h8n\": 1959,\n",
        "    \"h2g1q\": 1960,\n",
        "    \"h2g1r\": 1961,\n",
        "    \"h2g1b\": 1962,\n",
        "    \"h2g1n\": 1963,\n",
        "    \"h2h1q\": 1964,\n",
        "    \"h2h1r\": 1965,\n",
        "    \"h2h1b\": 1966,\n",
        "    \"h2h1n\": 1967,\n",
        "    \"0000\": 1968,\n",
        "    \"<loss>\": 1969,\n",
        "    \"<pad>\": 1970,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code continued"
      ],
      "metadata": {
        "id": "9N3opENdxoRo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "-zx88Kg_0SHU"
      },
      "outputs": [],
      "source": [
        "df[\"Comment\"] = df[\"Comment\"].fillna('This move does not need a comment')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Imvj6lQD0SHU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_board_array(board_column):\n",
        "    # Define the pieces dictionary\n",
        "\n",
        "    # Split the board column into rows\n",
        "    rows = board_column.strip().split('\\n')\n",
        "\n",
        "    # Create an empty numpy array of size 8x8\n",
        "    board_array = np.zeros((8, 8), dtype=int)\n",
        "\n",
        "    # Iterate over the rows and columns to fill the board array\n",
        "    for i, row in enumerate(rows):\n",
        "        for j, piece in enumerate(row.split()):\n",
        "            board_array[i, j] = PIECES[piece]\n",
        "\n",
        "    return board_array\n",
        "\n",
        "# Example usage:\n",
        "board_states = df[\"Board\"].apply(lambda x : create_board_array(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "AkJB-U3h0SHU"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Create the tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Tokenize the comments\n",
        "tokenized_comments = df[\"Comment\"].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "PZl3VnKO0SHU"
      },
      "outputs": [],
      "source": [
        "moves = df[\"Move\"].apply(lambda x: UCI_MOVES[x])\n",
        "turn = df[\"Player\"].apply(lambda x: TURN[x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "bQ5VBYoJ0SHU"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ChessDataset(Dataset):\n",
        "    def __init__(self, board_states, moves,turn,tokenized_comments,move_number,game_id):\n",
        "        self.board_states = board_states\n",
        "        self.moves = moves\n",
        "        self.turn = turn\n",
        "        self.comment = tokenized_comments\n",
        "        self.move_number = move_number\n",
        "        self.game_id = game_id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.board_states)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        out = tokenizer.encode_plus(self.comment[idx], return_attention_mask=True,return_tensors=\"pt\")\n",
        "        tokens = out[\"input_ids\"]\n",
        "        attention_mask = out[\"attention_mask\"]\n",
        "        return {\n",
        "            \"board\": torch.tensor(self.board_states[idx], dtype=torch.long),\n",
        "            \"move\": torch.tensor(self.moves[idx], dtype=torch.long),\n",
        "            \"turn\": torch.tensor(self.turn[idx], dtype=torch.long),\n",
        "            \"comment\": torch.tensor(tokens, dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
        "            \"move_number\": torch.tensor(self.move_number[idx], dtype=torch.long),\n",
        "            \"game_id\": torch.tensor(self.game_id[idx], dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "XBuIrLDn0SHU"
      },
      "outputs": [],
      "source": [
        "chessdata = ChessDataset(board_states, moves ,turn,df[\"Comment\"],df['Move Number'],df['Game Number'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a2qS-Bc0SHV",
        "outputId": "a21cdf36-6143-4ef1-8d79-f12068423eae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-50-020a9c49831d>:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"comment\": torch.tensor(tokens, dtype=torch.long),\n",
            "<ipython-input-50-020a9c49831d>:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'board': tensor([[ 5,  7,  9, 11, 13,  9,  7,  5],\n",
              "         [ 3,  3,  3,  3,  3,  3,  3,  3],\n",
              "         [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
              "         [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
              "         [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
              "         [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
              "         [ 2,  2,  2,  2,  2,  2,  2,  2],\n",
              "         [ 4,  6,  8, 10, 12,  8,  6,  4]]),\n",
              " 'move': tensor(757),\n",
              " 'turn': tensor(1),\n",
              " 'comment': tensor([[  47, 6326, 2743,  709, 5341,  352,   13,  304,   19,  287,  881,  262,\n",
              "           976,  835,  355,  597,  286,  262,  584, 1353,   12, 5715, 6951,   82,\n",
              "            13]]),\n",
              " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1]]),\n",
              " 'move_number': tensor(1),\n",
              " 'game_id': tensor(1)}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "chessdata[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz8ukBv0xdtp"
      },
      "source": [
        "# New stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "5EBFcQuE0SHV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, board_size, move_size, player_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Assuming the board is represented as a 8x8 grid and each square is one-hot encoded\n",
        "        self.board_size = board_size  # This could be 8*8 if you're simply flattening the board\n",
        "        self.move_size = move_size  # Size of the move representation\n",
        "        self.player_size = player_size  # Size of the player turn representation\n",
        "\n",
        "        # Optional: Convolutional layers to process the board state\n",
        "        # Adjust these parameters according to your needs\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.board_processing_size = 32 * 8 * 8  # Adjust based on your conv layer designs\n",
        "\n",
        "        # Fully connected layer to combine board state, move, and player turn into a single vector\n",
        "        total_input_size = self.board_processing_size + move_size + player_size\n",
        "        self.fc = nn.Linear(total_input_size, hidden_size)\n",
        "\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, board, move, player, hidden):\n",
        "        # Assuming board is a 8x8 grid, reshape it for the convolutional layer\n",
        "        # print(\"Board shape\",board.shape)\n",
        "        board = board.float()\n",
        "        board = board.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n",
        "        # print(\"Board after unsqueeze\",board.shape)\n",
        "        board = F.relu(self.conv1(board))\n",
        "        board = F.relu(self.conv2(board))\n",
        "        board = board.view(1, -1)  # Flatten the output for the fully connected layer\n",
        "        # print(\"Board after view\",board.shape)\n",
        "        move = move.float().view(1, -1)\n",
        "        # print(\"Move shape\",move.shape)\n",
        "        player = player.float().view(1, -1)\n",
        "        # print(\"Player shape\",player.shape)\n",
        "        # Concatenate the processed board with move and player turn representations\n",
        "        combined = torch.cat((board, move, player), 1)\n",
        "        # print(\"Combined shape\",combined.shape)\n",
        "\n",
        "        # Pass the combined vector through the fully connected layer\n",
        "        combined = F.relu(self.fc(combined))\n",
        "\n",
        "        # GRU expects inputs of shape (seq_len, batch, input_size), seq_len and batch are 1 in this case\n",
        "        combined = combined.unsqueeze(0)\n",
        "        output, hidden = self.gru(combined, hidden)\n",
        "        # output = output.unsqueeze(0)  # Add a batch dimension if not already present\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size)\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=100):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attn = nn.Linear(hidden_size * 2, 1)  # Adjusted to expected sequence length of encoder_outputs\n",
        "        self.attn_combine = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # Ensure encoder_outputs is 3D and properly shaped for bmm\n",
        "        if encoder_outputs.dim() == 2:\n",
        "            encoder_outputs = encoder_outputs.unsqueeze(0)  # Adding batch dimension if missing\n",
        "\n",
        "        attn_weights = torch.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "\n",
        "        # attn_weights needs to be 3D to use bmm, and ensure it matches the batch size\n",
        "        attn_weights = attn_weights.unsqueeze(0)\n",
        "\n",
        "        # Ensuring both tensors are 3D\n",
        "        # print(f\"attn_weights shape: {attn_weights.shape}\")  # Should be [1, 1, sequence_length]\n",
        "        # print(f\"encoder_outputs shape: {encoder_outputs.shape}\")  # Should be [1, sequence_length, features]\n",
        "\n",
        "        attn_applied = torch.bmm(attn_weights, encoder_outputs)\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied.squeeze(0)), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = torch.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = torch.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "\n",
        "\n",
        "# class AttnDecoderRNN(nn.Module):\n",
        "#     def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=100):\n",
        "#         super(AttnDecoderRNN, self).__init__()\n",
        "#         self.hidden_size = hidden_size\n",
        "#         self.output_size = output_size\n",
        "#         self.dropout_p = dropout_p\n",
        "#         self.max_length = max_length\n",
        "\n",
        "#         self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "#         self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "#         self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "#         self.dropout = nn.Dropout(self.dropout_p)\n",
        "#         self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "#         self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "#     def forward(self, input, hidden, encoder_outputs):\n",
        "#         embedded = self.embedding(input).view(1, 1, -1)\n",
        "#         embedded = self.dropout(embedded)\n",
        "\n",
        "#         attn_weights = torch.softmax(\n",
        "#             self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "#         attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "#                                  encoder_outputs.unsqueeze(0))\n",
        "\n",
        "#         output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "#         output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "#         output = torch.relu(output)\n",
        "#         output, hidden = self.gru(output, hidden)\n",
        "\n",
        "#         output = torch.log_softmax(self.out(output[0]), dim=1)\n",
        "#         return output, hidden, attn_weights\n",
        "\n",
        "#     def initHidden(self):\n",
        "#         return torch.zeros(1, 1, self.hidden_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "lNDzKsPV0SHV"
      },
      "outputs": [],
      "source": [
        "EOS_token = tokenizer.eos_token_id\n",
        "SOS_token = tokenizer.bos_token_id\n",
        "\n",
        "def train(chess_dataset, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, device, epoch):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for idx in range(len(chess_dataset)):\n",
        "        data = chess_dataset[idx]\n",
        "\n",
        "        # Prepare data inputs and targets\n",
        "        board = data[\"board\"].to(device)\n",
        "        move = data[\"move\"].to(device)\n",
        "        player = data[\"turn\"].to(device)\n",
        "        comment = data[\"comment\"].squeeze(0).to(device)  # Remove extra dimensions\n",
        "        target_length = comment.size(0)\n",
        "\n",
        "        # Initialize hidden state and zero the gradients\n",
        "        encoder_hidden = encoder.initHidden().to(device)\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        loss = 0\n",
        "\n",
        "        # Forward pass through encoder\n",
        "        encoder_output, encoder_hidden = encoder(board, move, player, encoder_hidden)\n",
        "\n",
        "        # Decoder input starts with the SOS token\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # Adjust SOS_token definition and device\n",
        "\n",
        "        # Forward batch of sequences through decoder one time step at a time\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, encoder_hidden, encoder_output)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # Next input is previous output\n",
        "\n",
        "            loss += criterion(decoder_output, comment[di].unsqueeze(0))\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        average_loss = total_loss / len(chess_dataset)\n",
        "        print(f'Average Loss: {average_loss:.4f}')\n",
        "\n",
        "    return total_loss / len(chess_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "nAM8hR0P0SHV"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder = EncoderRNN(64, 1, 1, 256).to(device)\n",
        "vocab_size = tokenizer.vocab_size  # GPT-2 tokenizer's vocabulary size\n",
        "decoder = AttnDecoderRNN(256, vocab_size).to(device)\n",
        "encoder_optimizer = optim.SGD(encoder.parameters(), lr=0.01)\n",
        "decoder_optimizer = optim.SGD(decoder.parameters(), lr=0.01)\n",
        "criterion = nn.NLLLoss().to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ye5kiRBWxdtq",
        "outputId": "e35a50bb-456f-489d-e3dd-75c066e9d11f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-50-020a9c49831d>:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"comment\": torch.tensor(tokens, dtype=torch.long),\n",
            "<ipython-input-50-020a9c49831d>:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Loss: 0.0882\n",
            "Average Loss: 0.3328\n",
            "Average Loss: 0.3573\n",
            "Average Loss: 0.3805\n",
            "Average Loss: 0.5488\n",
            "Average Loss: 0.6633\n",
            "Average Loss: 0.6848\n",
            "Average Loss: 0.7502\n",
            "Average Loss: 0.8126\n",
            "Average Loss: 0.8307\n",
            "Average Loss: 1.0509\n",
            "Average Loss: 1.0678\n",
            "Average Loss: 1.0817\n",
            "Average Loss: 1.2465\n",
            "Average Loss: 1.2587\n",
            "Average Loss: 1.2679\n",
            "Average Loss: 1.2745\n",
            "Average Loss: 1.5273\n",
            "Average Loss: 1.5544\n",
            "Average Loss: 1.5782\n",
            "Average Loss: 1.6245\n",
            "Average Loss: 1.6767\n",
            "Average Loss: 1.8543\n",
            "Average Loss: 1.8773\n",
            "Average Loss: 2.1881\n",
            "Average Loss: 2.2089\n",
            "Average Loss: 2.2256\n",
            "Average Loss: 2.2390\n",
            "Average Loss: 2.2497\n",
            "Average Loss: 2.2777\n",
            "Average Loss: 2.2866\n",
            "Average Loss: 2.2936\n",
            "Average Loss: 2.4078\n",
            "Average Loss: 2.4208\n",
            "Average Loss: 2.5337\n",
            "Average Loss: 2.5459\n",
            "Average Loss: 2.5550\n",
            "Average Loss: 2.5618\n",
            "Average Loss: 2.5671\n",
            "Average Loss: 2.5717\n",
            "Average Loss: 2.5761\n",
            "Average Loss: 2.7831\n",
            "Average Loss: 2.8785\n",
            "Average Loss: 2.9014\n",
            "Average Loss: 2.9221\n",
            "Average Loss: 2.9407\n",
            "Average Loss: 2.9579\n",
            "Average Loss: 2.9731\n",
            "Average Loss: 2.9860\n",
            "Average Loss: 2.9967\n",
            "Average Loss: 3.0051\n",
            "Average Loss: 3.0128\n",
            "Average Loss: 3.0604\n",
            "Average Loss: 3.1927\n",
            "Average Loss: 3.2034\n",
            "Average Loss: 3.2115\n",
            "Average Loss: 3.2212\n",
            "Average Loss: 3.2260\n",
            "Average Loss: 3.3966\n",
            "Average Loss: 3.4140\n",
            "Average Loss: 3.4283\n",
            "Average Loss: 3.4400\n",
            "Average Loss: 3.4496\n",
            "Average Loss: 3.7450\n",
            "Average Loss: 3.7597\n",
            "Average Loss: 3.7702\n",
            "Average Loss: 3.8040\n",
            "Average Loss: 3.8126\n",
            "Average Loss: 3.8195\n",
            "Average Loss: 3.8250\n",
            "Average Loss: 3.8294\n",
            "Average Loss: 3.8338\n",
            "Average Loss: 4.0617\n",
            "Average Loss: 4.0804\n",
            "Average Loss: 4.0967\n",
            "Average Loss: 4.1111\n",
            "Average Loss: 4.1242\n",
            "Average Loss: 4.1357\n",
            "Average Loss: 4.1460\n",
            "Average Loss: 4.2642\n",
            "Average Loss: 4.4512\n",
            "Average Loss: 4.4634\n",
            "Average Loss: 4.4722\n",
            "Average Loss: 4.4789\n",
            "Average Loss: 4.5259\n",
            "Average Loss: 4.5325\n",
            "Average Loss: 4.5377\n",
            "Average Loss: 4.5417\n",
            "Average Loss: 4.7194\n",
            "Average Loss: 4.7323\n",
            "Average Loss: 4.7437\n",
            "Average Loss: 4.7535\n",
            "Average Loss: 5.1563\n",
            "Average Loss: 5.1749\n",
            "Average Loss: 5.3800\n",
            "Average Loss: 5.3926\n",
            "Average Loss: 5.4031\n",
            "Average Loss: 5.4116\n",
            "Average Loss: 5.4184\n",
            "Average Loss: 5.4239\n",
            "Average Loss: 5.4981\n",
            "Average Loss: 5.5049\n",
            "Average Loss: 5.6789\n",
            "Average Loss: 5.6874\n",
            "Average Loss: 5.6941\n",
            "Average Loss: 5.9200\n",
            "Average Loss: 5.9292\n",
            "Average Loss: 5.9364\n",
            "Average Loss: 5.9421\n",
            "Average Loss: 5.9472\n",
            "Average Loss: 5.9513\n",
            "Average Loss: 5.9547\n",
            "Average Loss: 6.1210\n",
            "Average Loss: 6.1336\n",
            "Average Loss: 6.1441\n",
            "Average Loss: 6.1529\n",
            "Average Loss: 6.1603\n",
            "Average Loss: 6.1671\n",
            "Average Loss: 6.1727\n",
            "Average Loss: 6.2409\n",
            "Average Loss: 6.2457\n",
            "Average Loss: 6.2505\n",
            "Average Loss: 6.3661\n",
            "Average Loss: 6.3726\n",
            "Average Loss: 6.3774\n",
            "Average Loss: 6.3823\n",
            "Average Loss: 6.4030\n",
            "Average Loss: 6.5182\n",
            "Average Loss: 6.5871\n",
            "Average Loss: 6.6359\n",
            "Average Loss: 6.6441\n",
            "Average Loss: 6.6495\n",
            "Average Loss: 6.6544\n",
            "Average Loss: 6.6582\n",
            "Average Loss: 6.6613\n",
            "Average Loss: 6.7370\n",
            "Average Loss: 6.7430\n",
            "Average Loss: 6.7998\n",
            "Average Loss: 6.8059\n",
            "Average Loss: 6.8103\n",
            "Average Loss: 6.8137\n",
            "Average Loss: 6.8167\n",
            "Average Loss: 6.8190\n",
            "Average Loss: 6.8209\n",
            "Average Loss: 6.9027\n",
            "Average Loss: 6.9078\n",
            "Average Loss: 6.9458\n",
            "Average Loss: 6.9705\n",
            "Average Loss: 6.9763\n",
            "Average Loss: 6.9803\n",
            "Average Loss: 6.9833\n",
            "Average Loss: 6.9856\n",
            "Average Loss: 6.9874\n",
            "Average Loss: 6.9889\n",
            "Average Loss: 6.9902\n",
            "Average Loss: 7.0719\n",
            "Average Loss: 7.0771\n",
            "Average Loss: 7.1241\n",
            "Average Loss: 7.1295\n",
            "Average Loss: 7.1331\n",
            "Average Loss: 7.1360\n",
            "Average Loss: 7.1382\n",
            "Average Loss: 7.1398\n",
            "Average Loss: 7.1411\n",
            "Average Loss: 7.1422\n",
            "Average Loss: 7.2578\n",
            "Average Loss: 7.3099\n",
            "Average Loss: 7.3148\n",
            "Average Loss: 7.3180\n",
            "Average Loss: 7.3207\n",
            "Average Loss: 7.3227\n",
            "Average Loss: 7.3242\n",
            "Average Loss: 7.3253\n",
            "Average Loss: 7.3261\n",
            "Average Loss: 7.3268\n",
            "Average Loss: 7.3273\n",
            "Average Loss: 7.3278\n",
            "Average Loss: 7.3282\n",
            "Average Loss: 7.3285\n",
            "Average Loss: 7.3288\n",
            "Average Loss: 7.3291\n",
            "Average Loss: 7.3293\n",
            "Average Loss: 7.3295\n",
            "Average Loss: 7.3297\n",
            "Average Loss: 7.3299\n",
            "Average Loss: 7.3301\n",
            "Average Loss: 7.3303\n",
            "Average Loss: 7.3304\n",
            "Average Loss: 7.3306\n",
            "Average Loss: 7.3308\n",
            "Average Loss: 7.3309\n",
            "Average Loss: 7.3310\n",
            "Average Loss: 7.3311\n",
            "Average Loss: 7.3313\n",
            "Average Loss: 7.3314\n",
            "Average Loss: 7.6990\n",
            "Average Loss: 7.8301\n",
            "Average Loss: 7.8363\n",
            "Average Loss: 7.8395\n",
            "Average Loss: 7.9429\n",
            "Average Loss: 8.0802\n",
            "Average Loss: 8.0918\n",
            "Average Loss: 8.1006\n",
            "Average Loss: 8.3856\n",
            "Average Loss: 8.3975\n",
            "Average Loss: 8.4046\n",
            "Average Loss: 8.4983\n",
            "Average Loss: 8.5055\n",
            "Average Loss: 8.5107\n",
            "Average Loss: 8.5142\n",
            "Average Loss: 8.5165\n",
            "Average Loss: 8.6454\n",
            "Average Loss: 8.6507\n",
            "Average Loss: 8.6544\n",
            "Average Loss: 8.7272\n",
            "Average Loss: 8.7307\n",
            "Average Loss: 8.7331\n",
            "Average Loss: 8.7346\n",
            "Average Loss: 8.7355\n",
            "Average Loss: 8.7364\n",
            "Average Loss: 8.7368\n",
            "Average Loss: 8.9076\n",
            "Average Loss: 9.0393\n",
            "Average Loss: 9.1785\n",
            "Average Loss: 9.1865\n",
            "Average Loss: 9.1918\n",
            "Average Loss: 9.1956\n",
            "Average Loss: 9.1984\n",
            "Average Loss: 9.2006\n",
            "Average Loss: 9.4008\n",
            "Average Loss: 9.4137\n",
            "Average Loss: 9.4593\n",
            "Average Loss: 9.4650\n",
            "Average Loss: 9.4681\n",
            "Average Loss: 9.4703\n",
            "Average Loss: 9.4715\n",
            "Average Loss: 9.5886\n",
            "Average Loss: 9.5938\n",
            "Average Loss: 9.8219\n",
            "Average Loss: 9.8420\n",
            "Average Loss: 9.8531\n",
            "Average Loss: 9.8570\n",
            "Average Loss: 9.8908\n",
            "Average Loss: 9.8951\n",
            "Average Loss: 9.8980\n",
            "Average Loss: 9.9004\n",
            "Average Loss: 9.9023\n",
            "Average Loss: 9.9037\n",
            "Average Loss: 9.9621\n",
            "Average Loss: 9.9648\n",
            "Average Loss: 9.9660\n",
            "Average Loss: 9.9667\n",
            "Average Loss: 10.0863\n",
            "Average Loss: 10.1267\n",
            "Average Loss: 10.1323\n",
            "Average Loss: 10.1366\n",
            "Average Loss: 10.2202\n",
            "Average Loss: 10.2251\n",
            "Average Loss: 10.2286\n",
            "Average Loss: 10.2313\n",
            "Average Loss: 10.2336\n",
            "Average Loss: 10.2353\n",
            "Average Loss: 10.2367\n",
            "Average Loss: 10.2378\n",
            "Average Loss: 10.2385\n",
            "Average Loss: 10.2390\n",
            "Average Loss: 10.3596\n",
            "Average Loss: 10.3630\n",
            "Average Loss: 10.3659\n",
            "Average Loss: 10.3684\n",
            "Average Loss: 10.3705\n",
            "Average Loss: 10.3723\n",
            "Average Loss: 10.4383\n",
            "Average Loss: 10.4403\n",
            "Average Loss: 10.6074\n",
            "Average Loss: 10.6105\n",
            "Average Loss: 10.6121\n",
            "Average Loss: 10.6356\n",
            "Average Loss: 10.6375\n",
            "Average Loss: 10.6386\n",
            "Average Loss: 10.6393\n",
            "Average Loss: 10.6399\n",
            "Average Loss: 10.7151\n",
            "Average Loss: 10.7164\n",
            "Average Loss: 10.7172\n",
            "Average Loss: 10.7177\n",
            "Average Loss: 10.7181\n",
            "Average Loss: 10.7367\n",
            "Average Loss: 10.7376\n",
            "Average Loss: 10.7382\n",
            "Average Loss: 10.8884\n",
            "Average Loss: 10.8902\n",
            "Average Loss: 11.0646\n",
            "Average Loss: 11.0669\n",
            "Average Loss: 11.0681\n",
            "Average Loss: 11.2296\n",
            "Average Loss: 11.3717\n",
            "Average Loss: 11.4281\n",
            "Average Loss: 11.4383\n",
            "Average Loss: 11.5538\n",
            "Average Loss: 11.5865\n",
            "Average Loss: 11.5924\n",
            "Average Loss: 11.5963\n",
            "Average Loss: 11.5990\n",
            "Average Loss: 11.6011\n",
            "Average Loss: 11.7355\n",
            "Average Loss: 11.7386\n",
            "Average Loss: 11.7406\n",
            "Average Loss: 11.8296\n",
            "Average Loss: 11.8323\n",
            "Average Loss: 11.8337\n",
            "Average Loss: 11.8347\n",
            "Average Loss: 11.8353\n",
            "Average Loss: 11.9775\n",
            "Average Loss: 11.9817\n",
            "Average Loss: 12.1578\n",
            "Average Loss: 12.2563\n",
            "Average Loss: 12.2612\n",
            "Average Loss: 12.2649\n",
            "Average Loss: 12.2930\n",
            "Average Loss: 12.2966\n",
            "Average Loss: 12.2993\n",
            "Average Loss: 12.3014\n",
            "Average Loss: 12.3031\n",
            "Average Loss: 12.3044\n",
            "Average Loss: 12.3727\n",
            "Average Loss: 12.5041\n",
            "Average Loss: 12.5139\n",
            "Average Loss: 12.5162\n",
            "Average Loss: 12.5176\n",
            "Average Loss: 12.5184\n",
            "Average Loss: 12.5191\n",
            "Average Loss: 12.5958\n",
            "Average Loss: 12.5973\n",
            "Average Loss: 12.7036\n",
            "Average Loss: 12.7060\n",
            "Average Loss: 12.7072\n",
            "Average Loss: 12.7080\n",
            "Average Loss: 12.7086\n",
            "Average Loss: 12.7090\n",
            "Average Loss: 12.7689\n",
            "Average Loss: 12.7700\n",
            "Average Loss: 12.7707\n",
            "Average Loss: 12.7712\n",
            "Average Loss: 12.7716\n",
            "Average Loss: 12.7719\n",
            "Average Loss: 12.7722\n",
            "Average Loss: 12.8275\n",
            "Average Loss: 12.9550\n",
            "Average Loss: 12.9575\n",
            "Average Loss: 13.0973\n",
            "Average Loss: 13.0996\n",
            "Average Loss: 13.3534\n",
            "Average Loss: 13.3633\n",
            "Average Loss: 13.4734\n",
            "Average Loss: 13.5344\n",
            "Average Loss: 13.5386\n",
            "Average Loss: 13.5412\n",
            "Average Loss: 13.5428\n",
            "Average Loss: 13.5440\n",
            "Average Loss: 13.5447\n",
            "Average Loss: 13.5452\n",
            "Average Loss: 13.5456\n",
            "Average Loss: 13.5458\n",
            "Average Loss: 13.5461\n",
            "Average Loss: 13.6937\n",
            "Average Loss: 13.7073\n",
            "Average Loss: 13.7189\n",
            "Average Loss: 13.7287\n",
            "Average Loss: 13.7362\n",
            "Average Loss: 13.7422\n",
            "Average Loss: 13.7466\n",
            "Average Loss: 13.7500\n",
            "Average Loss: 13.8356\n",
            "Average Loss: 13.8385\n",
            "Average Loss: 13.8405\n",
            "Average Loss: 13.8420\n",
            "Average Loss: 13.8430\n",
            "Average Loss: 13.8437\n",
            "Average Loss: 13.8442\n",
            "Average Loss: 13.8446\n",
            "Average Loss: 13.9291\n",
            "Average Loss: 13.9332\n",
            "Average Loss: 13.9364\n",
            "Average Loss: 13.9392\n",
            "Average Loss: 13.9416\n",
            "Average Loss: 13.9437\n",
            "Average Loss: 14.0606\n",
            "Average Loss: 14.0635\n",
            "Average Loss: 14.0656\n",
            "Average Loss: 14.0671\n",
            "Average Loss: 14.0682\n",
            "Average Loss: 14.0691\n",
            "Average Loss: 14.1373\n",
            "Average Loss: 14.1393\n",
            "Average Loss: 14.1407\n",
            "Average Loss: 14.2028\n",
            "Average Loss: 14.2993\n",
            "Average Loss: 14.3025\n",
            "Average Loss: 14.3044\n",
            "Average Loss: 14.4697\n",
            "Average Loss: 14.4763\n",
            "Average Loss: 14.4795\n",
            "Average Loss: 14.4818\n",
            "Average Loss: 14.5465\n",
            "Average Loss: 14.5489\n",
            "Average Loss: 14.5506\n",
            "Average Loss: 14.5517\n",
            "Average Loss: 14.5525\n",
            "Average Loss: 14.5530\n",
            "Average Loss: 14.5535\n",
            "Average Loss: 14.5538\n",
            "Average Loss: 14.5541\n",
            "Average Loss: 14.5543\n"
          ]
        }
      ],
      "source": [
        "train(chessdata, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, device, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "3XxeQWB13q1v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "AYQkUfR0xdtq"
      },
      "outputs": [],
      "source": [
        "test = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/566 Project/testdata.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test[\"Comment\"] = test[\"Comment\"].fillna('This move does not need a comment')"
      ],
      "metadata": {
        "id": "mPdWESwO44LQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "board_states = test[\"Board\"].apply(lambda x : create_board_array(x))\n",
        "tokenized_comments = test[\"Comment\"].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
        "moves = test[\"Move\"].apply(lambda x: UCI_MOVES[x])\n",
        "turn = test[\"Player\"].apply(lambda x: TURN[x])\n",
        "testdata = ChessDataset(board_states, moves ,turn,test[\"Comment\"],test['Move Number'],test['Game Number'])"
      ],
      "metadata": {
        "id": "aypDaCMs4Uux"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(test_dataset, encoder, decoder, criterion, device):\n",
        "    encoder.eval()  # Set the encoder to evaluation mode\n",
        "    decoder.eval()  # Set the decoder to evaluation mode\n",
        "\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():  # No need to track gradients\n",
        "        for idx in range(len(test_dataset)):  # Safely iterate based on dataset length\n",
        "            data = test_dataset[idx]\n",
        "            board = data[\"board\"].to(device)\n",
        "            move = data[\"move\"].to(device)\n",
        "            player = data[\"turn\"].to(device)\n",
        "            comment = data[\"comment\"].squeeze(0).to(device)\n",
        "            target_length = comment.size(0)\n",
        "\n",
        "            encoder_hidden = encoder.initHidden().to(device)\n",
        "\n",
        "            # Forward pass through encoder\n",
        "            encoder_output, encoder_hidden = encoder(board, move, player, encoder_hidden)\n",
        "\n",
        "            # Start decoding process\n",
        "            decoder_input = torch.tensor([[SOS_token]], device=device)  # Start with SOS token\n",
        "            decoder_hidden = encoder_hidden\n",
        "            loss = 0\n",
        "\n",
        "            for di in range(target_length):\n",
        "                decoder_output, decoder_hidden, _ = decoder(\n",
        "                    decoder_input, decoder_hidden, encoder_output)\n",
        "                topv, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze().detach()\n",
        "\n",
        "                loss += criterion(decoder_output, comment[di].unsqueeze(0))\n",
        "                if decoder_input.item() == EOS_token:\n",
        "                    break\n",
        "\n",
        "            total_loss += loss.item() / target_length\n",
        "\n",
        "    average_loss = total_loss / len(test_dataset)\n",
        "    print(f'Test Loss: {average_loss:.4f}')\n"
      ],
      "metadata": {
        "id": "XroAT_XD3-Yo"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming test_dataset is already created and preprocessed\n",
        "encoder = EncoderRNN(board_size=64, move_size=1, player_size=1, hidden_size=256)\n",
        "decoder = AttnDecoderRNN(hidden_size=256, output_size=tokenizer.vocab_size, dropout_p=0.1, max_length=100)\n",
        "\n",
        "# Load saved models (if saved previously)\n",
        "# encoder.load_state_dict(torch.load('encoder.pth'))\n",
        "# decoder.load_state_dict(torch.load('decoder.pth'))\n",
        "\n",
        "# Move models to the appropriate device\n",
        "encoder.to(device)\n",
        "decoder.to(device)\n",
        "\n",
        "# Run the test\n",
        "test_model(testdata, encoder, decoder, criterion, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJzeOC8t4MRT",
        "outputId": "f42bafa4-2a17-4ca1-e986-070b073c41d5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-020a9c49831d>:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"comment\": torch.tensor(tokens, dtype=torch.long),\n",
            "<ipython-input-11-020a9c49831d>:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 10.8610\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_commentary(encoder, decoder, board, move, player, device, max_length=50):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Prepare input tensors and move to the appropriate device\n",
        "        board_tensor = board.to(device)\n",
        "        move_tensor = move.to(device)\n",
        "        player_tensor = player.to(device)\n",
        "\n",
        "        # Initialize hidden state and encode the inputs\n",
        "        encoder_hidden = encoder.initHidden().to(device)\n",
        "        encoder_output, encoder_hidden = encoder(board_tensor, move_tensor, player_tensor, encoder_hidden)\n",
        "\n",
        "        # Start decoding process\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # Start with SOS token\n",
        "        decoder_hidden = encoder_hidden\n",
        "        output_tokens = []\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            decoder_output, decoder_hidden, _ = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_output)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            next_token = topi.item()\n",
        "            output_tokens.append(next_token)\n",
        "\n",
        "            if next_token == EOS_token:\n",
        "                break\n",
        "\n",
        "            decoder_input = topi.squeeze().detach().unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "        # Convert token IDs to text\n",
        "        generated_text = tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
        "        return generated_text\n"
      ],
      "metadata": {
        "id": "Z5v82xMn5B6c"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testdata[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTsRaarI-VUf",
        "outputId": "ab2d2a1f-48a1-493e-f889-00a4cb1f19aa"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-020a9c49831d>:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"comment\": torch.tensor(tokens, dtype=torch.long),\n",
            "<ipython-input-11-020a9c49831d>:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'board': tensor([[ 5,  7,  9, 11, 13,  9,  7,  5],\n",
              "         [ 3,  3,  3,  3,  3,  3,  3,  3],\n",
              "         [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
              "         [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
              "         [ 0,  0,  0,  0,  2,  0,  0,  0],\n",
              "         [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
              "         [ 2,  2,  2,  2,  0,  2,  2,  2],\n",
              "         [ 4,  6,  8, 10, 12,  8,  6,  4]]),\n",
              " 'move': tensor(890),\n",
              " 'turn': tensor(0),\n",
              " 'comment': tensor([[18521,   257, 28799, 35824,   393,  4141,  5947,   994,  2619, 14245,\n",
              "           4113,   257, 29649,   287,  2166,   286,  2631,    19,    11,   290,\n",
              "           1139,   366,  3919,  2252,  1911,  2635,   468,  3088,   617,  3257,\n",
              "           5260,    11,   884,   355,   257,  2677,   338, 14014,  2545,    11,\n",
              "            284,   651,  5755,   286,  2631,    20,    11,   475,   262,   517,\n",
              "           2219,  3164,   777,  1528,   318,   284,  4174,  3833,   284,  2619,\n",
              "            338,  2292,   477,   625,   262,  3096,    13]]),\n",
              " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
              " 'move_number': tensor(2),\n",
              " 'game_id': tensor(4)}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W6g2NxyF-UpX"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_commentary(encoder, decoder, testdata[1]['board'], testdata[1]['move'], testdata[1]['turn'], device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "ZmuThAtX-RHs",
        "outputId": "89b4a57a-602e-499f-d56b-6de2413b47ad"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-020a9c49831d>:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"comment\": torch.tensor(tokens, dtype=torch.long),\n",
            "<ipython-input-11-020a9c49831d>:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Format chart Sand Sand Rel Sand campaigning campaigningãƒ¯ Sand campaigning campaigningãƒ¯ Sand campaigning campaigningãƒ¯ Sand campaigning campaigningãƒ¯ Sand campaigning campaigningãƒ¯ Sand campaigning campaigningãƒ¯ Sand campaigning campaigningãƒ¯ Sand campaigning campaigningãƒ¯ Sand campaigning campaigningãƒ¯ Sand campaigning campaigningãƒ¯ Sand campaigning campaigningãƒ¯ Sand'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x4UBiQrB-uGW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}