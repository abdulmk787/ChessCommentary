{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8dE3wqIqIxxv",
    "outputId": "250c4fab-de2b-4891-b0bb-b9df34325cd8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.16.6-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.0)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-1.45.0-py2.py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (68.2.2)\n",
      "Collecting appdirs>=1.4.3 (from wandb)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting protobuf!=4.21.0,<5,>=3.19.0 (from wandb)\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.4.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading wandb-0.16.6-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m156.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.40.1-py3-none-any.whl (9.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m158.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.4.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m120.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-1.45.0-py2.py3-none-any.whl (267 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.1/267.1 kB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: appdirs, tzdata, smmap, setproctitle, sentry-sdk, safetensors, regex, protobuf, docker-pycreds, pandas, huggingface-hub, gitdb, tokenizers, GitPython, wandb, transformers\n",
      "Successfully installed GitPython-3.1.43 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.11 huggingface-hub-0.22.2 pandas-2.2.2 protobuf-4.25.3 regex-2024.4.16 safetensors-0.4.3 sentry-sdk-1.45.0 setproctitle-1.3.3 smmap-5.0.1 tokenizers-0.19.1 transformers-4.40.1 tzdata-2024.1 wandb-0.16.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install wandb pandas transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eywDwjFDLCnk",
    "outputId": "7a3b4ca3-8d8a-4bcc-dc13-fe3d732d54d9"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wyytxRKF8lQ3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import wandb\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "Q18RO4uBI901",
    "outputId": "202eab28-7125-4299-a134-5d70076c85cc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "-J0Qxrjf8WbF",
    "outputId": "186b05b3-14ff-4a59-e79b-ba6383a91672"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HkwUL1OF8jTX"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "7LAZ4G8D8rof",
    "outputId": "c553ced5-a4b7-4706-f9fc-494b43dc185e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Game Number</th>\n",
       "      <th>Move Number</th>\n",
       "      <th>Player</th>\n",
       "      <th>Move</th>\n",
       "      <th>Board</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2263</td>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "      <td>d2d4</td>\n",
       "      <td>r n b q k b n r\\np p p p p p p p\\n. . . . . . ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2263</td>\n",
       "      <td>2</td>\n",
       "      <td>Black</td>\n",
       "      <td>g8f6</td>\n",
       "      <td>r n b q k b n r\\np p p p p p p p\\n. . . . . . ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2263</td>\n",
       "      <td>3</td>\n",
       "      <td>White</td>\n",
       "      <td>c2c4</td>\n",
       "      <td>r n b q k b . r\\np p p p p p p p\\n. . . . . n ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2263</td>\n",
       "      <td>4</td>\n",
       "      <td>Black</td>\n",
       "      <td>g7g6</td>\n",
       "      <td>r n b q k b . r\\np p p p p p p p\\n. . . . . n ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2263</td>\n",
       "      <td>5</td>\n",
       "      <td>White</td>\n",
       "      <td>b1c3</td>\n",
       "      <td>r n b q k b . r\\np p p p p p . p\\n. . . . . n ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184924</th>\n",
       "      <td>3</td>\n",
       "      <td>94</td>\n",
       "      <td>Black</td>\n",
       "      <td>h5h3</td>\n",
       "      <td>. . . . . . . .\\n. . . . k . p .\\n. p n . . p ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184925</th>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "      <td>White</td>\n",
       "      <td>b1b6</td>\n",
       "      <td>. . . . . . . .\\n. . . . k . p .\\n. p n . . p ...</td>\n",
       "      <td>There goes the extra pawn.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184926</th>\n",
       "      <td>3</td>\n",
       "      <td>96</td>\n",
       "      <td>Black</td>\n",
       "      <td>e7d6</td>\n",
       "      <td>. . . . . . . .\\n. . . . k . p .\\n. R n . . p ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184927</th>\n",
       "      <td>3</td>\n",
       "      <td>97</td>\n",
       "      <td>White</td>\n",
       "      <td>g1g2</td>\n",
       "      <td>. . . . . . . .\\n. . . . . . p .\\n. R n k . p ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184928</th>\n",
       "      <td>3</td>\n",
       "      <td>98</td>\n",
       "      <td>Black</td>\n",
       "      <td>h3c3</td>\n",
       "      <td>. . . . . . . .\\n. . . . . . p .\\n. R n k . p ...</td>\n",
       "      <td>After 50. Bg3+ Black's Pg7 might well fall, so...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>184929 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Game Number  Move Number Player  Move  \\\n",
       "0              2263            1  White  d2d4   \n",
       "1              2263            2  Black  g8f6   \n",
       "2              2263            3  White  c2c4   \n",
       "3              2263            4  Black  g7g6   \n",
       "4              2263            5  White  b1c3   \n",
       "...             ...          ...    ...   ...   \n",
       "184924            3           94  Black  h5h3   \n",
       "184925            3           95  White  b1b6   \n",
       "184926            3           96  Black  e7d6   \n",
       "184927            3           97  White  g1g2   \n",
       "184928            3           98  Black  h3c3   \n",
       "\n",
       "                                                    Board  \\\n",
       "0       r n b q k b n r\\np p p p p p p p\\n. . . . . . ...   \n",
       "1       r n b q k b n r\\np p p p p p p p\\n. . . . . . ...   \n",
       "2       r n b q k b . r\\np p p p p p p p\\n. . . . . n ...   \n",
       "3       r n b q k b . r\\np p p p p p p p\\n. . . . . n ...   \n",
       "4       r n b q k b . r\\np p p p p p . p\\n. . . . . n ...   \n",
       "...                                                   ...   \n",
       "184924  . . . . . . . .\\n. . . . k . p .\\n. p n . . p ...   \n",
       "184925  . . . . . . . .\\n. . . . k . p .\\n. p n . . p ...   \n",
       "184926  . . . . . . . .\\n. . . . k . p .\\n. R n . . p ...   \n",
       "184927  . . . . . . . .\\n. . . . . . p .\\n. R n k . p ...   \n",
       "184928  . . . . . . . .\\n. . . . . . p .\\n. R n k . p ...   \n",
       "\n",
       "                                                  Comment  \n",
       "0                                                     NaN  \n",
       "1                                                     NaN  \n",
       "2                                                     NaN  \n",
       "3                                                     NaN  \n",
       "4                                                     NaN  \n",
       "...                                                   ...  \n",
       "184924                                                NaN  \n",
       "184925                         There goes the extra pawn.  \n",
       "184926                                                NaN  \n",
       "184927                                                NaN  \n",
       "184928  After 50. Bg3+ Black's Pg7 might well fall, so...  \n",
       "\n",
       "[184929 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4GDujLeb8uvV"
   },
   "outputs": [],
   "source": [
    "df = df.dropna().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Game Number</th>\n",
       "      <th>Move Number</th>\n",
       "      <th>Player</th>\n",
       "      <th>Move</th>\n",
       "      <th>Board</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>2263</td>\n",
       "      <td>23</td>\n",
       "      <td>White</td>\n",
       "      <td>e1d2</td>\n",
       "      <td>r n b . . r k .\\np p . . p p b p\\n. . . . . . ...</td>\n",
       "      <td>(In this line White has to accept some risks. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>2263</td>\n",
       "      <td>25</td>\n",
       "      <td>White</td>\n",
       "      <td>d4d5</td>\n",
       "      <td>r n b r . . k .\\np p . . p p b p\\n. . . . . . ...</td>\n",
       "      <td>(The first step of White's strategical plan: t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>2263</td>\n",
       "      <td>26</td>\n",
       "      <td>Black</td>\n",
       "      <td>b8c6</td>\n",
       "      <td>r n b r . . k .\\np p . . p p b p\\n. . . . . . ...</td>\n",
       "      <td>(Taking advantage of the pin.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>2263</td>\n",
       "      <td>29</td>\n",
       "      <td>White</td>\n",
       "      <td>d5d6</td>\n",
       "      <td>r . b r . . k .\\np p . . . p b p\\n. . n . p . ...</td>\n",
       "      <td>(The second step.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>2263</td>\n",
       "      <td>30</td>\n",
       "      <td>Black</td>\n",
       "      <td>b7b6</td>\n",
       "      <td>r . b r . . k .\\np p . . . p b p\\n. . n P p . ...</td>\n",
       "      <td>(What about 15...e5? A good plan would be then...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20768</th>\n",
       "      <td>184912</td>\n",
       "      <td>3</td>\n",
       "      <td>82</td>\n",
       "      <td>Black</td>\n",
       "      <td>d6f7</td>\n",
       "      <td>. . . . . . . .\\n. . . . k . p .\\n. p q n . p ...</td>\n",
       "      <td>Going toward the king-side isn't necessary now...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20769</th>\n",
       "      <td>184915</td>\n",
       "      <td>3</td>\n",
       "      <td>85</td>\n",
       "      <td>White</td>\n",
       "      <td>f3e5</td>\n",
       "      <td>. . . . . . . .\\n. . . . k n p .\\n. p q . . p ...</td>\n",
       "      <td>a very nifty tactical shot!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20770</th>\n",
       "      <td>184920</td>\n",
       "      <td>3</td>\n",
       "      <td>90</td>\n",
       "      <td>Black</td>\n",
       "      <td>f7g5</td>\n",
       "      <td>. . . . . . . .\\n. . . . k n p .\\n. p n . . p ...</td>\n",
       "      <td>Kasparov has won a pawn, but his attack on Kg1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20771</th>\n",
       "      <td>184925</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "      <td>White</td>\n",
       "      <td>b1b6</td>\n",
       "      <td>. . . . . . . .\\n. . . . k . p .\\n. p n . . p ...</td>\n",
       "      <td>There goes the extra pawn.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20772</th>\n",
       "      <td>184928</td>\n",
       "      <td>3</td>\n",
       "      <td>98</td>\n",
       "      <td>Black</td>\n",
       "      <td>h3c3</td>\n",
       "      <td>. . . . . . . .\\n. . . . . . p .\\n. R n k . p ...</td>\n",
       "      <td>After 50. Bg3+ Black's Pg7 might well fall, so...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20773 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  Game Number  Move Number Player  Move  \\\n",
       "0          22         2263           23  White  e1d2   \n",
       "1          24         2263           25  White  d4d5   \n",
       "2          25         2263           26  Black  b8c6   \n",
       "3          28         2263           29  White  d5d6   \n",
       "4          29         2263           30  Black  b7b6   \n",
       "...       ...          ...          ...    ...   ...   \n",
       "20768  184912            3           82  Black  d6f7   \n",
       "20769  184915            3           85  White  f3e5   \n",
       "20770  184920            3           90  Black  f7g5   \n",
       "20771  184925            3           95  White  b1b6   \n",
       "20772  184928            3           98  Black  h3c3   \n",
       "\n",
       "                                                   Board  \\\n",
       "0      r n b . . r k .\\np p . . p p b p\\n. . . . . . ...   \n",
       "1      r n b r . . k .\\np p . . p p b p\\n. . . . . . ...   \n",
       "2      r n b r . . k .\\np p . . p p b p\\n. . . . . . ...   \n",
       "3      r . b r . . k .\\np p . . . p b p\\n. . n . p . ...   \n",
       "4      r . b r . . k .\\np p . . . p b p\\n. . n P p . ...   \n",
       "...                                                  ...   \n",
       "20768  . . . . . . . .\\n. . . . k . p .\\n. p q n . p ...   \n",
       "20769  . . . . . . . .\\n. . . . k n p .\\n. p q . . p ...   \n",
       "20770  . . . . . . . .\\n. . . . k n p .\\n. p n . . p ...   \n",
       "20771  . . . . . . . .\\n. . . . k . p .\\n. p n . . p ...   \n",
       "20772  . . . . . . . .\\n. . . . . . p .\\n. R n k . p ...   \n",
       "\n",
       "                                                 Comment  \n",
       "0      (In this line White has to accept some risks. ...  \n",
       "1      (The first step of White's strategical plan: t...  \n",
       "2                         (Taking advantage of the pin.)  \n",
       "3                                     (The second step.)  \n",
       "4      (What about 15...e5? A good plan would be then...  \n",
       "...                                                  ...  \n",
       "20768  Going toward the king-side isn't necessary now...  \n",
       "20769                        a very nifty tactical shot!  \n",
       "20770  Kasparov has won a pawn, but his attack on Kg1...  \n",
       "20771                         There goes the extra pawn.  \n",
       "20772  After 50. Bg3+ Black's Pg7 might well fall, so...  \n",
       "\n",
       "[20773 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "gbPbFS5-9yoK",
    "outputId": "e190f317-cbe0-496d-f9b1-6859e3f1276a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Game Number</th>\n",
       "      <th>Move Number</th>\n",
       "      <th>Player</th>\n",
       "      <th>Move</th>\n",
       "      <th>Board</th>\n",
       "      <th>Comment</th>\n",
       "      <th>FEN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>2263</td>\n",
       "      <td>23</td>\n",
       "      <td>White</td>\n",
       "      <td>e1d2</td>\n",
       "      <td>r n b . . r k .\\np p . . p p b p\\n. . . . . . ...</td>\n",
       "      <td>(In this line White has to accept some risks. ...</td>\n",
       "      <td>rnb2rk1/pp2ppbp/6p1/8/3PP3/4BN2/P2q1PPP/1R2KB1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>2263</td>\n",
       "      <td>25</td>\n",
       "      <td>White</td>\n",
       "      <td>d4d5</td>\n",
       "      <td>r n b r . . k .\\np p . . p p b p\\n. . . . . . ...</td>\n",
       "      <td>(The first step of White's strategical plan: t...</td>\n",
       "      <td>rnbr2k1/pp2ppbp/6p1/8/3PP3/4BN2/P2K1PPP/1R3B1R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>2263</td>\n",
       "      <td>26</td>\n",
       "      <td>Black</td>\n",
       "      <td>b8c6</td>\n",
       "      <td>r n b r . . k .\\np p . . p p b p\\n. . . . . . ...</td>\n",
       "      <td>(Taking advantage of the pin.)</td>\n",
       "      <td>rnbr2k1/pp2ppbp/6p1/3P4/4P3/4BN2/P2K1PPP/1R3B1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>2263</td>\n",
       "      <td>29</td>\n",
       "      <td>White</td>\n",
       "      <td>d5d6</td>\n",
       "      <td>r . b r . . k .\\np p . . . p b p\\n. . n . p . ...</td>\n",
       "      <td>(The second step.)</td>\n",
       "      <td>r1br2k1/pp3pbp/2n1p1p1/2BP4/4P3/5N2/P2K1PPP/1R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>2263</td>\n",
       "      <td>30</td>\n",
       "      <td>Black</td>\n",
       "      <td>b7b6</td>\n",
       "      <td>r . b r . . k .\\np p . . . p b p\\n. . n P p . ...</td>\n",
       "      <td>(What about 15...e5? A good plan would be then...</td>\n",
       "      <td>r1br2k1/pp3pbp/2nPp1p1/2B5/4P3/5N2/P2K1PPP/1R3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Game Number  Move Number Player  Move  \\\n",
       "0     22         2263           23  White  e1d2   \n",
       "1     24         2263           25  White  d4d5   \n",
       "2     25         2263           26  Black  b8c6   \n",
       "3     28         2263           29  White  d5d6   \n",
       "4     29         2263           30  Black  b7b6   \n",
       "\n",
       "                                               Board  \\\n",
       "0  r n b . . r k .\\np p . . p p b p\\n. . . . . . ...   \n",
       "1  r n b r . . k .\\np p . . p p b p\\n. . . . . . ...   \n",
       "2  r n b r . . k .\\np p . . p p b p\\n. . . . . . ...   \n",
       "3  r . b r . . k .\\np p . . . p b p\\n. . n . p . ...   \n",
       "4  r . b r . . k .\\np p . . . p b p\\n. . n P p . ...   \n",
       "\n",
       "                                             Comment  \\\n",
       "0  (In this line White has to accept some risks. ...   \n",
       "1  (The first step of White's strategical plan: t...   \n",
       "2                     (Taking advantage of the pin.)   \n",
       "3                                 (The second step.)   \n",
       "4  (What about 15...e5? A good plan would be then...   \n",
       "\n",
       "                                                 FEN  \n",
       "0  rnb2rk1/pp2ppbp/6p1/8/3PP3/4BN2/P2q1PPP/1R2KB1...  \n",
       "1  rnbr2k1/pp2ppbp/6p1/8/3PP3/4BN2/P2K1PPP/1R3B1R...  \n",
       "2  rnbr2k1/pp2ppbp/6p1/3P4/4P3/4BN2/P2K1PPP/1R3B1...  \n",
       "3  r1br2k1/pp3pbp/2n1p1p1/2BP4/4P3/5N2/P2K1PPP/1R...  \n",
       "4  r1br2k1/pp3pbp/2nPp1p1/2B5/4P3/5N2/P2K1PPP/1R3...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def board_to_fen(board, player, move_number):\n",
    "    # Extract rows and reverse to match FEN order (from 8th to 1st rank)\n",
    "    rows = board.strip().split('\\n')\n",
    "\n",
    "    # Convert board rows to FEN format\n",
    "    fen_rows = []\n",
    "    for row in rows:\n",
    "        clean_row = re.sub(r\"\\s+\", \"\", row)  # Remove spaces\n",
    "        # Correctly replace consecutive dots with the count\n",
    "        fen_row = re.sub(r\"(\\.+) \", lambda m: str(len(m.group(1))), clean_row)  # Incorrectly placed space\n",
    "        fen_row = re.sub(r\"\\.+\", lambda m: str(len(m.group(0))), fen_row)  # Handle all dots\n",
    "        fen_rows.append(fen_row)\n",
    "\n",
    "    fen_pieces = \"/\".join(fen_rows)\n",
    "\n",
    "    # Active color ('w' for White, 'b' for Black)\n",
    "    active_color = 'w' if player == 'White' else 'b'\n",
    "\n",
    "    # Default values for other FEN components (assuming basic capabilities)\n",
    "    castling_availability = 'KQkq'  # Assuming all castling is still possible\n",
    "    en_passant_target = '-'  # No en passant target\n",
    "    halfmove_clock = '0'  # Reset on pawn moves or captures, not shown here\n",
    "    fullmove_number = str(move_number)\n",
    "\n",
    "    # Compile full FEN string\n",
    "    fen = f\"{fen_pieces} {active_color} {castling_availability} {en_passant_target} {halfmove_clock} {fullmove_number}\"\n",
    "    return fen\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df['FEN'] = df.apply(lambda row: board_to_fen(row['Board'], row['Player'], row['Move Number']), axis=1)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "gYeg4UWLAty4"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'Model': 'gpt2',\n",
    "    'NUMER_OF_DATA_DIRS': 12,\n",
    "    'batch_size': 2,\n",
    "    'lr':  3e-5,\n",
    "    'train_precentege': 0.9,\n",
    "    'epochs': 100,\n",
    "    'data_to_use': {'<fen>': True, '<moves>': True, '<last move description>': False,\n",
    "                    '<legal moves>': False, '<attacked by>': False, '<attacks>': False},\n",
    "    'max_length':300\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "359XaEmSeOLu"
   },
   "outputs": [],
   "source": [
    "# combined csv\n",
    "# def convert_data_to_text(row, max_length=config['max_length'], end_of_text_token=\"\"):\n",
    "#     # Assuming 'FEN', 'moves', and 'comment' are column names in the DataFrame\n",
    "#     FEN, moves, comment = row['FEN'], row['Move'], row['Comment']\n",
    "#     FEN, moves, comment = FEN[:max_length], moves[:max_length], comment[:max_length]\n",
    "#     token_to_data = {'<fen>': FEN, '<moves>': moves}\n",
    "#     text = \"\"\n",
    "#     for token, data in token_to_data.items():\n",
    "#         text += f\"{token} {data} \"\n",
    "#     text += f\"<comment> {comment} {end_of_text_token}\"\n",
    "#     return text\n",
    "\n",
    "def convert_data_to_text(row, max_length=config['max_length'], end_of_text_token=\"\"):\n",
    "    # Assuming 'FEN', 'moves', and 'comment' are column names in the DataFrame\n",
    "    FEN, moves, comment = row['FEN'], row['Move'], row['Comment']\n",
    "    FEN, moves, comment = FEN[:max_length], moves[:max_length], comment[:max_length]\n",
    "\n",
    "    # Split moves into a list\n",
    "    move_list = moves.split(',')\n",
    "\n",
    "    # Truncate move history to last 5 moves (or less for initial moves)\n",
    "    history_moves = move_list[-min(len(move_list), 5):]  # Get last 5 or less moves\n",
    "    history_moves_str = \", \".join(history_moves)  # Join moves with comma separator\n",
    "\n",
    "    token_to_data = {'<fen>': FEN, '<moves>': history_moves_str}\n",
    "    text = \"\"\n",
    "    for token, data in token_to_data.items():\n",
    "        text += f\"{token} {data} \"\n",
    "    text += f\"<comment> {comment} {end_of_text_token}\"\n",
    "    return text\n",
    "\n",
    "\n",
    "# def convert_data_to_text_2(data_object, max_length=768, end_of_text_token=\"<|endoftext|>\"):\n",
    "#     (FEN, moves, last_move_desc, legal_moves, attackers_list, attacks_list, comment) = data_object\n",
    "#     (FEN, moves, last_move_desc, legal_moves,\n",
    "#      attackers_list, attacks_list, comment) = (FEN[:max_length], moves[:max_length], last_move_desc[:max_length],\n",
    "#                                                legal_moves[:max_length], attackers_list[:max_length],\n",
    "#                                                attacks_list[:max_length], comment[:max_length])\n",
    "#     moves = moves.split(',')[-1].strip() if ',' in moves else moves\n",
    "\n",
    "#     token_to_data = {'<fen>': FEN, '<moves>': moves, '<last move description>': last_move_desc,\n",
    "#                      '<legal moves>': legal_moves, '<attacked by>': attackers_list, '<attacks>': attacks_list}\n",
    "#     text = \"\"\n",
    "#     for token in token_to_data.keys():\n",
    "#         if config['data_to_use'][token]:\n",
    "#             text += f\"{token} {token_to_data[token]} \"\n",
    "#     text += f\"<comment> {comment} {end_of_text_token}\"  # comment always included at the end + end token\n",
    "\n",
    "#     return text\n",
    "\n",
    "def convert_data_to_text_2(data_object, max_length=768, end_of_text_token=\"<|endoftext|>\"):\n",
    "    (FEN, moves, last_move_desc, legal_moves, attackers_list, attacks_list, comment) = data_object\n",
    "    (FEN, moves, last_move_desc, legal_moves,\n",
    "     attackers_list, attacks_list, comment) = (FEN[:max_length], moves[:max_length], last_move_desc[:max_length],\n",
    "                                               legal_moves[:max_length], attackers_list[:max_length],\n",
    "                                               attacks_list[:max_length], comment[:max_length])\n",
    "    moves = moves.split(',')[-1].strip() if ',' in moves else moves\n",
    "\n",
    "    # Split moves into a list\n",
    "    move_list = moves.split(',')\n",
    "\n",
    "    # Truncate move history to last 5 moves (or less for initial moves)\n",
    "    history_moves = move_list[-min(len(move_list), 5):]  # Get last 5 or less moves\n",
    "    history_moves_str = \", \".join(history_moves)  # Join moves with comma separator\n",
    "\n",
    "    token_to_data = {'<fen>': FEN, '<moves>': history_moves_str, '<last move description>': last_move_desc,\n",
    "                     '<legal moves>': legal_moves, '<attacked by>': attackers_list, '<attacks>': attacks_list}\n",
    "    text = \"\"\n",
    "    for token in token_to_data.keys():\n",
    "        if config['data_to_use'][token]:\n",
    "            text += f\"{token} {token_to_data[token]} \"\n",
    "    text += f\"<comment> {comment} {end_of_text_token}\"  # comment always included at the end + end token\n",
    "\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "n00akTndAG-O"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "dataset_tokens = list(config['data_to_use'].keys()) + ['<comment>']\n",
    "\n",
    "\n",
    "class ProcessDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=config['max_length']):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.comment_encoding = tokenizer.get_added_vocab()['<comment>']\n",
    "        self.proccessed_data = []\n",
    "        self.attn_masks = []\n",
    "        self.labels = []\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            text = convert_data_to_text(row, max_length)\n",
    "            enc_text = tokenizer(text, truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "            inputs = enc_text['input_ids']\n",
    "            label_idx = inputs.index(self.comment_encoding) + 1\n",
    "            labels = [-100] * label_idx + inputs[label_idx:]\n",
    "\n",
    "            self.proccessed_data.append(torch.tensor(inputs))\n",
    "            self.attn_masks.append(torch.tensor(enc_text['attention_mask']))\n",
    "            self.labels.append(torch.tensor(labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.proccessed_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.proccessed_data[index], self.attn_masks[index], self.labels[index]\n",
    "\n",
    "class MovesDataset(Dataset):\n",
    "    def __init__(self, paths, tokenizer, max_length=768):\n",
    "\n",
    "        self.comment_encoding = tokenizer.get_added_vocab()['<comment>']\n",
    "\n",
    "        self.proccessed_data = []\n",
    "        self.attn_masks = []\n",
    "        self.labels = []\n",
    "\n",
    "\n",
    "        for path in paths:\n",
    "            with open(path, 'rb') as file:\n",
    "                raw_data = pickle.load(file)\n",
    "            for data_object in raw_data:\n",
    "                text = convert_data_to_text_2(data_object)\n",
    "\n",
    "                enc_text = tokenizer(text, truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "\n",
    "                inputs = enc_text['input_ids']\n",
    "                label_idx = inputs.index(self.comment_encoding) + 1\n",
    "                labels = [-100] * label_idx + inputs[label_idx:]\n",
    "\n",
    "                self.proccessed_data.append(torch.tensor(inputs))\n",
    "                self.attn_masks.append(torch.tensor(enc_text['attention_mask']))\n",
    "                self.labels.append(torch.tensor(labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.proccessed_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.proccessed_data[index], self.attn_masks[index], self.labels[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQCG8sq0HnTi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "CuYMVboH8J5O"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Config, GPT2Tokenizer\n",
    "\n",
    "GPT2_TYPE = \"gpt2\"\n",
    "\n",
    "class GPT2:\n",
    "    def __init__(self):\n",
    "        print(\"Initialization\\n\")\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(GPT2_TYPE)\n",
    "        special_tokens_dict = {\n",
    "            'pad_token': '[PAD]',\n",
    "            'additional_special_tokens': dataset_tokens\n",
    "        }\n",
    "        self.tokenizer.add_special_tokens(special_tokens_dict)\n",
    "        # self.tokenizer.add_tokens(get_chess_tokens())\n",
    "\n",
    "        self.configuration = GPT2Config.from_pretrained(GPT2_TYPE)\n",
    "        self.model = GPT2LMHeadModel(self.configuration)\n",
    "\n",
    "        # Resize token embeddings to accommodate new tokens\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        print(\"Loading model\\n\")\n",
    "        state_dict = torch.load(model_path, map_location=self.device)\n",
    "        self.model.load_state_dict(state_dict)\n",
    "        self.model.to(self.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qHLCuoYOAJRi",
    "outputId": "86694063-7220-47a7-8847-392300dd42b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt2 = GPT2()\n",
    "model = gpt2.model\n",
    "model.load_state_dict(torch.load(\"/workspace/models/final.bin\"))\n",
    "model.train()\n",
    "tokenizer = gpt2.tokenizer\n",
    "max_length = config['max_length']\n",
    "eof = '<|endoftext|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50265, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "EF7-BmPuKjCl"
   },
   "outputs": [],
   "source": [
    "games_data_path = '/workspace'\n",
    "saved_models_path = '/workspace/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "uzwm1mrqER_f"
   },
   "outputs": [],
   "source": [
    "df[\"Comment\"] = df[\"Comment\"].fillna('No Comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "zMIqNFmKIEjd"
   },
   "outputs": [],
   "source": [
    "dataset = ProcessDataset(df,tokenizer)\n",
    "# dataset = MovesDataset([f'{games_data_path}1.p'], tokenizer, max_length=max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "YP115lPlIRVF"
   },
   "outputs": [],
   "source": [
    "train_size = int(config['train_precentege'] * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349,
     "referenced_widgets": [
      "11d81c1544db432699e563160a7d0e21",
      "b584d317803948838f214fb633fa1f94",
      "ee9243cf51244b2f809dfe3f6c8f1805",
      "b25b47cbb6644852990f6be859742d1a",
      "a252e6ad908442b798af0d47664ee75d",
      "ca158732ae63434183a638107ff399db",
      "89643eead3b5432bb37588ab67ecb59c",
      "bb5bae5f7345440ca7adbee5a37292b1"
     ]
    },
    "id": "0sq9R4yjIl63",
    "outputId": "a7250c17-55d3-41e8-8910-2236dc47cab3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:uwnql7sw) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='3.340 MB of 3.340 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>loss</td><td>▂▄▄▁▂▄▅▇▃▂▂▁▂▄▇▃▄█▄▂▅▆▇▂▂▃▄▄▇▂▃▁▃▂▇▁▅▂▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>0.61295</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">daily-durian-13</strong> at: <a href='https://wandb.ai/mzkingsl6/LmChess/runs/uwnql7sw' target=\"_blank\">https://wandb.ai/mzkingsl6/LmChess/runs/uwnql7sw</a><br/> View project at: <a href='https://wandb.ai/mzkingsl6/LmChess' target=\"_blank\">https://wandb.ai/mzkingsl6/LmChess</a><br/>Synced 6 W&B file(s), 3604 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240425_032626-uwnql7sw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:uwnql7sw). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97609244dfc4f1db40469b5b265f734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112711588955588, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240425_175441-apq19z0z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mzkingsl6/LmChess/runs/apq19z0z' target=\"_blank\">smart-serenity-14</a></strong> to <a href='https://wandb.ai/mzkingsl6/LmChess' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mzkingsl6/LmChess' target=\"_blank\">https://wandb.ai/mzkingsl6/LmChess</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mzkingsl6/LmChess/runs/apq19z0z' target=\"_blank\">https://wandb.ai/mzkingsl6/LmChess/runs/apq19z0z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"LmChess\", config={'batch size': config['batch_size'], 'lr': config['lr'], 'epochs': config['epochs']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "aT9YoKwHJL5A"
   },
   "outputs": [],
   "source": [
    "#validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "GHS4fgRRJbwS"
   },
   "outputs": [],
   "source": [
    "validation_proccessed_data, validation_attn_masks, validation_labels = next(iter(test_dataloader))\n",
    "\n",
    "validation_input_encodings = []\n",
    "for i in range(config['batch_size']):\n",
    "  textual_validation_data = tokenizer.decode(token_ids = validation_proccessed_data[i], skip_special_tokens=False).split('<comment>')\n",
    "\n",
    "  validation_target_text = textual_validation_data[1].split(eof)[0]\n",
    "  validation_input_text = textual_validation_data[0]\n",
    "\n",
    "  wandb.log({f\"validation_target_text {i}\": wandb.Html(f'<p>{validation_target_text}</p>')})\n",
    "  wandb.log({f\"validation_input_text {i}\": wandb.Html(f'<p>{validation_input_text}</p>')})\n",
    "\n",
    "  comment_idx = list(validation_proccessed_data[i]).index(dataset.comment_encoding) + 1\n",
    "  validation_input_encoding = validation_proccessed_data[i][:comment_idx].unsqueeze(0).cuda()\n",
    "  #validation_input_encoding  = tokenizer.encode(validation_input_text, return_tensors=\"pt\").cuda()\n",
    "\n",
    "  validation_input_encodings.append(validation_input_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9348"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IpZ6G7E7Jc5v",
    "outputId": "10931e86-0596-403b-d790-a85256d846d2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "18696it [08:14, 37.82it/s]                             \n",
      "18696it [08:19, 37.44it/s]                             \n",
      "18696it [08:13, 37.85it/s]                             \n",
      "18696it [08:13, 37.89it/s]                             \n",
      "18696it [08:07, 38.32it/s]                             \n",
      "18696it [08:18, 37.51it/s]                             \n",
      "18696it [08:07, 38.32it/s]                             \n",
      "18696it [08:13, 37.89it/s]                             \n",
      "18696it [08:00, 38.90it/s]                             \n",
      "18696it [07:50, 39.73it/s]                             \n",
      "18696it [08:16, 37.68it/s]                             \n",
      "18696it [08:12, 37.93it/s]                             \n",
      "18696it [08:22, 37.20it/s]                             \n",
      "18696it [08:24, 37.08it/s]                             \n",
      "18696it [08:21, 37.31it/s]                             \n",
      "18696it [08:14, 37.78it/s]                             \n",
      "18696it [08:03, 38.67it/s]                             \n",
      "18696it [08:14, 37.84it/s]                             \n",
      "18696it [08:13, 37.88it/s]                             \n",
      "18696it [08:15, 37.77it/s]                             \n",
      "18696it [08:16, 37.63it/s]                             \n",
      "12794it [05:28, 49.88it/s]                             "
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = AdamW(model.parameters(), lr= config['lr'])\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=5000, num_training_steps=-1\n",
    ")\n",
    "\n",
    "loss = 0\n",
    "pad_token_id = tokenizer('[PAD]')['input_ids'][0]\n",
    "\n",
    "epochs = config['epochs']\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    with tqdm(total=len(dataset) / 2) as pbar:\n",
    "        for idx,entry in enumerate(train_dataloader):\n",
    "\n",
    "            if idx % 500 == 0 and idx != 0:\n",
    "              for i in range(config['batch_size']):\n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(validation_input_encodings[i], num_beams=2, no_repeat_ngram_size=2, max_length=max_length+1, pad_token_id=pad_token_id)\n",
    "                    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                wandb.log({f\"output_text {i}\": wandb.Html(f'<p>{output_text}</p>')})\n",
    "\n",
    "            # if idx % 9000 == 0:\n",
    "            #   torch.save(model.state_dict(), f'{saved_models_path}{idx}_{time.time()}_{int(loss)}.bin')\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            inputs = entry[0].cuda()\n",
    "            attn_masks = entry[1].cuda()\n",
    "            labels = entry[2].cuda()\n",
    "            outputs = model(inputs, labels=labels, attention_mask = attn_masks)\n",
    "\n",
    "            loss = outputs['loss']\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            wandb.log({\"epoch\": epoch, \"loss\": loss})\n",
    "            pbar.update(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F7FjFqk7K34f"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'{saved_models_path}final100.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3UCf84ZbMpmo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BjF6ilFMrTz"
   },
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "NE22F4CxN3CT"
   },
   "outputs": [],
   "source": [
    "model_path = '/workspace/models/final100.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MRQnd-RBNoEV",
    "outputId": "981ee369-e94b-473a-b173-d5979646ac3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization\n",
      "\n",
      "Loading model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt2_test = GPT2()\n",
    "\n",
    "gpt2_test.load_model(model_path)\n",
    "\n",
    "gpt2_test.model = gpt2_test.model.eval().cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ROrkDkWrOOdW"
   },
   "outputs": [],
   "source": [
    "tested_model = gpt2_test\n",
    "max = config['max_length']\n",
    "eof = '<|endoftext|>'\n",
    "pad_token_id = tested_model.tokenizer('[PAD]')['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "uOiwojk1MsfC"
   },
   "outputs": [],
   "source": [
    "dataloader = test_dataloader\n",
    "def get_results():\n",
    "    # Set device based on availability of CUDA\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Move the model to the appropriate device\n",
    "    tested_model.model.to(device)\n",
    "\n",
    "    # Retrieve batch data\n",
    "    processed_data, attn_masks, labels = next(iter(dataloader))\n",
    "\n",
    "    # Initialize input encodings container\n",
    "    input_encodings = []\n",
    "\n",
    "    for i in range(config['batch_size']):\n",
    "        # Decode the token_ids to text and split at <comment>\n",
    "        textual_data = tested_model.tokenizer.decode(token_ids=processed_data[i], skip_special_tokens=False).split('<comment>')\n",
    "\n",
    "        # Target text ends at EOF (assuming EOF is defined elsewhere in your code)\n",
    "        target_text = textual_data[1].split(eof)[0]\n",
    "        input_text = textual_data[0]\n",
    "\n",
    "        # Find the index of <comment> and prepare input encoding\n",
    "        comment_idx = list(processed_data[i]).index(tested_model.tokenizer.get_added_vocab()['<comment>']) + 1\n",
    "        input_encoding = processed_data[i][:comment_idx].unsqueeze(0).to(device)\n",
    "\n",
    "        input_encodings.append(input_encoding)\n",
    "\n",
    "    # Initialize results container\n",
    "    results = []\n",
    "\n",
    "    for input_encoding in input_encodings:\n",
    "        with torch.no_grad():\n",
    "            # Generate output using the model's generate function\n",
    "            outputs = tested_model.model.generate(\n",
    "                input_encoding,\n",
    "                num_beams=2,\n",
    "                no_repeat_ngram_size=2,\n",
    "                max_length=max + 1,  # assuming 'max' is defined as the maximum length\n",
    "                pad_token_id=pad_token_id  # assuming 'pad_token_id' is defined elsewhere\n",
    "            )\n",
    "            output_text = tested_model.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            results.append(output_text)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oSnNK-A1NT4v",
    "outputId": "8e3c1166-f045-4326-ae2a-675936da5b73"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" rnbqkb1r/pppppp1p/5np1/8/3P4/5N2/PPP1PPPP/RNBQKB1R w KQkq - 0 5  c2c4  This is a pawns.  White's pieces. The Black's king, but the pawn. He's pawn at d4, and the f-pawn. Now the d3-file, he's without the Black has to play the e4-f4 and Black to the Bishop. White to be to Black Queen, the Queen. But the White without pawn, it's Bishop, so the Knight, Black without without Black, for the center, as the without e5-Knight,, which without to e3, without f4. Black pawn is to win, to f7-c7,\\n awn is not be a-h4 is the c4 (   Black King, with his pieces on the King to his Queen's Queen and without d5, White pawn to attack. This's d-d4 to a without his pawn on Black Knight's position, on d7. I without White, in the a7 and f5. It's King's attack the\\nthe pawn and e7 to without it without\\ns, at f1-e5 and d2-square, a Knight. In the bishop.Nf5  without\",\n",
       " \" r2qr1k1/1ppb1pp1/p1np1bnp/4p3/1P1PP3/PNP1BN2/2B2PPP/R2Q1RK1 b KQkq - 0 26  f6g5  Black's pieces on the pawns, but the f-pawn. awn is a pawn, and the d4. The Bishop. He's king. Now the Black has to the White's pawn at d-file, he can be to play the e4, so Black to be a-Knight, as the Knight, the Queen, it's position, for the center. But Black pawn. Black without the Bishop, which is the bishop. White has a Knight. This is not have to win,, Black Queen. It's without without to Black can have a4-f5, to attack on e5.Nf4 and without pawn on Black King, White without Black is now the attack. In the King's d5-c4 is without f7-e5 and Black Knight and White to without his Queen and f3-d5 is to f4 to a without a7, without White. I without d3, on his pieces to e7 and he's to c3. without e3 and his pawn to his King. the c4).  White Queen's. However,.\"]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = get_results()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4tJ9GQAGNVXc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDDzfyjmOo2Z"
   },
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.3)\n",
      "Collecting pyarrow>=12.0.0 (from datasets)\n",
      "  Downloading pyarrow-16.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.65.0)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.2.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.22.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow)\n",
      "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.9.0)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow)\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.0.0 (from tensorflow)\n",
      "  Downloading keras-3.3.2-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.36.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2024.4.16)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting rich (from keras>=3.0.0->tensorflow)\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.0.0->tensorflow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.17,>=2.16->tensorflow)\n",
      "  Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.17,>=2.16->tensorflow)\n",
      "  Downloading werkzeug-3.0.2-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.3)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.0.0->tensorflow)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (2.15.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m134.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.3.2-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-16.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m107.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.36.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.0-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.2/301.2 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.0.2-py3-none-any.whl (226 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.8/226.8 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, xxhash, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, pyarrow-hotfix, pyarrow, opt-einsum, multidict, ml-dtypes, mdurl, markdown, joblib, h5py, grpcio, google-pasta, gast, frozenlist, dill, async-timeout, absl-py, yarl, tensorboard, nltk, multiprocess, markdown-it-py, aiosignal, rich, aiohttp, keras, tensorflow, datasets\n",
      "Successfully installed absl-py-2.1.0 aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.19.0 dill-0.3.8 flatbuffers-24.3.25 frozenlist-1.4.1 gast-0.5.4 google-pasta-0.2.0 grpcio-1.62.2 h5py-3.11.0 joblib-1.4.0 keras-3.3.2 libclang-18.1.1 markdown-3.6 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.3.2 multidict-6.0.5 multiprocess-0.70.16 namex-0.0.8 nltk-3.8.1 opt-einsum-3.3.0 pyarrow-16.0.0 pyarrow-hotfix-0.6 rich-13.7.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-io-gcs-filesystem-0.36.0 termcolor-2.4.0 werkzeug-3.0.2 wrapt-1.16.0 xxhash-3.4.1 yarl-1.9.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets tensorflow nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "8ZJ0IFmuOqo0",
    "outputId": "5af4736b-1968-4c7a-870f-80ab76b7c6ce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 02:33:49.383598: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-24 02:33:50.479411: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from datasets import load_metric\n",
    "import tensorflow as tf\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "\n",
    "def perplexity(model, dataloader):\n",
    "    eval_loss = 0\n",
    "    with tqdm(total=len(dataloader)) as pbar:\n",
    "        for idx, entry in enumerate(dataloader):\n",
    "            with torch.no_grad():\n",
    "                inputs = entry[0].cuda()\n",
    "                attn_masks = entry[1].cuda()\n",
    "                labels = entry[2].cuda()\n",
    "                outputs = model(inputs, labels=labels, attention_mask=attn_masks)\n",
    "            loss = outputs[0]\n",
    "            eval_loss += loss.mean().item()\n",
    "            pbar.update(2)\n",
    "    final_eval_loss = eval_loss / len(dataloader)\n",
    "    perplexity = torch.exp(torch.tensor(final_eval_loss))\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def bleurt(target_texts, output_texts):\n",
    "    metric = load_metric(\"bleurt\")\n",
    "    tf.compat.v1.flags.DEFINE_string('f', '', '')\n",
    "\n",
    "    scores = metric.compute(predictions=output_texts, references=target_texts)['scores']\n",
    "    return scores\n",
    "\n",
    "\n",
    "def bleu(target_texts, output_texts):\n",
    "    scores = []\n",
    "    for idx in range(len(output_texts)):\n",
    "        reference = [target_texts[idx].split()]\n",
    "        candidate = output_texts[idx].split()\n",
    "        scores.append(sentence_bleu(reference, candidate))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iGKGbdSiXWwu",
    "outputId": "4f4c5284-9277-4700-eb94-843edaca0f99"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "914it [00:04, 185.91it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8497)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_perplexity = perplexity(tested_model.model, dataloader)\n",
    "print(test_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "TdovyEjmYCSV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_targets_and_outputs(model, dataset, comment_encoding, pad_token_id, max_length=768, eof='<|endoftext|>'):\n",
    "    target_texts = []\n",
    "    output_texts = []\n",
    "    with tqdm(total=len(dataset)) as pbar:\n",
    "        for idx, entry in enumerate(dataset):\n",
    "\n",
    "          textual_data = model.tokenizer.decode(token_ids=entry[0], skip_special_tokens=False)\n",
    "          textual_data = textual_data.split('<comment>')[1].split(eof)[0]\n",
    "          target_texts.append(textual_data)\n",
    "\n",
    "          comment_idx = list(entry[0]).index(comment_encoding) + 1\n",
    "          input_encoding = entry[0][:comment_idx].unsqueeze(0).cuda()\n",
    "          with torch.no_grad():\n",
    "              outputs = model.model.generate(input_encoding, num_beams=2, no_repeat_ngram_size=2, max_length=max_length+1, pad_token_id=pad_token_id)\n",
    "              output_text = model.tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "              output_text = output_text.split('<comment>')[1].split(eof)[0]\n",
    "          output_texts.append(output_text)\n",
    "\n",
    "          pbar.update(1)\n",
    "    return target_texts, output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "xJ__FqFtXsvv",
    "outputId": "ddd30cf6-7558-414e-fd09-12c372aa8802"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9129/9129 [5:57:02<00:00,  2.35s/it]  \n"
     ]
    }
   ],
   "source": [
    "target_texts, output_texts = get_targets_and_outputs(tested_model, dataset, dataset.comment_encoding, pad_token_id, max_length=max, eof=eof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/google-research/bleurt.git\n",
      "  Cloning https://github.com/google-research/bleurt.git to /tmp/pip-req-build-029f8586\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/google-research/bleurt.git /tmp/pip-req-build-029f8586\n",
      "  Resolved https://github.com/google-research/bleurt.git to commit cebe7e6f996b40910cfaa520a63db47807e3bf5c\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pandas (from BLEURT==0.0.2)\n",
      "  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from BLEURT==0.0.2) (1.26.3)\n",
      "Collecting scipy (from BLEURT==0.0.2)\n",
      "  Downloading scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow (from BLEURT==0.0.2)\n",
      "  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Collecting tf-slim>=1.1 (from BLEURT==0.0.2)\n",
      "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting sentencepiece (from BLEURT==0.0.2)\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting absl-py>=0.2.2 (from tf-slim>=1.1->BLEURT==0.0.2)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->BLEURT==0.0.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->BLEURT==0.0.2) (2023.3.post1)\n",
      "Collecting tzdata>=2022.7 (from pandas->BLEURT==0.0.2)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->BLEURT==0.0.2) (1.6.3)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow->BLEURT==0.0.2)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow->BLEURT==0.0.2)\n",
      "  Downloading gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow->BLEURT==0.0.2)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow->BLEURT==0.0.2)\n",
      "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow->BLEURT==0.0.2)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow->BLEURT==0.0.2)\n",
      "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow->BLEURT==0.0.2)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow->BLEURT==0.0.2) (23.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow->BLEURT==0.0.2)\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->BLEURT==0.0.2) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow->BLEURT==0.0.2) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->BLEURT==0.0.2) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow->BLEURT==0.0.2)\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow->BLEURT==0.0.2) (4.9.0)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow->BLEURT==0.0.2)\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow->BLEURT==0.0.2)\n",
      "  Downloading grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow->BLEURT==0.0.2)\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.0.0 (from tensorflow->BLEURT==0.0.2)\n",
      "  Downloading keras-3.3.2-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow->BLEURT==0.0.2)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.36.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow->BLEURT==0.0.2) (0.41.2)\n",
      "Collecting rich (from keras>=3.0.0->tensorflow->BLEURT==0.0.2)\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.0.0->tensorflow->BLEURT==0.0.2)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow->BLEURT==0.0.2) (0.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (2024.2.2)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.17,>=2.16->tensorflow->BLEURT==0.0.2)\n",
      "  Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow->BLEURT==0.0.2)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.17,>=2.16->tensorflow->BLEURT==0.0.2)\n",
      "  Downloading werkzeug-3.0.2-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow->BLEURT==0.0.2) (2.1.3)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.0.0->tensorflow->BLEURT==0.0.2)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow->BLEURT==0.0.2) (2.15.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow->BLEURT==0.0.2)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.1/352.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m110.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.3.2-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.36.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.0.2-py3-none-any.whl (226 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.8/226.8 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: BLEURT\n",
      "  Building wheel for BLEURT (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for BLEURT: filename=BLEURT-0.0.2-py3-none-any.whl size=16456764 sha256=44f2816b43b59178e151318e88eb1a6f59fd69f6536e04bf4797e4e1ebfddd90\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-g96h52jj/wheels/64/f4/2c/509a6c31b8ebde891a81029fd94f199b1b92f0e7cfc20d417a\n",
      "Successfully built BLEURT\n",
      "Installing collected packages: sentencepiece, namex, libclang, flatbuffers, wrapt, werkzeug, tzdata, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, scipy, protobuf, opt-einsum, ml-dtypes, mdurl, markdown, h5py, grpcio, google-pasta, gast, absl-py, tf-slim, tensorboard, pandas, markdown-it-py, rich, keras, tensorflow, BLEURT\n",
      "Successfully installed BLEURT-0.0.2 absl-py-2.1.0 flatbuffers-24.3.25 gast-0.5.4 google-pasta-0.2.0 grpcio-1.62.2 h5py-3.11.0 keras-3.3.2 libclang-18.1.1 markdown-3.6 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.3.2 namex-0.0.8 opt-einsum-3.3.0 pandas-2.2.2 protobuf-4.25.3 rich-13.7.1 scipy-1.13.0 sentencepiece-0.2.0 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-io-gcs-filesystem-0.36.0 termcolor-2.4.0 tf-slim-1.1.0 tzdata-2024.1 werkzeug-3.0.2 wrapt-1.16.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/google-research/bleurt.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "background_save": true,
     "referenced_widgets": [
      "cec5a51bfa4e4097b2a022dfd0ee53d7"
     ]
    },
    "id": "7msGQy-_Xbxr",
    "outputId": "ad8dfe7b-b18d-4647-ac74-483c57203871"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bleurt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_bleurt \u001b[38;5;241m=\u001b[39m \u001b[43mbleurt\u001b[49m(target_texts, output_texts)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bleurt' is not defined"
     ]
    }
   ],
   "source": [
    "test_bleurt = bleurt(target_texts, output_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Eyd6VQXyXol8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_bleurt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28msum\u001b[39m(\u001b[43mtest_bleurt\u001b[49m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_bleurt))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_bleurt' is not defined"
     ]
    }
   ],
   "source": [
    "print(sum(test_bleurt)/len(test_bleurt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "yRQ2qRM-YEt2"
   },
   "outputs": [],
   "source": [
    "test_bleu = bleu(target_texts, output_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "_Qtb0fILYKzy"
   },
   "outputs": [],
   "source": [
    "print(sum(test_bleu)/len(test_bleu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zaOsUzSbspCe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "11d81c1544db432699e563160a7d0e21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b584d317803948838f214fb633fa1f94",
       "IPY_MODEL_ee9243cf51244b2f809dfe3f6c8f1805"
      ],
      "layout": "IPY_MODEL_b25b47cbb6644852990f6be859742d1a"
     }
    },
    "89643eead3b5432bb37588ab67ecb59c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a252e6ad908442b798af0d47664ee75d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b25b47cbb6644852990f6be859742d1a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b584d317803948838f214fb633fa1f94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a252e6ad908442b798af0d47664ee75d",
      "placeholder": "​",
      "style": "IPY_MODEL_ca158732ae63434183a638107ff399db",
      "value": "0.018 MB of 0.018 MB uploaded\r"
     }
    },
    "bb5bae5f7345440ca7adbee5a37292b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ca158732ae63434183a638107ff399db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ee9243cf51244b2f809dfe3f6c8f1805": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89643eead3b5432bb37588ab67ecb59c",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bb5bae5f7345440ca7adbee5a37292b1",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
