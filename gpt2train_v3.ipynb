{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8dE3wqIqIxxv",
    "outputId": "250c4fab-de2b-4891-b0bb-b9df34325cd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.4.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m150.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m159.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.4.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m119.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m134.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m163.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tzdata, safetensors, regex, pandas, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.22.2 pandas-2.2.2 regex-2024.4.16 safetensors-0.4.3 tokenizers-0.19.1 transformers-4.40.0 tzdata-2024.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install wandb pandas transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eywDwjFDLCnk",
    "outputId": "7a3b4ca3-8d8a-4bcc-dc13-fe3d732d54d9"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wyytxRKF8lQ3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import wandb\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "Q18RO4uBI901",
    "outputId": "202eab28-7125-4299-a134-5d70076c85cc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "-J0Qxrjf8WbF",
    "outputId": "186b05b3-14ff-4a59-e79b-ba6383a91672"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HkwUL1OF8jTX"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "7LAZ4G8D8rof",
    "outputId": "c553ced5-a4b7-4706-f9fc-494b43dc185e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Game Number</th>\n",
       "      <th>Move Number</th>\n",
       "      <th>Player</th>\n",
       "      <th>Move</th>\n",
       "      <th>Board</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>576</td>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "      <td>b2b4</td>\n",
       "      <td>r n b q k b n r\\np p p p p p p p\\n. . . . . . ...</td>\n",
       "      <td>WHAT?!?! In the true hypermodern style, Tartak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>576</td>\n",
       "      <td>2</td>\n",
       "      <td>Black</td>\n",
       "      <td>e7e6</td>\n",
       "      <td>r n b q k b n r\\np p p p p p p p\\n. . . . . . ...</td>\n",
       "      <td>Maroczy (Black) makes an illogical move of his...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>576</td>\n",
       "      <td>3</td>\n",
       "      <td>White</td>\n",
       "      <td>c1b2</td>\n",
       "      <td>r n b q k b n r\\np p p p . p p p\\n. . . . p . ...</td>\n",
       "      <td>Tartakower doesn't need to defend the pawn yet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>576</td>\n",
       "      <td>4</td>\n",
       "      <td>Black</td>\n",
       "      <td>g8f6</td>\n",
       "      <td>r n b q k b n r\\np p p p . p p p\\n. . . . p . ...</td>\n",
       "      <td>Maroczy follows more conventional lines, devel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>576</td>\n",
       "      <td>5</td>\n",
       "      <td>White</td>\n",
       "      <td>b4b5</td>\n",
       "      <td>r n b q k b . r\\np p p p . p p p\\n. . . . p n ...</td>\n",
       "      <td>Tartakower inhibits the development of Black's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49495</th>\n",
       "      <td>3</td>\n",
       "      <td>94</td>\n",
       "      <td>Black</td>\n",
       "      <td>h5h3</td>\n",
       "      <td>. . . . . . . .\\n. . . . k . p .\\n. p n . . p ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49496</th>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "      <td>White</td>\n",
       "      <td>b1b6</td>\n",
       "      <td>. . . . . . . .\\n. . . . k . p .\\n. p n . . p ...</td>\n",
       "      <td>There goes the extra pawn.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49497</th>\n",
       "      <td>3</td>\n",
       "      <td>96</td>\n",
       "      <td>Black</td>\n",
       "      <td>e7d6</td>\n",
       "      <td>. . . . . . . .\\n. . . . k . p .\\n. R n . . p ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49498</th>\n",
       "      <td>3</td>\n",
       "      <td>97</td>\n",
       "      <td>White</td>\n",
       "      <td>g1g2</td>\n",
       "      <td>. . . . . . . .\\n. . . . . . p .\\n. R n k . p ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49499</th>\n",
       "      <td>3</td>\n",
       "      <td>98</td>\n",
       "      <td>Black</td>\n",
       "      <td>h3c3</td>\n",
       "      <td>. . . . . . . .\\n. . . . . . p .\\n. R n k . p ...</td>\n",
       "      <td>After 50. Bg3+ Black's Pg7 might well fall, so...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49500 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Game Number  Move Number Player  Move  \\\n",
       "0              576            1  White  b2b4   \n",
       "1              576            2  Black  e7e6   \n",
       "2              576            3  White  c1b2   \n",
       "3              576            4  Black  g8f6   \n",
       "4              576            5  White  b4b5   \n",
       "...            ...          ...    ...   ...   \n",
       "49495            3           94  Black  h5h3   \n",
       "49496            3           95  White  b1b6   \n",
       "49497            3           96  Black  e7d6   \n",
       "49498            3           97  White  g1g2   \n",
       "49499            3           98  Black  h3c3   \n",
       "\n",
       "                                                   Board  \\\n",
       "0      r n b q k b n r\\np p p p p p p p\\n. . . . . . ...   \n",
       "1      r n b q k b n r\\np p p p p p p p\\n. . . . . . ...   \n",
       "2      r n b q k b n r\\np p p p . p p p\\n. . . . p . ...   \n",
       "3      r n b q k b n r\\np p p p . p p p\\n. . . . p . ...   \n",
       "4      r n b q k b . r\\np p p p . p p p\\n. . . . p n ...   \n",
       "...                                                  ...   \n",
       "49495  . . . . . . . .\\n. . . . k . p .\\n. p n . . p ...   \n",
       "49496  . . . . . . . .\\n. . . . k . p .\\n. p n . . p ...   \n",
       "49497  . . . . . . . .\\n. . . . k . p .\\n. R n . . p ...   \n",
       "49498  . . . . . . . .\\n. . . . . . p .\\n. R n k . p ...   \n",
       "49499  . . . . . . . .\\n. . . . . . p .\\n. R n k . p ...   \n",
       "\n",
       "                                                 Comment  \n",
       "0      WHAT?!?! In the true hypermodern style, Tartak...  \n",
       "1      Maroczy (Black) makes an illogical move of his...  \n",
       "2      Tartakower doesn't need to defend the pawn yet...  \n",
       "3      Maroczy follows more conventional lines, devel...  \n",
       "4      Tartakower inhibits the development of Black's...  \n",
       "...                                                  ...  \n",
       "49495                                                NaN  \n",
       "49496                         There goes the extra pawn.  \n",
       "49497                                                NaN  \n",
       "49498                                                NaN  \n",
       "49499  After 50. Bg3+ Black's Pg7 might well fall, so...  \n",
       "\n",
       "[49500 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4GDujLeb8uvV"
   },
   "outputs": [],
   "source": [
    "df = df.dropna().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "gbPbFS5-9yoK",
    "outputId": "e190f317-cbe0-496d-f9b1-6859e3f1276a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Game Number</th>\n",
       "      <th>Move Number</th>\n",
       "      <th>Player</th>\n",
       "      <th>Move</th>\n",
       "      <th>Board</th>\n",
       "      <th>Comment</th>\n",
       "      <th>FEN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>576</td>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "      <td>b2b4</td>\n",
       "      <td>r n b q k b n r\\np p p p p p p p\\n. . . . . . ...</td>\n",
       "      <td>WHAT?!?! In the true hypermodern style, Tartak...</td>\n",
       "      <td>rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>576</td>\n",
       "      <td>2</td>\n",
       "      <td>Black</td>\n",
       "      <td>e7e6</td>\n",
       "      <td>r n b q k b n r\\np p p p p p p p\\n. . . . . . ...</td>\n",
       "      <td>Maroczy (Black) makes an illogical move of his...</td>\n",
       "      <td>rnbqkbnr/pppppppp/8/8/1P6/8/P1PPPPPP/RNBQKBNR ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>576</td>\n",
       "      <td>3</td>\n",
       "      <td>White</td>\n",
       "      <td>c1b2</td>\n",
       "      <td>r n b q k b n r\\np p p p . p p p\\n. . . . p . ...</td>\n",
       "      <td>Tartakower doesn't need to defend the pawn yet...</td>\n",
       "      <td>rnbqkbnr/pppp1ppp/4p3/8/1P6/8/P1PPPPPP/RNBQKBN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>576</td>\n",
       "      <td>4</td>\n",
       "      <td>Black</td>\n",
       "      <td>g8f6</td>\n",
       "      <td>r n b q k b n r\\np p p p . p p p\\n. . . . p . ...</td>\n",
       "      <td>Maroczy follows more conventional lines, devel...</td>\n",
       "      <td>rnbqkbnr/pppp1ppp/4p3/8/1P6/8/PBPPPPPP/RN1QKBN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>576</td>\n",
       "      <td>5</td>\n",
       "      <td>White</td>\n",
       "      <td>b4b5</td>\n",
       "      <td>r n b q k b . r\\np p p p . p p p\\n. . . . p n ...</td>\n",
       "      <td>Tartakower inhibits the development of Black's...</td>\n",
       "      <td>rnbqkb1r/pppp1ppp/4pn2/8/1P6/8/PBPPPPPP/RN1QKB...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Game Number  Move Number Player  Move  \\\n",
       "0      0          576            1  White  b2b4   \n",
       "1      1          576            2  Black  e7e6   \n",
       "2      2          576            3  White  c1b2   \n",
       "3      3          576            4  Black  g8f6   \n",
       "4      4          576            5  White  b4b5   \n",
       "\n",
       "                                               Board  \\\n",
       "0  r n b q k b n r\\np p p p p p p p\\n. . . . . . ...   \n",
       "1  r n b q k b n r\\np p p p p p p p\\n. . . . . . ...   \n",
       "2  r n b q k b n r\\np p p p . p p p\\n. . . . p . ...   \n",
       "3  r n b q k b n r\\np p p p . p p p\\n. . . . p . ...   \n",
       "4  r n b q k b . r\\np p p p . p p p\\n. . . . p n ...   \n",
       "\n",
       "                                             Comment  \\\n",
       "0  WHAT?!?! In the true hypermodern style, Tartak...   \n",
       "1  Maroczy (Black) makes an illogical move of his...   \n",
       "2  Tartakower doesn't need to defend the pawn yet...   \n",
       "3  Maroczy follows more conventional lines, devel...   \n",
       "4  Tartakower inhibits the development of Black's...   \n",
       "\n",
       "                                                 FEN  \n",
       "0  rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w ...  \n",
       "1  rnbqkbnr/pppppppp/8/8/1P6/8/P1PPPPPP/RNBQKBNR ...  \n",
       "2  rnbqkbnr/pppp1ppp/4p3/8/1P6/8/P1PPPPPP/RNBQKBN...  \n",
       "3  rnbqkbnr/pppp1ppp/4p3/8/1P6/8/PBPPPPPP/RN1QKBN...  \n",
       "4  rnbqkb1r/pppp1ppp/4pn2/8/1P6/8/PBPPPPPP/RN1QKB...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def board_to_fen(board, player, move_number):\n",
    "    # Extract rows and reverse to match FEN order (from 8th to 1st rank)\n",
    "    rows = board.strip().split('\\n')\n",
    "\n",
    "    # Convert board rows to FEN format\n",
    "    fen_rows = []\n",
    "    for row in rows:\n",
    "        clean_row = re.sub(r\"\\s+\", \"\", row)  # Remove spaces\n",
    "        # Correctly replace consecutive dots with the count\n",
    "        fen_row = re.sub(r\"(\\.+) \", lambda m: str(len(m.group(1))), clean_row)  # Incorrectly placed space\n",
    "        fen_row = re.sub(r\"\\.+\", lambda m: str(len(m.group(0))), fen_row)  # Handle all dots\n",
    "        fen_rows.append(fen_row)\n",
    "\n",
    "    fen_pieces = \"/\".join(fen_rows)\n",
    "\n",
    "    # Active color ('w' for White, 'b' for Black)\n",
    "    active_color = 'w' if player == 'White' else 'b'\n",
    "\n",
    "    # Default values for other FEN components (assuming basic capabilities)\n",
    "    castling_availability = 'KQkq'  # Assuming all castling is still possible\n",
    "    en_passant_target = '-'  # No en passant target\n",
    "    halfmove_clock = '0'  # Reset on pawn moves or captures, not shown here\n",
    "    fullmove_number = str(move_number)\n",
    "\n",
    "    # Compile full FEN string\n",
    "    fen = f\"{fen_pieces} {active_color} {castling_availability} {en_passant_target} {halfmove_clock} {fullmove_number}\"\n",
    "    return fen\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df['FEN'] = df.apply(lambda row: board_to_fen(row['Board'], row['Player'], row['Move Number']), axis=1)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gYeg4UWLAty4"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'Model': 'gpt2',\n",
    "    'NUMER_OF_DATA_DIRS': 12,\n",
    "    'batch_size': 2,\n",
    "    'lr':  3e-5,\n",
    "    'train_precentege': 0.9,\n",
    "    'epochs': 5,\n",
    "    'data_to_use': {'<fen>': True, '<moves>': True, '<last move description>': False,\n",
    "                    '<legal moves>': False, '<attacked by>': False, '<attacks>': False},\n",
    "    'max_length':300\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "359XaEmSeOLu"
   },
   "outputs": [],
   "source": [
    "# combined csv\n",
    "# def convert_data_to_text(row, max_length=config['max_length'], end_of_text_token=\"\"):\n",
    "#     # Assuming 'FEN', 'moves', and 'comment' are column names in the DataFrame\n",
    "#     FEN, moves, comment = row['FEN'], row['Move'], row['Comment']\n",
    "#     FEN, moves, comment = FEN[:max_length], moves[:max_length], comment[:max_length]\n",
    "#     token_to_data = {'<fen>': FEN, '<moves>': moves}\n",
    "#     text = \"\"\n",
    "#     for token, data in token_to_data.items():\n",
    "#         text += f\"{token} {data} \"\n",
    "#     text += f\"<comment> {comment} {end_of_text_token}\"\n",
    "#     return text\n",
    "\n",
    "def convert_data_to_text(row, max_length=config['max_length'], end_of_text_token=\"\"):\n",
    "    # Assuming 'FEN', 'moves', and 'comment' are column names in the DataFrame\n",
    "    FEN, moves, comment = row['FEN'], row['Move'], row['Comment']\n",
    "    FEN, moves, comment = FEN[:max_length], moves[:max_length], comment[:max_length]\n",
    "\n",
    "    # Split moves into a list\n",
    "    move_list = moves.split(',')\n",
    "\n",
    "    # Truncate move history to last 5 moves (or less for initial moves)\n",
    "    history_moves = move_list[-min(len(move_list), 5):]  # Get last 5 or less moves\n",
    "    history_moves_str = \", \".join(history_moves)  # Join moves with comma separator\n",
    "\n",
    "    token_to_data = {'<fen>': FEN, '<moves>': history_moves_str}\n",
    "    text = \"\"\n",
    "    for token, data in token_to_data.items():\n",
    "        text += f\"{token} {data} \"\n",
    "    text += f\"<comment> {comment} {end_of_text_token}\"\n",
    "    return text\n",
    "\n",
    "\n",
    "# def convert_data_to_text_2(data_object, max_length=768, end_of_text_token=\"<|endoftext|>\"):\n",
    "#     (FEN, moves, last_move_desc, legal_moves, attackers_list, attacks_list, comment) = data_object\n",
    "#     (FEN, moves, last_move_desc, legal_moves,\n",
    "#      attackers_list, attacks_list, comment) = (FEN[:max_length], moves[:max_length], last_move_desc[:max_length],\n",
    "#                                                legal_moves[:max_length], attackers_list[:max_length],\n",
    "#                                                attacks_list[:max_length], comment[:max_length])\n",
    "#     moves = moves.split(',')[-1].strip() if ',' in moves else moves\n",
    "\n",
    "#     token_to_data = {'<fen>': FEN, '<moves>': moves, '<last move description>': last_move_desc,\n",
    "#                      '<legal moves>': legal_moves, '<attacked by>': attackers_list, '<attacks>': attacks_list}\n",
    "#     text = \"\"\n",
    "#     for token in token_to_data.keys():\n",
    "#         if config['data_to_use'][token]:\n",
    "#             text += f\"{token} {token_to_data[token]} \"\n",
    "#     text += f\"<comment> {comment} {end_of_text_token}\"  # comment always included at the end + end token\n",
    "\n",
    "#     return text\n",
    "\n",
    "def convert_data_to_text_2(data_object, max_length=768, end_of_text_token=\"<|endoftext|>\"):\n",
    "    (FEN, moves, last_move_desc, legal_moves, attackers_list, attacks_list, comment) = data_object\n",
    "    (FEN, moves, last_move_desc, legal_moves,\n",
    "     attackers_list, attacks_list, comment) = (FEN[:max_length], moves[:max_length], last_move_desc[:max_length],\n",
    "                                               legal_moves[:max_length], attackers_list[:max_length],\n",
    "                                               attacks_list[:max_length], comment[:max_length])\n",
    "    moves = moves.split(',')[-1].strip() if ',' in moves else moves\n",
    "\n",
    "    # Split moves into a list\n",
    "    move_list = moves.split(',')\n",
    "\n",
    "    # Truncate move history to last 5 moves (or less for initial moves)\n",
    "    history_moves = move_list[-min(len(move_list), 5):]  # Get last 5 or less moves\n",
    "    history_moves_str = \", \".join(history_moves)  # Join moves with comma separator\n",
    "\n",
    "    token_to_data = {'<fen>': FEN, '<moves>': history_moves_str, '<last move description>': last_move_desc,\n",
    "                     '<legal moves>': legal_moves, '<attacked by>': attackers_list, '<attacks>': attacks_list}\n",
    "    text = \"\"\n",
    "    for token in token_to_data.keys():\n",
    "        if config['data_to_use'][token]:\n",
    "            text += f\"{token} {token_to_data[token]} \"\n",
    "    text += f\"<comment> {comment} {end_of_text_token}\"  # comment always included at the end + end token\n",
    "\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "n00akTndAG-O"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "dataset_tokens = list(config['data_to_use'].keys()) + ['<comment>']\n",
    "\n",
    "\n",
    "class ProcessDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=config['max_length']):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.comment_encoding = tokenizer.get_added_vocab()['<comment>']\n",
    "        self.proccessed_data = []\n",
    "        self.attn_masks = []\n",
    "        self.labels = []\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            text = convert_data_to_text(row, max_length)\n",
    "            enc_text = tokenizer(text, truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "            inputs = enc_text['input_ids']\n",
    "            label_idx = inputs.index(self.comment_encoding) + 1\n",
    "            labels = [-100] * label_idx + inputs[label_idx:]\n",
    "\n",
    "            self.proccessed_data.append(torch.tensor(inputs))\n",
    "            self.attn_masks.append(torch.tensor(enc_text['attention_mask']))\n",
    "            self.labels.append(torch.tensor(labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.proccessed_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.proccessed_data[index], self.attn_masks[index], self.labels[index]\n",
    "\n",
    "class MovesDataset(Dataset):\n",
    "    def __init__(self, paths, tokenizer, max_length=768):\n",
    "\n",
    "        self.comment_encoding = tokenizer.get_added_vocab()['<comment>']\n",
    "\n",
    "        self.proccessed_data = []\n",
    "        self.attn_masks = []\n",
    "        self.labels = []\n",
    "\n",
    "\n",
    "        for path in paths:\n",
    "            with open(path, 'rb') as file:\n",
    "                raw_data = pickle.load(file)\n",
    "            for data_object in raw_data:\n",
    "                text = convert_data_to_text_2(data_object)\n",
    "\n",
    "                enc_text = tokenizer(text, truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "\n",
    "                inputs = enc_text['input_ids']\n",
    "                label_idx = inputs.index(self.comment_encoding) + 1\n",
    "                labels = [-100] * label_idx + inputs[label_idx:]\n",
    "\n",
    "                self.proccessed_data.append(torch.tensor(inputs))\n",
    "                self.attn_masks.append(torch.tensor(enc_text['attention_mask']))\n",
    "                self.labels.append(torch.tensor(labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.proccessed_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.proccessed_data[index], self.attn_masks[index], self.labels[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQCG8sq0HnTi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "CuYMVboH8J5O"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Config, GPT2Tokenizer\n",
    "\n",
    "GPT2_TYPE = \"gpt2\"\n",
    "\n",
    "class GPT2:\n",
    "    def __init__(self):\n",
    "        print(\"Initialization\\n\")\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(GPT2_TYPE)\n",
    "        special_tokens_dict = {\n",
    "            'pad_token': '[PAD]',\n",
    "            'additional_special_tokens': dataset_tokens\n",
    "        }\n",
    "        self.tokenizer.add_special_tokens(special_tokens_dict)\n",
    "        # self.tokenizer.add_tokens(get_chess_tokens())\n",
    "\n",
    "        self.configuration = GPT2Config.from_pretrained(GPT2_TYPE)\n",
    "        self.model = GPT2LMHeadModel(self.configuration)\n",
    "\n",
    "        # Resize token embeddings to accommodate new tokens\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        print(\"Loading model\\n\")\n",
    "        state_dict = torch.load(model_path, map_location=self.device)\n",
    "        self.model.load_state_dict(state_dict)\n",
    "        self.model.to(self.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qHLCuoYOAJRi",
    "outputId": "86694063-7220-47a7-8847-392300dd42b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e4a721d8c5148d0b19b9078d2d83edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f62a264912d14cc694d275868cc915f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a12374a1a6450f8cea365c70df40ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8afabd35524915866584ac6da8f280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "315496fb4eb94a39a588d44780cf5e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpt2 = GPT2()\n",
    "model = gpt2.model.train()\n",
    "tokenizer = gpt2.tokenizer\n",
    "max_length = config['max_length']\n",
    "eof = '<|endoftext|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "EF7-BmPuKjCl"
   },
   "outputs": [],
   "source": [
    "games_data_path = '/home'\n",
    "saved_models_path = '/home/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uzwm1mrqER_f"
   },
   "outputs": [],
   "source": [
    "df[\"Comment\"] = df[\"Comment\"].fillna('No Comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "zMIqNFmKIEjd"
   },
   "outputs": [],
   "source": [
    "dataset = ProcessDataset(df,tokenizer)\n",
    "# dataset = MovesDataset([f'{games_data_path}1.p'], tokenizer, max_length=max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "YP115lPlIRVF"
   },
   "outputs": [],
   "source": [
    "train_size = int(config['train_precentege'] * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349,
     "referenced_widgets": [
      "11d81c1544db432699e563160a7d0e21",
      "b584d317803948838f214fb633fa1f94",
      "ee9243cf51244b2f809dfe3f6c8f1805",
      "b25b47cbb6644852990f6be859742d1a",
      "a252e6ad908442b798af0d47664ee75d",
      "ca158732ae63434183a638107ff399db",
      "89643eead3b5432bb37588ab67ecb59c",
      "bb5bae5f7345440ca7adbee5a37292b1"
     ]
    },
    "id": "0sq9R4yjIl63",
    "outputId": "a7250c17-55d3-41e8-8910-2236dc47cab3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmzkingsl6\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/wandb/run-20240423_002517-c7jmh5g9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mzkingsl6/LmChess/runs/c7jmh5g9' target=\"_blank\">ancient-voice-7</a></strong> to <a href='https://wandb.ai/mzkingsl6/LmChess' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mzkingsl6/LmChess' target=\"_blank\">https://wandb.ai/mzkingsl6/LmChess</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mzkingsl6/LmChess/runs/c7jmh5g9' target=\"_blank\">https://wandb.ai/mzkingsl6/LmChess/runs/c7jmh5g9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"LmChess\", config={'batch size': config['batch_size'], 'lr': config['lr'], 'epochs': config['epochs']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aT9YoKwHJL5A"
   },
   "outputs": [],
   "source": [
    "#validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "GHS4fgRRJbwS"
   },
   "outputs": [],
   "source": [
    "\n",
    "validation_proccessed_data, validation_attn_masks, validation_labels = next(iter(test_dataloader))\n",
    "\n",
    "validation_input_encodings = []\n",
    "for i in range(config['batch_size']):\n",
    "  textual_validation_data = tokenizer.decode(token_ids = validation_proccessed_data[i], skip_special_tokens=False).split('<comment>')\n",
    "\n",
    "  validation_target_text = textual_validation_data[1].split(eof)[0]\n",
    "  validation_input_text = textual_validation_data[0]\n",
    "\n",
    "  wandb.log({f\"validation_target_text {i}\": wandb.Html(f'<p>{validation_target_text}</p>')})\n",
    "  wandb.log({f\"validation_input_text {i}\": wandb.Html(f'<p>{validation_input_text}</p>')})\n",
    "\n",
    "  comment_idx = list(validation_proccessed_data[i]).index(dataset.comment_encoding) + 1\n",
    "  validation_input_encoding = validation_proccessed_data[i][:comment_idx].unsqueeze(0).cuda()\n",
    "  #validation_input_encoding  = tokenizer.encode(validation_input_text, return_tensors=\"pt\").cuda()\n",
    "\n",
    "  validation_input_encodings.append(validation_input_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IpZ6G7E7Jc5v",
    "outputId": "10931e86-0596-403b-d790-a85256d846d2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "8216it [03:08, 43.63it/s]                            \n",
      "8216it [03:05, 44.29it/s]                            \n",
      "8216it [03:08, 43.64it/s]                            \n",
      "8216it [03:05, 44.20it/s]                            \n",
      "8216it [03:05, 44.37it/s]                            \n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = AdamW(model.parameters(), lr= config['lr'])\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=5000, num_training_steps=-1\n",
    ")\n",
    "\n",
    "loss = 0\n",
    "pad_token_id = tokenizer('[PAD]')['input_ids'][0]\n",
    "\n",
    "epochs = config['epochs']\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    with tqdm(total=len(dataset) / 2) as pbar:\n",
    "        for idx,entry in enumerate(train_dataloader):\n",
    "\n",
    "            if idx % 500 == 0 and idx != 0:\n",
    "              for i in range(config['batch_size']):\n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(validation_input_encodings[i], num_beams=2, no_repeat_ngram_size=2, max_length=max_length+1, pad_token_id=pad_token_id)\n",
    "                    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                wandb.log({f\"output_text {i}\": wandb.Html(f'<p>{output_text}</p>')})\n",
    "\n",
    "            if idx % 1000 == 0:\n",
    "              torch.save(model.state_dict(), f'{saved_models_path}{idx}_{time.time()}_{int(loss)}.bin')\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            inputs = entry[0].cuda()\n",
    "            attn_masks = entry[1].cuda()\n",
    "            labels = entry[2].cuda()\n",
    "            outputs = model(inputs, labels=labels, attention_mask = attn_masks)\n",
    "\n",
    "            loss = outputs['loss']\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            wandb.log({\"epoch\": epoch, \"loss\": loss})\n",
    "            pbar.update(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "F7FjFqk7K34f"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'{saved_models_path}final.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3UCf84ZbMpmo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BjF6ilFMrTz"
   },
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "NE22F4CxN3CT"
   },
   "outputs": [],
   "source": [
    "model_path = '/home/models/final.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MRQnd-RBNoEV",
    "outputId": "981ee369-e94b-473a-b173-d5979646ac3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization\n",
      "\n",
      "Loading model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt2_test = GPT2()\n",
    "\n",
    "gpt2_test.load_model(model_path)\n",
    "\n",
    "gpt2_test.model = gpt2_test.model.eval().cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ROrkDkWrOOdW"
   },
   "outputs": [],
   "source": [
    "tested_model = gpt2_test\n",
    "max = config['max_length']\n",
    "eof = '<|endoftext|>'\n",
    "pad_token_id = tested_model.tokenizer('[PAD]')['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "uOiwojk1MsfC"
   },
   "outputs": [],
   "source": [
    "\n",
    "dataloader = test_dataloader\n",
    "def get_results():\n",
    "    # Set device based on availability of CUDA\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Move the model to the appropriate device\n",
    "    tested_model.model.to(device)\n",
    "\n",
    "    # Retrieve batch data\n",
    "    processed_data, attn_masks, labels = next(iter(dataloader))\n",
    "\n",
    "    # Initialize input encodings container\n",
    "    input_encodings = []\n",
    "\n",
    "    for i in range(config['batch_size']):\n",
    "        # Decode the token_ids to text and split at <comment>\n",
    "        textual_data = tested_model.tokenizer.decode(token_ids=processed_data[i], skip_special_tokens=False).split('<comment>')\n",
    "\n",
    "        # Target text ends at EOF (assuming EOF is defined elsewhere in your code)\n",
    "        target_text = textual_data[1].split(eof)[0]\n",
    "        input_text = textual_data[0]\n",
    "\n",
    "        # Find the index of <comment> and prepare input encoding\n",
    "        comment_idx = list(processed_data[i]).index(tested_model.tokenizer.get_added_vocab()['<comment>']) + 1\n",
    "        input_encoding = processed_data[i][:comment_idx].unsqueeze(0).to(device)\n",
    "\n",
    "        input_encodings.append(input_encoding)\n",
    "\n",
    "    # Initialize results container\n",
    "    results = []\n",
    "\n",
    "    for input_encoding in input_encodings:\n",
    "        with torch.no_grad():\n",
    "            # Generate output using the model's generate function\n",
    "            outputs = tested_model.model.generate(\n",
    "                input_encoding,\n",
    "                num_beams=2,\n",
    "                no_repeat_ngram_size=2,\n",
    "                max_length=max + 1,  # assuming 'max' is defined as the maximum length\n",
    "                pad_token_id=pad_token_id  # assuming 'pad_token_id' is defined elsewhere\n",
    "            )\n",
    "            output_text = tested_model.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            results.append(output_text)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oSnNK-A1NT4v",
    "outputId": "8e3c1166-f045-4326-ae2a-675936da5b73"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" r1bqkb1r/5ppp/p1np1N2/4p3/1pP1P3/N7/PP3PPP/R1BQKB1R b KQkq - 0 20  d8f6  This is a-pawn.. The Knight on d5, but he can't be a pawn, the White's\\nthe Black's position. Now it's pawn on e5-e5.Ng6, so he has been a5 and the b-file, and d4,\\na5   without d-f5 is\\nto be to the c-Bishop, he would have a\\ne4. But, for the Black Rook on the board, without White King. He's without Black King's Queen's Rooks on f-d5 (a6-Rook to without f3-g5 to a Rf3, a Knight.Rg4-b5 on without the d6. If he's a piece,, it can be without e-c5).  Black Knight's King to play...Nf8-Knight. In the without without c3 and f4 and without pawns on c5...Nc3. This, which can play on g-h5) and it without g5 without b3 is the f6 and Black pawn. Black Queen.\",\n",
       " \" 2rq2rk/3bbp1p/p1p1pp2/3p4/4PP2/5B2/PPP1N1PP/R2Q1R1K w KQkq - 0 33  f4f5  This is a pawn, but it's pawns on the Black's Queen.. The White's a-pawn, the b5, so he can't be a5-e5 and the c-file, and d5.Ng6-f4,\\nthe\\ne4-Bishop, he has the board. Now the White Rook on d-Rg5 (a5 to the d4 and f-d4. But, Black King's position, without White King. He's without d7-b5 is an attack on f7, it can be to without the without f3-Knight. If it.Rook to play...Nf6, a Rooks on e-c5).  without Black Rf3, for the f6 and without without e5...Nc3 and Black Queen's\\na7.d5 on without pawn on c3.g4 . without a Knight. In the e4 (e6.c4 is the g-side, this is to win the pawn for Black Knight on Black can without g5) without c5 without\"]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = get_results()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4tJ9GQAGNVXc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDDzfyjmOo2Z"
   },
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "8ZJ0IFmuOqo0",
    "outputId": "5af4736b-1968-4c7a-870f-80ab76b7c6ce"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "errorDetails": {
      "actions": [
       {
        "action": "open_url",
        "actionText": "Open Examples",
        "url": "/notebooks/snippets/importing_libraries.ipynb"
       }
      ]
     },
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-c836eee1c833>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbleu_score\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msentence_bleu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from datasets import load_metric\n",
    "import tensorflow as tf\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "\n",
    "def perplexity(model, dataloader):\n",
    "    eval_loss = 0\n",
    "    with tqdm(total=len(dataloader)) as pbar:\n",
    "        for idx, entry in enumerate(dataloader):\n",
    "            with torch.no_grad():\n",
    "                inputs = entry[0].cuda()\n",
    "                attn_masks = entry[1].cuda()\n",
    "                labels = entry[2].cuda()\n",
    "                outputs = model(inputs, labels=labels, attention_mask=attn_masks)\n",
    "            loss = outputs[0]\n",
    "            eval_loss += loss.mean().item()\n",
    "            pbar.update(2)\n",
    "    final_eval_loss = eval_loss / len(dataloader)\n",
    "    perplexity = torch.exp(torch.tensor(final_eval_loss))\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def bleurt(target_texts, output_texts):\n",
    "    metric = load_metric(\"bleurt\")\n",
    "    tf.compat.v1.flags.DEFINE_string('f', '', '')\n",
    "\n",
    "    scores = metric.compute(predictions=output_texts, references=target_texts)['scores']\n",
    "    return scores\n",
    "\n",
    "\n",
    "def bleu(target_texts, output_texts):\n",
    "    scores = []\n",
    "    for idx in range(len(output_texts)):\n",
    "        reference = [target_texts[idx].split()]\n",
    "        candidate = output_texts[idx].split()\n",
    "        scores.append(sentence_bleu(reference, candidate))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iGKGbdSiXWwu",
    "outputId": "4f4c5284-9277-4700-eb94-843edaca0f99"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [01:31, 32.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8895)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_perplexity = perplexity(tested_model.model, dataloader)\n",
    "print(test_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TdovyEjmYCSV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_targets_and_outputs(model, dataset, comment_encoding, pad_token_id, max_length=768, eof='<|endoftext|>'):\n",
    "    target_texts = []\n",
    "    output_texts = []\n",
    "    with tqdm(total=len(dataset)) as pbar:\n",
    "        for idx, entry in enumerate(dataset):\n",
    "\n",
    "          textual_data = model.tokenizer.decode(token_ids=entry[0], skip_special_tokens=False)\n",
    "          textual_data = textual_data.split('<comment>')[1].split(eof)[0]\n",
    "          target_texts.append(textual_data)\n",
    "\n",
    "          comment_idx = list(entry[0]).index(comment_encoding) + 1\n",
    "          input_encoding = entry[0][:comment_idx].unsqueeze(0).cuda()\n",
    "          with torch.no_grad():\n",
    "              outputs = model.model.generate(input_encoding, num_beams=2, no_repeat_ngram_size=2, max_length=max_length+1, pad_token_id=pad_token_id)\n",
    "              output_text = model.tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "              output_text = output_text.split('<comment>')[1].split(eof)[0]\n",
    "          output_texts.append(output_text)\n",
    "\n",
    "          pbar.update(1)\n",
    "    return target_texts, output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "xJ__FqFtXsvv",
    "outputId": "ddd30cf6-7558-414e-fd09-12c372aa8802"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [1:15:52<00:00,  6.59it/s]\n"
     ]
    }
   ],
   "source": [
    "target_texts, output_texts = get_targets_and_outputs(tested_model, dataset, dataset.comment_encoding, pad_token_id, max_length=max, eof=eof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "referenced_widgets": [
      "cec5a51bfa4e4097b2a022dfd0ee53d7"
     ]
    },
    "id": "7msGQy-_Xbxr",
    "outputId": "ad8dfe7b-b18d-4647-ac74-483c57203871"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-143-c836eee1c833>:26: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"bleurt\")\n",
      "/usr/local/lib/python3.10/dist-packages/datasets/load.py:759: FutureWarning: The repository for bleurt contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/bleurt/bleurt.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec5a51bfa4e4097b2a022dfd0ee53d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.96k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "To be able to use bleurt, you need to install the following dependency: bleurt.\nPlease install it using 'pip install git+https://github.com/google-research/bleurt.git' for instance.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-152-4170f5103bbf>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_bleurt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbleurt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-143-c836eee1c833>\u001b[0m in \u001b[0;36mbleurt\u001b[0;34m(target_texts, output_texts)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbleurt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bleurt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/utils/deprecation_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarning_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0m_emitted_deprecation_warnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_hash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdeprecated_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decorator_name_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"deprecated\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_metric\u001b[0;34m(path, config_name, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, revision, trust_remote_code, **metric_init_kwargs)\u001b[0m\n\u001b[1;32m   2102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[0mdownload_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDownloadMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_mode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mDownloadMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREUSE_DATASET_IF_EXISTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2104\u001b[0;31m         metric_module = metric_module_factory(\n\u001b[0m\u001b[1;32m   2105\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2106\u001b[0m             \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/utils/deprecation_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarning_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0m_emitted_deprecation_warnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_hash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdeprecated_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decorator_name_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"deprecated\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mmetric_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, trust_remote_code, **download_kwargs)\u001b[0m\n\u001b[1;32m   2020\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa if it's not in the cache, then it doesn't exist.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2021\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2022\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2023\u001b[0m                     raise FileNotFoundError(\n\u001b[1;32m   2024\u001b[0m                         \u001b[0;34mf\"Couldn't find a metric script at {relative_to_absolute_path(combined_path)}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mmetric_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, trust_remote_code, **download_kwargs)\u001b[0m\n\u001b[1;32m   2014\u001b[0m                     \u001b[0mdynamic_modules_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdynamic_modules_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2015\u001b[0m                     \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2016\u001b[0;31m                 ).get_module()\n\u001b[0m\u001b[1;32m   2017\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa all the attempts failed, before raising the error we should check if the module is already cached.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2018\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mget_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    780\u001b[0m                 )\n\u001b[1;32m    781\u001b[0m         \u001b[0mimports\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_imports\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m         local_imports = _download_additional_modules(\n\u001b[0m\u001b[1;32m    783\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m             \u001b[0mbase_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhf_github_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36m_download_additional_modules\u001b[0;34m(name, base_path, imports, download_config)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"Bio\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneeds_to_be_installed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mneeds_to_be_installed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Bio\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"biopython\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         raise ImportError(\n\u001b[0m\u001b[1;32m    354\u001b[0m             \u001b[0;34mf\"To be able to use {name}, you need to install the following {_dependencies_str}: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;34mf\"{', '.join(needs_to_be_installed)}.\\nPlease install {_them_str} using 'pip install \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: To be able to use bleurt, you need to install the following dependency: bleurt.\nPlease install it using 'pip install git+https://github.com/google-research/bleurt.git' for instance.",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "test_bleurt = bleurt(target_texts, output_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Eyd6VQXyXol8"
   },
   "outputs": [],
   "source": [
    "print(sum(test_bleurt)/len(test_bleurt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "yRQ2qRM-YEt2"
   },
   "outputs": [],
   "source": [
    "test_bleu = bleu(target_texts, output_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "_Qtb0fILYKzy"
   },
   "outputs": [],
   "source": [
    "print(sum(test_bleu)/len(test_bleu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zaOsUzSbspCe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "11d81c1544db432699e563160a7d0e21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b584d317803948838f214fb633fa1f94",
       "IPY_MODEL_ee9243cf51244b2f809dfe3f6c8f1805"
      ],
      "layout": "IPY_MODEL_b25b47cbb6644852990f6be859742d1a"
     }
    },
    "89643eead3b5432bb37588ab67ecb59c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a252e6ad908442b798af0d47664ee75d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b25b47cbb6644852990f6be859742d1a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b584d317803948838f214fb633fa1f94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a252e6ad908442b798af0d47664ee75d",
      "placeholder": "​",
      "style": "IPY_MODEL_ca158732ae63434183a638107ff399db",
      "value": "0.018 MB of 0.018 MB uploaded\r"
     }
    },
    "bb5bae5f7345440ca7adbee5a37292b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ca158732ae63434183a638107ff399db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ee9243cf51244b2f809dfe3f6c8f1805": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89643eead3b5432bb37588ab67ecb59c",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bb5bae5f7345440ca7adbee5a37292b1",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
